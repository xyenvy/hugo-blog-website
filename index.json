[{"content":"Kubernetes详细教程 1. Kubernetes介绍 1.1 应用部署方式演变 在部署应用程序的方式上，主要经历了三个时代：\n传统部署：互联网早期，会直接将应用程序部署在物理机上\n优点：简单，不需要其它技术的参与\n缺点：不能为应用程序定义资源使用边界，很难合理地分配计算资源，而且程序之间容易产生影响\n虚拟化部署：可以在一台物理机上运行多个虚拟机，每个虚拟机都是独立的一个环境\n优点：程序环境不会相互产生影响，提供了一定程度的安全性\n缺点：增加了操作系统，浪费了部分资源\n容器化部署：与虚拟化类似，但是共享了操作系统\n优点：\n可以保证每个容器拥有自己的文件系统、CPU、内存、进程空间等\n运行应用程序所需要的资源都被容器包装，并和底层基础架构解耦\n容器化的应用程序可以跨云服务商、跨Linux操作系统发行版进行部署\n容器化部署方式给带来很多的便利，但是也会出现一些问题，比如说：\n一个容器故障停机了，怎么样让另外一个容器立刻启动去替补停机的容器 当并发访问量变大的时候，怎么样做到横向扩展容器数量 这些容器管理的问题统称为容器编排问题，为了解决这些容器编排问题，就产生了一些容器编排的软件：\nSwarm：Docker自己的容器编排工具 Mesos：Apache的一个资源统一管控的工具，需要和Marathon结合使用 Kubernetes：Google开源的的容器编排工具 1.2 kubernetes简介 kubernetes，是一个全新的基于容器技术的分布式架构领先方案，是谷歌严格保密十几年的秘密武器\u0026mdash;-Borg系统的一个开源版本，于2014年9月发布第一个版本，2015年7月发布第一个正式版本。\nkubernetes的本质是一组服务器集群，它可以在集群的每个节点上运行特定的程序，来对节点中的容器进行管理。目的是实现资源管理的自动化，主要提供了如下的主要功能：\n自我修复：一旦某一个容器崩溃，能够在1秒中左右迅速启动新的容器 弹性伸缩：可以根据需要，自动对集群中正在运行的容器数量进行调整 服务发现：服务可以通过自动发现的形式找到它所依赖的服务 负载均衡：如果一个服务起动了多个容器，能够自动实现请求的负载均衡 版本回退：如果发现新发布的程序版本有问题，可以立即回退到原来的版本 存储编排：可以根据容器自身的需求自动创建存储卷 1.3 kubernetes组件 一个kubernetes集群主要是由控制节点(master)、**工作节点(node)**构成，每个节点上都会安装不同的组件。\nmaster：集群的控制平面，负责集群的决策 ( 管理 )\nApiServer : 资源操作的唯一入口，接收用户输入的命令，提供认证、授权、API注册和发现等机制\nScheduler : 负责集群资源调度，按照预定的调度策略将Pod调度到相应的node节点上\nControllerManager : 负责维护集群的状态，比如程序部署安排、故障检测、自动扩展、滚动更新等\nEtcd ：负责存储集群中各种资源对象的信息\nnode：集群的数据平面，负责为容器提供运行环境 ( 干活 )\nKubelet : 负责维护容器的生命周期，即通过控制docker，来创建、更新、销毁容器\nKubeProxy : 负责提供集群内部的服务发现和负载均衡\nDocker : 负责节点上容器的各种操作\n下面，以部署一个nginx服务来说明kubernetes系统各个组件调用关系：\n首先要明确，一旦kubernetes环境启动之后，master和node都会将自身的信息存储到etcd数据库中\n一个nginx服务的安装请求会首先被发送到master节点的apiServer组件\napiServer组件会调用scheduler组件来决定到底应该把这个服务安装到哪个node节点上\n在此时，它会从etcd中读取各个node节点的信息，然后按照一定的算法进行选择，并将结果告知apiServer\napiServer调用controller-manager去调度Node节点安装nginx服务\nkubelet接收到指令后，会通知docker，然后由docker来启动一个nginx的pod\npod是kubernetes的最小操作单元，容器必须跑在pod中至此，\n一个nginx服务就运行了，如果需要访问nginx，就需要通过kube-proxy来对pod产生访问的代理\n这样，外界用户就可以访问集群中的nginx服务了\n1.4 kubernetes概念 Master：集群控制节点，每个集群需要至少一个master节点负责集群的管控\nNode：工作负载节点，由master分配容器到这些node工作节点上，然后node节点上的docker负责容器的运行\nPod：kubernetes的最小控制单元，容器都是运行在pod中的，一个pod中可以有1个或者多个容器\nController：控制器，通过它来实现对pod的管理，比如启动pod、停止pod、伸缩pod的数量等等\nService：pod对外服务的统一入口，下面可以维护者同一类的多个pod\nLabel：标签，用于对pod进行分类，同一类pod会拥有相同的标签\nNameSpace：命名空间，用来隔离pod的运行环境\n2. kubernetes集群环境搭建 2.1 前置知识点 目前生产部署Kubernetes 集群主要有两种方式：\nkubeadm\nKubeadm 是一个K8s 部署工具，提供kubeadm init 和kubeadm join，用于快速部署Kubernetes 集群。\n官方地址：https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/\n二进制包\n从github 下载发行版的二进制包，手动部署每个组件，组成Kubernetes 集群。\nKubeadm 降低部署门槛，但屏蔽了很多细节，遇到问题很难排查。如果想更容易可控，推荐使用二进制包部署Kubernetes 集群，虽然手动部署麻烦点，期间可以学习很多工作原理，也利于后期维护。\n2.2 kubeadm 部署方式介绍 kubeadm 是官方社区推出的一个用于快速部署kubernetes 集群的工具，这个工具能通过两条指令完成一个kubernetes 集群的部署：\n创建一个Master 节点kubeadm init 将Node 节点加入到当前集群中$ kubeadm join \u0026lt;Master 节点的IP 和端口\u0026gt; 2.3 安装要求 在开始之前，部署Kubernetes 集群机器需要满足以下几个条件：\n一台或多台机器，操作系统CentOS7.x-86_x64 硬件配置：2GB 或更多RAM，2 个CPU 或更多CPU，硬盘30GB 或更多 集群中所有机器之间网络互通 可以访问外网，需要拉取镜像 禁止swap 分区 2.4 最终目标 在所有节点上安装Docker 和kubeadm 部署Kubernetes Master 部署容器网络插件 部署Kubernetes Node，将节点加入Kubernetes 集群中 部署Dashboard Web 页面，可视化查看Kubernetes 资源 2.5 准备环境 角色 IP地址 组件 master01 192.168.5.3 docker，kubectl，kubeadm，kubelet node01 192.168.5.4 docker，kubectl，kubeadm，kubelet node02 192.168.5.5 docker，kubectl，kubeadm，kubelet 2.6 环境初始化 2.6.1 检查操作系统的版本 # 此方式下安装kubernetes集群要求Centos版本要在7.5或之上 [root@master ~]# cat /etc/redhat-release Centos Linux 7.5.1804 (Core) 2.6.2 主机名解析 为了方便集群节点间的直接调用，在这个配置一下主机名解析，企业中推荐使用内部DNS服务器\n# 主机名成解析 编辑三台服务器的/etc/hosts文件，添加下面内容 192.168.90.100 master 192.168.90.106 node1 192.168.90.107 node2 2.6.3 时间同步 kubernetes要求集群中的节点时间必须精确一直，这里使用chronyd服务从网络同步时间\n企业中建议配置内部的会见同步服务器\n# 启动chronyd服务 [root@master ~]# systemctl start chronyd [root@master ~]# systemctl enable chronyd [root@master ~]# date 2.6.4 禁用iptable和firewalld服务 kubernetes和docker 在运行的中会产生大量的iptables规则，为了不让系统规则跟它们混淆，直接关闭系统的规则\n# 1 关闭firewalld服务 [root@master ~]# systemctl stop firewalld [root@master ~]# systemctl disable firewalld # 2 关闭iptables服务 [root@master ~]# systemctl stop iptables [root@master ~]# systemctl disable iptables 2.6.5 禁用selinux selinux是linux系统下的一个安全服务，如果不关闭它，在安装集群中会产生各种各样的奇葩问题\n# 编辑 /etc/selinux/config 文件，修改SELINUX的值为disable # 注意修改完毕之后需要重启linux服务 SELINUX=disabled 2.6.6 禁用swap分区 swap分区指的是虚拟内存分区，它的作用是物理内存使用完，之后将磁盘空间虚拟成内存来使用，启用swap设备会对系统的性能产生非常负面的影响，因此kubernetes要求每个节点都要禁用swap设备，但是如果因为某些原因确实不能关闭swap分区，就需要在集群安装过程中通过明确的参数进行配置说明\n# 编辑分区配置文件/etc/fstab，注释掉swap分区一行 # 注意修改完毕之后需要重启linux服务 vim /etc/fstab 注释掉 /dev/mapper/centos-swap swap # /dev/mapper/centos-swap swap 2.6.7 修改linux的内核参数 # 修改linux的内核采纳数，添加网桥过滤和地址转发功能 # 编辑/etc/sysctl.d/kubernetes.conf文件，添加如下配置： net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 # 重新加载配置 [root@master ~]# sysctl -p # 加载网桥过滤模块 [root@master ~]# modprobe br_netfilter # 查看网桥过滤模块是否加载成功 [root@master ~]# lsmod | grep br_netfilter 2.6.8 配置ipvs功能 在Kubernetes中Service有两种带来模型，一种是基于iptables的，一种是基于ipvs的两者比较的话，ipvs的性能明显要高一些，但是如果要使用它，需要手动载入ipvs模块\n# 1.安装ipset和ipvsadm [root@master ~]# yum install ipset ipvsadm -y # 2.添加需要加载的模块写入脚本文件 [root@master ~]# cat \u0026lt;\u0026lt;EOF\u0026gt; /etc/sysconfig/modules/ipvs.modules #!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 EOF # 3.为脚本添加执行权限 [root@master ~]# chmod +x /etc/sysconfig/modules/ipvs.modules # 4.执行脚本文件 [root@master ~]# /bin/bash /etc/sysconfig/modules/ipvs.modules # 5.查看对应的模块是否加载成功 [root@master ~]# lsmod | grep -e ip_vs -e nf_conntrack_ipv4 2.6.9 安装docker # 1、切换镜像源 [root@master ~]# wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo # 2、查看当前镜像源中支持的docker版本 [root@master ~]# yum list docker-ce --showduplicates # 3、安装特定版本的docker-ce # 必须制定--setopt=obsoletes=0，否则yum会自动安装更高版本 [root@master ~]# yum install --setopt=obsoletes=0 docker-ce-18.06.3.ce-3.el7 -y # 4、添加一个配置文件 #Docker 在默认情况下使用Vgroup Driver为cgroupfs，而Kubernetes推荐使用systemd来替代cgroupfs [root@master ~]# mkdir /etc/docker [root@master ~]# cat \u0026lt;\u0026lt;EOF\u0026gt; /etc/docker/daemon.json { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://kn0t2bca.mirror.aliyuncs.com\u0026#34;] } EOF # 5、启动dokcer [root@master ~]# systemctl restart docker [root@master ~]# systemctl enable docker 2.6.10 安装Kubernetes组件 # 1、由于kubernetes的镜像在国外，速度比较慢，这里切换成国内的镜像源 # 2、编辑/etc/yum.repos.d/kubernetes.repo,添加下面的配置 [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgchech=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg # 3、安装kubeadm、kubelet和kubectl [root@master ~]# yum install --setopt=obsoletes=0 kubeadm-1.17.4-0 kubelet-1.17.4-0 kubectl-1.17.4-0 -y # 4、配置kubelet的cgroup #编辑/etc/sysconfig/kubelet, 添加下面的配置 KUBELET_CGROUP_ARGS=\u0026#34;--cgroup-driver=systemd\u0026#34; KUBE_PROXY_MODE=\u0026#34;ipvs\u0026#34; # 5、设置kubelet开机自启 [root@master ~]# systemctl enable kubelet 2.6.11 准备集群镜像 # 在安装kubernetes集群之前，必须要提前准备好集群需要的镜像，所需镜像可以通过下面命令查看 [root@master ~]# kubeadm config kubernetes.images list # 下载镜像 # 此镜像kubernetes的仓库中，由于网络原因，无法连接，下面提供了一种替换方案 kubernetes.images=( kube-apiserver:v1.17.4 kube-controller-manager:v1.17.4 kube-scheduler:v1.17.4 kube-proxy:v1.17.4 pause:3.1 etcd:3.4.3-0 coredns:1.6.5 ) for imageName in ${kubernetes.images[@]};do docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName done 2.6.11 集群初始化 下面的操作只需要在master节点上执行即可\n# 创建集群 [root@master ~]# kubeadm init \\ --apiserver-advertise-address=192.168.90.100 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version=v1.17.4 \\ --service-cidr=10.96.0.0/12 \\ --pod-network-cidr=10.244.0.0/16 # 创建必要文件 [root@master ~]# mkdir -p $HOME/.kube [root@master ~]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config [root@master ~]# sudo chown $(id -u):$(id -g) $HOME/.kube/config 下面的操作只需要在node节点上执行即可\nkubeadm join 192.168.0.100:6443 --token awk15p.t6bamck54w69u4s8 \\ --discovery-token-ca-cert-hash sha256:a94fa09562466d32d29523ab6cff122186f1127599fa4dcd5fa0152694f17117 在master上查看节点信息\n[root@master ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION master NotReady master 6m v1.17.4 node1 NotReady \u0026lt;none\u0026gt; 22s v1.17.4 node2 NotReady \u0026lt;none\u0026gt; 19s v1.17.4 2.6.13 安装网络插件，只在master节点操作即可 wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 由于外网不好访问，如果出现无法访问的情况，可以直接用下面的 记得文件名是kube-flannel.yml，位置：/root/kube-flannel.yml内容：\nhttps://github.com/flannel-io/flannel/tree/master/Documentation/kube-flannel.yml 也可手动拉取指定版本 docker pull quay.io/coreos/flannel:v0.14.0 #拉取flannel网络，三台主机 docker kubernetes.images #查看仓库是否拉去下来\n个人笔记 若是集群状态一直是 notready,用下面语句查看原因， journalctl -f -u kubelet.service 若原因是： cni.go:237] Unable to update cni config: no networks found in /etc/cni/net.d mkdir -p /etc/cni/net.d #创建目录给flannel做配置文件 vim /etc/cni/net.d/10-flannel.conf #编写配置文件\n{ \u0026#34;name\u0026#34;:\u0026#34;cbr0\u0026#34;, \u0026#34;cniVersion\u0026#34;:\u0026#34;0.3.1\u0026#34;, \u0026#34;type\u0026#34;:\u0026#34;flannel\u0026#34;, \u0026#34;deledate\u0026#34;:{ \u0026#34;hairpinMode\u0026#34;:true, \u0026#34;isDefaultGateway\u0026#34;:true } } 2.6.14 使用kubeadm reset重置集群 #在master节点之外的节点进行操作 kubeadm reset systemctl stop kubelet systemctl stop docker rm -rf /var/lib/cni/ rm -rf /var/lib/kubelet/* rm -rf /etc/cni/ ifconfig cni0 down ifconfig flannel.1 down ifconfig docker0 down ip link delete cni0 ip link delete flannel.1 ##重启kubelet systemctl restart kubelet ##重启docker systemctl restart docker 2.6.15 重启kubelet和docker # 重启kubelet systemctl restart kubelet # 重启docker systemctl restart docker 使用配置文件启动fannel\nkubectl apply -f kube-flannel.yml 等待它安装完毕 发现已经是 集群的状态已经是Ready\n2.6.16 kubeadm中的命令 # 生成 新的token [root@master ~]# kubeadm token create --print-join-command 2.7 集群测试 2.7.1 创建一个nginx服务 kubectl create deployment nginx --image=nginx:1.14-alpine 2.7.2 暴露端口 kubectl expose deploy nginx --port=80 --target-port=80 --type=NodePort 2.7.3 查看服务 kubectl get pod,svc 2.7.4 查看pod 浏览器测试结果：\n3. 资源管理 3.1 资源管理介绍 在kubernetes中，所有的内容都抽象为资源，用户需要通过操作资源来管理kubernetes。\nkubernetes的本质上就是一个集群系统，用户可以在集群中部署各种服务，所谓的部署服务，其实就是在kubernetes集群中运行一个个的容器，并将指定的程序跑在容器中。\nkubernetes的最小管理单元是pod而不是容器，所以只能将容器放在Pod中，而kubernetes一般也不会直接管理Pod，而是通过Pod控制器来管理Pod的。\nPod可以提供服务之后，就要考虑如何访问Pod中服务，kubernetes提供了Service资源实现这个功能。\n当然，如果Pod中程序的数据需要持久化，kubernetes还提供了各种存储系统。\n学习kubernetes的核心，就是学习如何对集群上的Pod、Pod控制器、Service、存储等各种资源进行操作\n3.2 YAML语言介绍 YAML是一个类似 XML、JSON 的标记性语言。它强调以数据为中心，并不是以标识语言为重点。因而YAML本身的定义比较简单，号称\u0026quot;一种人性化的数据格式语言\u0026quot;。\n\u0026lt;heima\u0026gt; \u0026lt;age\u0026gt;15\u0026lt;/age\u0026gt; \u0026lt;address\u0026gt;Beijing\u0026lt;/address\u0026gt; \u0026lt;/heima\u0026gt; heima: age: 15 address: Beijing YAML的语法比较简单，主要有下面几个：\n大小写敏感 使用缩进表示层级关系 缩进不允许使用tab，只允许空格( 低版本限制 ) 缩进的空格数不重要，只要相同层级的元素左对齐即可 \u0026lsquo;#\u0026lsquo;表示注释 YAML支持以下几种数据类型：\n纯量：单个的、不可再分的值 对象：键值对的集合，又称为映射（mapping）/ 哈希（hash） / 字典（dictionary） 数组：一组按次序排列的值，又称为序列（sequence） / 列表（list） # 纯量, 就是指的一个简单的值，字符串、布尔值、整数、浮点数、Null、时间、日期 # 1 布尔类型 c1: true (或者True) # 2 整型 c2: 234 # 3 浮点型 c3: 3.14 # 4 null类型 c4: ~ # 使用~表示null # 5 日期类型 c5: 2018-02-17 # 日期必须使用ISO 8601格式，即yyyy-MM-dd # 6 时间类型 c6: 2018-02-17T15:02:31+08:00 # 时间使用ISO 8601格式，时间和日期之间使用T连接，最后使用+代表时区 # 7 字符串类型 c7: heima # 简单写法，直接写值 , 如果字符串中间有特殊字符，必须使用双引号或者单引号包裹 c8: line1 line2 # 字符串过多的情况可以拆成多行，每一行会被转化成一个空格 # 对象 # 形式一(推荐): heima: age: 15 address: Beijing # 形式二(了解): heima: {age: 15,address: Beijing} # 数组 # 形式一(推荐): address: - 顺义 - 昌平 # 形式二(了解): address: [顺义,昌平] 小提示：\n1 书写yaml切记: 后面要加一个空格\n2 如果需要将多段yaml配置放在一个文件中，中间要使用---分隔\n3 下面是一个yaml转json的网站，可以通过它验证yaml是否书写正确\nhttps://www.json2yaml.com/convert-yaml-to-json\n3.3 资源管理方式 命令式对象管理：直接使用命令去操作kubernetes资源\nkubectl run nginx-pod --image=nginx:1.17.1 --port=80 命令式对象配置：通过命令配置和配置文件去操作kubernetes资源\nkubectl create/patch -f nginx-pod.yaml 声明式对象配置：通过apply命令和配置文件去操作kubernetes资源\nkubectl apply -f nginx-pod.yaml 类型 操作对象 适用环境 优点 缺点 命令式对象管理 对象 测试 简单 只能操作活动对象，无法审计、跟踪 命令式对象配置 文件 开发 可以审计、跟踪 项目大时，配置文件多，操作麻烦 声明式对象配置 目录 开发 支持目录操作 意外情况下难以调试 3.3.1 命令式对象管理 kubectl命令\nkubectl是kubernetes集群的命令行工具，通过它能够对集群本身进行管理，并能够在集群上进行容器化应用的安装部署。kubectl命令的语法如下：\nkubectl [command] [type] [name] [flags] comand：指定要对资源执行的操作，例如create、get、delete\ntype：指定资源类型，比如deployment、pod、service\nname：指定资源的名称，名称大小写敏感\nflags：指定额外的可选参数\n# 查看所有pod kubectl get pod # 查看某个pod kubectl get pod pod_name # 查看某个pod,以yaml格式展示结果 kubectl get pod pod_name -o yaml 资源类型\nkubernetes中所有的内容都抽象为资源，可以通过下面的命令进行查看:\nkubectl api-resources 经常使用的资源有下面这些：\n资源分类 资源名称 缩写 资源作用 集群级别资源 nodes no 集群组成部分 namespaces ns 隔离Pod pod资源 pods po 装载容器 pod资源控制器 replicationcontrollers rc 控制pod资源 replicasets rs 控制pod资源 deployments deploy 控制pod资源 daemonsets ds 控制pod资源 jobs 控制pod资源 cronjobs cj 控制pod资源 horizontalpodautoscalers hpa 控制pod资源 statefulsets sts 控制pod资源 服务发现资源 services svc 统一pod对外接口 ingress ing 统一pod对外接口 存储资源 volumeattachments 存储 persistentvolumes pv 存储 persistentvolumeclaims pvc 存储 配置资源 configmaps cm 配置 secrets 配置 操作\nkubernetes允许对资源进行多种操作，可以通过\u0026ndash;help查看详细的操作命令\nkubectl --help 经常使用的操作有下面这些：\n命令分类 命令 翻译 命令作用 基本命令 create 创建 创建一个资源 edit 编辑 编辑一个资源 get 获取 获取一个资源 patch 更新 更新一个资源 delete 删除 删除一个资源 explain 解释 展示资源文档 运行和调试 run 运行 在集群中运行一个指定的镜像 expose 暴露 暴露资源为Service describe 描述 显示资源内部信息 logs 日志输出容器在 pod 中的日志 输出容器在 pod 中的日志 attach 缠绕进入运行中的容器 进入运行中的容器 exec 执行容器中的一个命令 执行容器中的一个命令 cp 复制 在Pod内外复制文件 rollout 首次展示 管理资源的发布 scale 规模 扩(缩)容Pod的数量 autoscale 自动调整 自动调整Pod的数量 高级命令 apply rc 通过文件对资源进行配置 label 标签 更新资源上的标签 其他命令 cluster-info 集群信息 显示集群信息 version 版本 显示当前Server和Client的版本 下面以一个namespace / pod的创建和删除简单演示下命令的使用：\n# 创建一个namespace [root@master ~]# kubectl create namespace dev namespace/dev created # 获取namespace [root@master ~]# kubectl get ns NAME STATUS AGE default Active 21h dev Active 21s kube-node-lease Active 21h kube-public Active 21h kube-system Active 21h # 在此namespace下创建并运行一个nginx的Pod [root@master ~]# kubectl run pod --image=nginx:latest -n dev kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead. deployment.apps/pod created # 查看新创建的pod [root@master ~]# kubectl get pod -n dev NAME READY STATUS RESTARTS AGE pod 1/1 Running 0 21s # 删除指定的pod [root@master ~]# kubectl delete pod pod-864f9875b9-pcw7x pod \u0026#34;pod\u0026#34; deleted # 删除指定的namespace [root@master ~]# kubectl delete ns dev namespace \u0026#34;dev\u0026#34; deleted 3.3.2 命令式对象配置 命令式对象配置就是使用命令配合配置文件一起来操作kubernetes资源。\n1） 创建一个nginxpod.yaml，内容如下：\napiVersion: v1 kind: Namespace metadata: name: dev --- apiVersion: v1 kind: Pod metadata: name: nginxpod namespace: dev spec: containers: - name: nginx-containers image: nginx:latest 2）执行create命令，创建资源：\n[root@master ~]# kubectl create -f nginxpod.yaml namespace/dev created pod/nginxpod created 此时发现创建了两个资源对象，分别是namespace和pod\n3）执行get命令，查看资源：\n[root@master ~]# kubectl get -f nginxpod.yaml NAME STATUS AGE namespace/dev Active 18s NAME READY STATUS RESTARTS AGE pod/nginxpod 1/1 Running 0 17s 这样就显示了两个资源对象的信息\n4）执行delete命令，删除资源：\n[root@master ~]# kubectl delete -f nginxpod.yaml namespace \u0026#34;dev\u0026#34; deleted pod \u0026#34;nginxpod\u0026#34; deleted 此时发现两个资源对象被删除了\n总结: 命令式对象配置的方式操作资源，可以简单的认为：命令 + yaml配置文件（里面是命令需要的各种参数） 3.3.3 声明式对象配置 声明式对象配置跟命令式对象配置很相似，但是它只有一个命令apply。\n# 首先执行一次kubectl apply -f yaml文件，发现创建了资源 [root@master ~]# kubectl apply -f nginxpod.yaml namespace/dev created pod/nginxpod created # 再次执行一次kubectl apply -f yaml文件，发现说资源没有变动 [root@master ~]# kubectl apply -f nginxpod.yaml namespace/dev unchanged pod/nginxpod unchanged 总结: 其实声明式对象配置就是使用apply描述一个资源最终的状态（在yaml中定义状态） 使用apply操作资源： 如果资源不存在，就创建，相当于 kubectl create 如果资源已存在，就更新，相当于 kubectl patch 扩展：kubectl可以在node节点上运行吗 ?\nkubectl的运行是需要进行配置的，它的配置文件是$HOME/.kube，如果想要在node节点运行此命令，需要将master上的.kube文件复制到node节点上，即在master节点上执行下面操作：\nscp -r HOME/.kube node1: HOME/ 使用推荐: 三种方式应该怎么用 ?\n创建/更新资源 使用声明式对象配置 kubectl apply -f XXX.yaml\n删除资源 使用命令式对象配置 kubectl delete -f XXX.yaml\n查询资源 使用命令式对象管理 kubectl get(describe) 资源名称\n4. 实战入门 本章节将介绍如何在kubernetes集群中部署一个nginx服务，并且能够对其进行访问。\n4.1 Namespace Namespace是kubernetes系统中的一种非常重要资源，它的主要作用是用来实现多套环境的资源隔离或者多租户的资源隔离。\n默认情况下，kubernetes集群中的所有的Pod都是可以相互访问的。但是在实际中，可能不想让两个Pod之间进行互相的访问，那此时就可以将两个Pod划分到不同的namespace下。kubernetes通过将集群内部的资源分配到不同的Namespace中，可以形成逻辑上的\u0026quot;组\u0026quot;，以方便不同的组的资源进行隔离使用和管理。\n可以通过kubernetes的授权机制，将不同的namespace交给不同租户进行管理，这样就实现了多租户的资源隔离。此时还能结合kubernetes的资源配额机制，限定不同租户能占用的资源，例如CPU使用量、内存使用量等等，来实现租户可用资源的管理。\nkubernetes在集群启动之后，会默认创建几个namespace\n[root@master ~]# kubectl get namespace NAME STATUS AGE default Active 45h # 所有未指定Namespace的对象都会被分配在default命名空间 kube-node-lease Active 45h # 集群节点之间的心跳维护，v1.13开始引入 kube-public Active 45h # 此命名空间下的资源可以被所有人访问（包括未认证用户） kube-system Active 45h # 所有由Kubernetes系统创建的资源都处于这个命名空间 下面来看namespace资源的具体操作：\n4.1.1 查看 # 1 查看所有的ns 命令：kubectl get ns [root@master ~]# kubectl get ns NAME STATUS AGE default Active 45h kube-node-lease Active 45h kube-public Active 45h kube-system Active 45h # 2 查看指定的ns 命令：kubectl get ns ns名称 [root@master ~]# kubectl get ns default NAME STATUS AGE default Active 45h # 3 指定输出格式 命令：kubectl get ns ns名称 -o 格式参数 # kubernetes支持的格式有很多，比较常见的是wide、json、yaml [root@master ~]# kubectl get ns default -o yaml apiVersion: v1 kind: Namespace metadata: creationTimestamp: \u0026#34;2021-05-08T04:44:16Z\u0026#34; name: default resourceVersion: \u0026#34;151\u0026#34; selfLink: /api/v1/namespaces/default uid: 7405f73a-e486-43d4-9db6-145f1409f090 spec: finalizers: - kubernetes status: phase: Active # 4 查看ns详情 命令：kubectl describe ns ns名称 [root@master ~]# kubectl describe ns default Name: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Status: Active # Active 命名空间正在使用中 Terminating 正在删除命名空间 # ResourceQuota 针对namespace做的资源限制 # LimitRange针对namespace中的每个组件做的资源限制 No resource quota. No LimitRange resource. 4.1.2 创建 # 创建namespace [root@master ~]# kubectl create ns dev namespace/dev created 4.1.3 删除 # 删除namespace [root@master ~]# kubectl delete ns dev namespace \u0026#34;dev\u0026#34; deleted 4.1.4 配置方式 首先准备一个yaml文件：ns-dev.yaml\napiVersion: v1 kind: Namespace metadata: name: dev 然后就可以执行对应的创建和删除命令了：\n创建：kubectl create -f ns-dev.yaml\n删除：kubectl delete -f ns-dev.yaml\n4.2 Pod Pod是kubernetes集群进行管理的最小单元，程序要运行必须部署在容器中，而容器必须存在于Pod中。\nPod可以认为是容器的封装，一个Pod中可以存在一个或者多个容器。\nkubernetes在集群启动之后，集群中的各个组件也都是以Pod方式运行的。可以通过下面命令查看：\n[root@master ~]# kubectl get pod -n kube-system NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-6955765f44-68g6v 1/1 Running 0 2d1h kube-system coredns-6955765f44-cs5r8 1/1 Running 0 2d1h kube-system etcd-master 1/1 Running 0 2d1h kube-system kube-apiserver-master 1/1 Running 0 2d1h kube-system kube-controller-manager-master 1/1 Running 0 2d1h kube-system kube-flannel-ds-amd64-47r25 1/1 Running 0 2d1h kube-system kube-flannel-ds-amd64-ls5lh 1/1 Running 0 2d1h kube-system kube-proxy-685tk 1/1 Running 0 2d1h kube-system kube-proxy-87spt 1/1 Running 0 2d1h kube-system kube-scheduler-master 1/1 Running 0 2d1h 4.2.1 创建并运行 kubernetes没有提供单独运行Pod的命令，都是通过Pod控制器来实现的\n# 命令格式： kubectl run (pod控制器名称) [参数] # --image 指定Pod的镜像 # --port 指定端口 # --namespace 指定namespace [root@master ~]# kubectl run nginx --image=nginx:latest --port=80 --namespace dev deployment.apps/nginx created 4.2.2 查看pod信息 # 查看Pod基本信息 [root@master ~]# kubectl get pods -n dev NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 43s # 查看Pod的详细信息 [root@master ~]# kubectl describe pod nginx -n dev Name: nginx Namespace: dev Priority: 0 Node: node1/192.168.5.4 Start Time: Wed, 08 May 2021 09:29:24 +0800 Labels: pod-template-hash=5ff7956ff6 run=nginx Annotations: \u0026lt;none\u0026gt; Status: Running IP: 10.244.1.23 IPs: IP: 10.244.1.23 Controlled By: ReplicaSet/nginx Containers: nginx: Container ID: docker://4c62b8c0648d2512380f4ffa5da2c99d16e05634979973449c98e9b829f6253c Image: nginx:latest Image ID: docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 Port: 80/TCP Host Port: 0/TCP State: Running Started: Wed, 08 May 2021 09:30:01 +0800 Ready: True Restart Count: 0 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-hwvvw (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-hwvvw: Type: Secret (a volume populated by a Secret) SecretName: default-token-hwvvw Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled \u0026lt;unknown\u0026gt; default-scheduler Successfully assigned dev/nginx-5ff7956ff6-fg2db to node1 Normal Pulling 4m11s kubelet, node1 Pulling image \u0026#34;nginx:latest\u0026#34; Normal Pulled 3m36s kubelet, node1 Successfully pulled image \u0026#34;nginx:latest\u0026#34; Normal Created 3m36s kubelet, node1 Created container nginx Normal Started 3m36s kubelet, node1 Started container nginx 4.2.3 访问Pod # 获取podIP [root@master ~]# kubectl get pods -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE ... nginx 1/1 Running 0 190s 10.244.1.23 node1 ... #访问POD [root@master ~]# curl http://10.244.1.23:80 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 4.2.4 删除指定Pod # 删除指定Pod [root@master ~]# kubectl delete pod nginx -n dev pod \u0026#34;nginx\u0026#34; deleted # 此时，显示删除Pod成功，但是再查询，发现又新产生了一个 [root@master ~]# kubectl get pods -n dev NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 21s # 这是因为当前Pod是由Pod控制器创建的，控制器会监控Pod状况，一旦发现Pod死亡，会立即重建 # 此时要想删除Pod，必须删除Pod控制器 # 先来查询一下当前namespace下的Pod控制器 [root@master ~]# kubectl get deploy -n dev NAME READY UP-TO-DATE AVAILABLE AGE nginx 1/1 1 1 9m7s # 接下来，删除此PodPod控制器 [root@master ~]# kubectl delete deploy nginx -n dev deployment.apps \u0026#34;nginx\u0026#34; deleted # 稍等片刻，再查询Pod，发现Pod被删除了 [root@master ~]# kubectl get pods -n dev No resources found in dev namespace. 4.2.5 配置操作 创建一个pod-nginx.yaml，内容如下：\napiVersion: v1 kind: Pod metadata: name: nginx namespace: dev spec: containers: - image: nginx:latest name: pod ports: - name: nginx-port containerPort: 80 protocol: TCP 然后就可以执行对应的创建和删除命令了：\n创建：kubectl create -f pod-nginx.yaml\n删除：kubectl delete -f pod-nginx.yaml\n4.3 Label Label是kubernetes系统中的一个重要概念。它的作用就是在资源上添加标识，用来对它们进行区分和选择。\nLabel的特点：\n一个Label会以key/value键值对的形式附加到各种对象上，如Node、Pod、Service等等 一个资源对象可以定义任意数量的Label ，同一个Label也可以被添加到任意数量的资源对象上去 Label通常在资源对象定义时确定，当然也可以在对象创建后动态添加或者删除 可以通过Label实现资源的多维度分组，以便灵活、方便地进行资源分配、调度、配置、部署等管理工作。\n一些常用的Label 示例如下：\n版本标签：\u0026ldquo;version\u0026rdquo;:\u0026ldquo;release\u0026rdquo;, \u0026ldquo;version\u0026rdquo;:\u0026ldquo;stable\u0026rdquo;\u0026hellip;\u0026hellip; 环境标签：\u0026ldquo;environment\u0026rdquo;:\u0026ldquo;dev\u0026rdquo;，\u0026ldquo;environment\u0026rdquo;:\u0026ldquo;test\u0026rdquo;，\u0026ldquo;environment\u0026rdquo;:\u0026ldquo;pro\u0026rdquo; 架构标签：\u0026ldquo;tier\u0026rdquo;:\u0026ldquo;frontend\u0026rdquo;，\u0026ldquo;tier\u0026rdquo;:\u0026ldquo;backend\u0026rdquo; 标签定义完毕之后，还要考虑到标签的选择，这就要使用到Label Selector，即：\nLabel用于给某个资源对象定义标识\nLabel Selector用于查询和筛选拥有某些标签的资源对象\n当前有两种Label Selector：\n基于等式的Label Selector\nname = slave: 选择所有包含Label中key=\u0026ldquo;name\u0026quot;且value=\u0026ldquo;slave\u0026quot;的对象\nenv != production: 选择所有包括Label中的key=\u0026ldquo;env\u0026quot;且value不等于\u0026quot;production\u0026quot;的对象\n基于集合的Label Selector\nname in (master, slave): 选择所有包含Label中的key=\u0026ldquo;name\u0026quot;且value=\u0026ldquo;master\u0026quot;或\u0026quot;slave\u0026quot;的对象\nname not in (frontend): 选择所有包含Label中的key=\u0026ldquo;name\u0026quot;且value不等于\u0026quot;frontend\u0026quot;的对象\n标签的选择条件可以使用多个，此时将多个Label Selector进行组合，使用逗号\u0026rdquo;,\u0026ldquo;进行分隔即可。例如：\nname=slave，env!=production\nname not in (frontend)，env!=production\n4.3.1 命令方式 # 为pod资源打标签 [root@master ~]# kubectl label pod nginx-pod version=1.0 -n dev pod/nginx-pod labeled # 为pod资源更新标签 [root@master ~]# kubectl label pod nginx-pod version=2.0 -n dev --overwrite pod/nginx-pod labeled # 查看标签 [root@master ~]# kubectl get pod nginx-pod -n dev --show-labels NAME READY STATUS RESTARTS AGE LABELS nginx-pod 1/1 Running 0 10m version=2.0 # 筛选标签 [root@master ~]# kubectl get pod -n dev -l version=2.0 --show-labels NAME READY STATUS RESTARTS AGE LABELS nginx-pod 1/1 Running 0 17m version=2.0 [root@master ~]# kubectl get pod -n dev -l version!=2.0 --show-labels No resources found in dev namespace. #删除标签 [root@master ~]# kubectl label pod nginx-pod -n dev tier- pod/nginx unlabeled 4.3.2 配置方式 apiVersion: v1 kind: Pod metadata: name: nginx namespace: dev labels: version: \u0026#34;3.0\u0026#34; env: \u0026#34;test\u0026#34; spec: containers: - image: nginx:latest name: pod ports: - name: nginx-port containerPort: 80 protocol: TCP 然后就可以执行对应的更新命令了：kubectl apply -f pod-nginx.yaml\n4.4 Deployment 在kubernetes中，Pod是最小的控制单元，但是kubernetes很少直接控制Pod，一般都是通过Pod控制器来完成的。Pod控制器用于pod的管理，确保pod资源符合预期的状态，当pod的资源出现故障时，会尝试进行重启或重建pod。\n在kubernetes中Pod控制器的种类有很多，本章节只介绍一种：Deployment。\n4.4.1 命令操作 # 命令格式: kubectl create deployment 名称 [参数] # --image 指定pod的镜像 # --port 指定端口 # --replicas 指定创建pod数量 # --namespace 指定namespace [root@master ~]# kubectl run nginx --image=nginx:latest --port=80 --replicas=3 -n dev deployment.apps/nginx created # 查看创建的Pod [root@master ~]# kubectl get pods -n dev NAME READY STATUS RESTARTS AGE nginx-5ff7956ff6-6k8cb 1/1 Running 0 19s nginx-5ff7956ff6-jxfjt 1/1 Running 0 19s nginx-5ff7956ff6-v6jqw 1/1 Running 0 19s # 查看deployment的信息 [root@master ~]# kubectl get deploy -n dev NAME READY UP-TO-DATE AVAILABLE AGE nginx 3/3 3 3 2m42s # UP-TO-DATE：成功升级的副本数量 # AVAILABLE：可用副本的数量 [root@master ~]# kubectl get deploy -n dev -o wide NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR nginx 3/3 3 3 2m51s nginx nginx:latest run=nginx # 查看deployment的详细信息 [root@master ~]# kubectl describe deploy nginx -n dev Name: nginx Namespace: dev CreationTimestamp: Wed, 08 May 2021 11:14:14 +0800 Labels: run=nginx Annotations: deployment.kubernetes.io/revision: 1 Selector: run=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max 违规词汇 Pod Template: Labels: run=nginx Containers: nginx: Image: nginx:latest Port: 80/TCP Host Port: 0/TCP Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: nginx-5ff7956ff6 (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 5m43s deployment-controller Scaled up replicaset nginx-5ff7956ff6 to 3 # 删除 [root@master ~]# kubectl delete deploy nginx -n dev deployment.apps \u0026#34;nginx\u0026#34; deleted 4.4.2 配置操作 创建一个deploy-nginx.yaml，内容如下：\napiVersion: apps/v1 kind: Deployment metadata: name: nginx namespace: dev spec: replicas: 3 selector: matchLabels: run: nginx template: metadata: labels: run: nginx spec: containers: - image: nginx:latest name: nginx ports: - containerPort: 80 protocol: TCP 然后就可以执行对应的创建和删除命令了：\n创建：kubectl create -f deploy-nginx.yaml\n删除：kubectl delete -f deploy-nginx.yaml\n4.5 Service 通过上节课的学习，已经能够利用Deployment来创建一组Pod来提供具有高可用性的服务。\n虽然每个Pod都会分配一个单独的Pod IP，然而却存在如下两问题：\nPod IP 会随着Pod的重建产生变化 Pod IP 仅仅是集群内可见的虚拟IP，外部无法访问 这样对于访问这个服务带来了难度。因此，kubernetes设计了Service来解决这个问题。\nService可以看作是一组同类Pod对外的访问接口。借助Service，应用可以方便地实现服务发现和负载均衡。\n4.5.1 创建集群内部可访问的Service # 暴露Service [root@master ~]# kubectl expose deploy nginx --name=svc-nginx1 --type=ClusterIP --port=80 --target-port=80 -n dev service/svc-nginx1 exposed # 查看service [root@master ~]# kubectl get svc svc-nginx1 -n dev -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR svc-nginx1 ClusterIP 10.109.179.231 \u0026lt;none\u0026gt; 80/TCP 3m51s run=nginx # 这里产生了一个CLUSTER-IP，这就是service的IP，在Service的生命周期中，这个地址是不会变动的 # 可以通过这个IP访问当前service对应的POD [root@master ~]# curl 10.109.179.231:80 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; ....... \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 4.5.2 创建集群外部也可访问的Service # 上面创建的Service的type类型为ClusterIP，这个ip地址只用集群内部可访问 # 如果需要创建外部也可以访问的Service，需要修改type为NodePort [root@master ~]# kubectl expose deploy nginx --name=svc-nginx2 --type=NodePort --port=80 --target-port=80 -n dev service/svc-nginx2 exposed # 此时查看，会发现出现了NodePort类型的Service，而且有一对Port（80:31928/TC） [root@master ~]# kubectl get svc svc-nginx2 -n dev -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR svc-nginx2 NodePort 10.100.94.0 \u0026lt;none\u0026gt; 80:31928/TCP 9s run=nginx # 接下来就可以通过集群外的主机访问 节点IP:31928访问服务了 # 例如在的电脑主机上通过浏览器访问下面的地址 http://192.168.90.100:31928/ 4.5.3 删除Service [root@master ~]# kubectl delete svc svc-nginx-1 -n dev service \u0026#34;svc-nginx-1\u0026#34; deleted 4.5.4 配置方式 创建一个svc-nginx.yaml，内容如下：\napiVersion: v1 kind: Service metadata: name: svc-nginx namespace: dev spec: clusterIP: 10.109.179.231 #固定svc的内网ip ports: - port: 80 protocol: TCP targetPort: 80 selector: run: nginx type: ClusterIP 然后就可以执行对应的创建和删除命令了：\n创建：kubectl create -f svc-nginx.yaml\n删除：kubectl delete -f svc-nginx.yaml\n小结\n至此，已经掌握了Namespace、Pod、Deployment、Service资源的基本操作，有了这些操作，就可以在kubernetes集群中实现一个服务的简单部署和访问了，但是如果想要更好的使用kubernetes，就需要深入学习这几种资源的细节和原理。\n5. Pod详解 5.1 Pod介绍 5.1.1 Pod结构 每个Pod中都可以包含一个或者多个容器，这些容器可以分为两类：\n用户程序所在的容器，数量可多可少\nPause容器，这是每个Pod都会有的一个根容器，它的作用有两个：\n可以以它为依据，评估整个Pod的健康状态\n可以在根容器上设置Ip地址，其它容器都此Ip（Pod IP），以实现Pod内部的网路通信\n这里是Pod内部的通讯，Pod的之间的通讯采用虚拟二层网络技术来实现，我们当前环境用的是Flannel 5.1.2 Pod定义 下面是Pod的资源清单：\napiVersion: v1 #必选，版本号，例如v1 kind: Pod #必选，资源类型，例如 Pod metadata: #必选，元数据 name: string #必选，Pod名称 namespace: string #Pod所属的命名空间,默认为\u0026#34;default\u0026#34; labels: #自定义标签列表 - name: string spec: #必选，Pod中容器的详细定义 containers: #必选，Pod中容器列表 - name: string #必选，容器名称 image: string #必选，容器的镜像名称 imagePullPolicy: [ Always|Never|IfNotPresent ] #获取镜像的策略 command: [string] #容器的启动命令列表，如不指定，使用打包时使用的启动命令 args: [string] #容器的启动命令参数列表 workingDir: string #容器的工作目录 volumeMounts: #挂载到容器内部的存储卷配置 - name: string #引用pod定义的共享存储卷的名称，需用volumes[]部分定义的的卷名 mountPath: string #存储卷在容器内mount的绝对路径，应少于512字符 readOnly: boolean #是否为只读模式 ports: #需要暴露的端口库号列表 - name: string #端口的名称 containerPort: int #容器需要监听的端口号 hostPort: int #容器所在主机需要监听的端口号，默认与Container相同 protocol: string #端口协议，支持TCP和UDP，默认TCP env: #容器运行前需设置的环境变量列表 - name: string #环境变量名称 value: string #环境变量的值 resources: #资源限制和请求的设置 limits: #资源限制的设置 cpu: string #Cpu的限制，单位为core数，将用于docker run --cpu-shares参数 memory: string #内存限制，单位可以为Mib/Gib，将用于docker run --memory参数 requests: #资源请求的设置 cpu: string #Cpu请求，容器启动的初始可用数量 memory: string #内存请求,容器启动的初始可用数量 lifecycle: #生命周期钩子 postStart: #容器启动后立即执行此钩子,如果执行失败,会根据重启策略进行重启 preStop: #容器终止前执行此钩子,无论结果如何,容器都会终止 livenessProbe: #对Pod内各容器健康检查的设置，当探测无响应几次后将自动重启该容器 exec: #对Pod容器内检查方式设置为exec方式 command: [string] #exec方式需要制定的命令或脚本 httpGet: #对Pod内个容器健康检查方法设置为HttpGet，需要制定Path、port path: string port: number host: string scheme: string HttpHeaders: - name: string value: string tcpSocket: #对Pod内个容器健康检查方式设置为tcpSocket方式 port: number initialDelaySeconds: 0 #容器启动完成后首次探测的时间，单位为秒 timeoutSeconds: 0 #对容器健康检查探测等待响应的超时时间，单位秒，默认1秒 periodSeconds: 0 #对容器监控检查的定期探测时间设置，单位秒，默认10秒一次 successThreshold: 0 failureThreshold: 0 securityContext: privileged: false restartPolicy: [Always | Never | OnFailure] #Pod的重启策略 nodeName: \u0026lt;string\u0026gt; #设置NodeName表示将该Pod调度到指定到名称的node节点上 nodeSelector: obeject #设置NodeSelector表示将该Pod调度到包含这个label的node上 imagePullSecrets: #Pull镜像时使用的secret名称，以key：secretkey格式指定 - name: string hostNetwork: false #是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络 volumes: #在该pod上定义共享存储卷列表 - name: string #共享存储卷名称 （volumes类型有很多种） emptyDir: {} #类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。为空值 hostPath: string #类型为hostPath的存储卷，表示挂载Pod所在宿主机的目录 path: string #Pod所在宿主机的目录，将被用于同期中mount的目录 secret: #类型为secret的存储卷，挂载集群与定义的secret对象到容器内部 scretname: string items: - key: string path: string configMap: #类型为configMap的存储卷，挂载预定义的configMap对象到容器内部 name: string items: - key: string path: string #小提示： # 在这里，可通过一个命令来查看每种资源的可配置项 # kubectl explain 资源类型 查看某种资源可以配置的一级属性 # kubectl explain 资源类型.属性 查看属性的子属性 [root@k8s-master01 ~]# kubectl explain pod KIND: Pod VERSION: v1 FIELDS: apiVersion \u0026lt;string\u0026gt; kind \u0026lt;string\u0026gt; metadata \u0026lt;Object\u0026gt; spec \u0026lt;Object\u0026gt; status \u0026lt;Object\u0026gt; [root@k8s-master01 ~]# kubectl explain pod.metadata KIND: Pod VERSION: v1 RESOURCE: metadata \u0026lt;Object\u0026gt; FIELDS: annotations \u0026lt;map[string]string\u0026gt; clusterName \u0026lt;string\u0026gt; creationTimestamp \u0026lt;string\u0026gt; deletionGracePeriodSeconds \u0026lt;integer\u0026gt; deletionTimestamp \u0026lt;string\u0026gt; finalizers \u0026lt;[]string\u0026gt; generateName \u0026lt;string\u0026gt; generation \u0026lt;integer\u0026gt; labels \u0026lt;map[string]string\u0026gt; managedFields \u0026lt;[]Object\u0026gt; name \u0026lt;string\u0026gt; namespace \u0026lt;string\u0026gt; ownerReferences \u0026lt;[]Object\u0026gt; resourceVersion \u0026lt;string\u0026gt; selfLink \u0026lt;string\u0026gt; uid \u0026lt;string\u0026gt; 在kubernetes中基本所有资源的一级属性都是一样的，主要包含5部分：\napiVersion 版本，由kubernetes内部定义，版本号必须可以用 kubectl api-versions 查询到 kind 类型，由kubernetes内部定义，版本号必须可以用 kubectl api-resources 查询到 metadata 元数据，主要是资源标识和说明，常用的有name、namespace、labels等 spec 描述，这是配置中最重要的一部分，里面是对各种资源配置的详细描述 status 状态信息，里面的内容不需要定义，由kubernetes自动生成 在上面的属性中，spec是接下来研究的重点，继续看下它的常见子属性:\ncontainers \u0026lt;[]Object\u0026gt; 容器列表，用于定义容器的详细信息 nodeName 根据nodeName的值将pod调度到指定的Node节点上 nodeSelector \u0026lt;map[]\u0026gt; 根据NodeSelector中定义的信息选择将该Pod调度到包含这些label的Node 上 hostNetwork 是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络 volumes \u0026lt;[]Object\u0026gt; 存储卷，用于定义Pod上面挂在的存储信息 restartPolicy 重启策略，表示Pod在遇到故障的时候的处理策略 5.2 Pod配置 本小节主要来研究pod.spec.containers属性，这也是pod配置中最为关键的一项配置。\n[root@k8s-master01 ~]# kubectl explain pod.spec.containers KIND: Pod VERSION: v1 RESOURCE: containers \u0026lt;[]Object\u0026gt; # 数组，代表可以有多个容器 FIELDS: name \u0026lt;string\u0026gt; # 容器名称 image \u0026lt;string\u0026gt; # 容器需要的镜像地址 imagePullPolicy \u0026lt;string\u0026gt; # 镜像拉取策略 command \u0026lt;[]string\u0026gt; # 容器的启动命令列表，如不指定，使用打包时使用的启动命令 args \u0026lt;[]string\u0026gt; # 容器的启动命令需要的参数列表 env \u0026lt;[]Object\u0026gt; # 容器环境变量的配置 ports \u0026lt;[]Object\u0026gt; # 容器需要暴露的端口号列表 resources \u0026lt;Object\u0026gt; # 资源限制和资源请求的设置 5.2.1 基本配置 创建pod-base.yaml文件，内容如下：\napiVersion: v1 kind: Pod metadata: name: pod-base namespace: dev labels: user: heima spec: containers: - name: nginx image: nginx:1.17.1 - name: busybox image: busybox:1.30 上面定义了一个比较简单Pod的配置，里面有两个容器：\nnginx：用1.17.1版本的nginx镜像创建，（nginx是一个轻量级web容器） busybox：用1.30版本的busybox镜像创建，（busybox是一个小巧的linux命令集合） # 创建Pod [root@k8s-master01 pod]# kubectl apply -f pod-base.yaml pod/pod-base created # 查看Pod状况 # READY 1/2 : 表示当前Pod中有2个容器，其中1个准备就绪，1个未就绪 # RESTARTS : 重启次数，因为有1个容器故障了，Pod一直在重启试图恢复它 [root@k8s-master01 pod]# kubectl get pod -n dev NAME READY STATUS RESTARTS AGE pod-base 1/2 Running 4 95s # 可以通过describe查看内部的详情 # 此时已经运行起来了一个基本的Pod，虽然它暂时有问题 [root@k8s-master01 pod]# kubectl describe pod pod-base -n dev 5.2.2 镜像拉取 创建pod-imagepullpolicy.yaml文件，内容如下：\napiVersion: v1 kind: Pod metadata: name: pod-imagepullpolicy namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 imagePullPolicy: Never # 用于设置镜像拉取策略 - name: busybox image: busybox:1.30 imagePullPolicy，用于设置镜像拉取策略，kubernetes支持配置三种拉取策略：\nAlways：总是从远程仓库拉取镜像（一直远程下载） IfNotPresent：本地有则使用本地镜像，本地没有则从远程仓库拉取镜像（本地有就本地 本地没远程下载） Never：只使用本地镜像，从不去远程仓库拉取，本地没有就报错 （一直使用本地） 默认值说明：\n如果镜像tag为具体版本号， 默认策略是：IfNotPresent\n如果镜像tag为：latest（最终版本） ，默认策略是always\n# 创建Pod [root@k8s-master01 pod]# kubectl create -f pod-imagepullpolicy.yaml pod/pod-imagepullpolicy created # 查看Pod详情 # 此时明显可以看到nginx镜像有一步Pulling image \u0026#34;nginx:1.17.1\u0026#34;的过程 [root@k8s-master01 pod]# kubectl describe pod pod-imagepullpolicy -n dev ...... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled \u0026lt;unknown\u0026gt; default-scheduler Successfully assigned dev/pod-imagePullPolicy to node1 Normal Pulling 32s kubelet, node1 Pulling image \u0026#34;nginx:1.17.1\u0026#34; Normal Pulled 26s kubelet, node1 Successfully pulled image \u0026#34;nginx:1.17.1\u0026#34; Normal Created 26s kubelet, node1 Created container nginx Normal Started 25s kubelet, node1 Started container nginx Normal Pulled 7s (x3 over 25s) kubelet, node1 Container image \u0026#34;busybox:1.30\u0026#34; already present on machine Normal Created 7s (x3 over 25s) kubelet, node1 Created container busybox Normal Started 7s (x3 over 25s) kubelet, node1 Started container busybox 5.2.3 启动命令 在前面的案例中，一直有一个问题没有解决，就是的busybox容器一直没有成功运行，那么到底是什么原因导致这个容器的故障呢？\n原来busybox并不是一个程序，而是类似于一个工具类的集合，kubernetes集群启动管理后，它会自动关闭。解决方法就是让其一直在运行，这就用到了command配置。\n创建pod-command.yaml文件，内容如下：\napiVersion: v1 kind: Pod metadata: name: pod-command namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 - name: busybox image: busybox:1.30 command: [\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;touch /tmp/hello.txt;while true;do /bin/echo $(date +%T) \u0026gt;\u0026gt; /tmp/hello.txt; sleep 3; done;\u0026#34;] command，用于在pod中的容器初始化完毕之后运行一个命令。\n稍微解释下上面命令的意思：\n\u0026ldquo;/bin/sh\u0026rdquo;,\u0026quot;-c\u0026rdquo;, 使用sh执行命令\ntouch /tmp/hello.txt; 创建一个/tmp/hello.txt 文件\nwhile true;do /bin/echo $(date +%T) \u0026raquo; /tmp/hello.txt; sleep 3; done; 每隔3秒向文件中写入当前时间\n# 创建Pod [root@k8s-master01 pod]# kubectl create -f pod-command.yaml pod/pod-command created # 查看Pod状态 # 此时发现两个pod都正常运行了 [root@k8s-master01 pod]# kubectl get pods pod-command -n dev NAME READY STATUS RESTARTS AGE pod-command 2/2 Runing 0 2s # 进入pod中的busybox容器，查看文件内容 # 补充一个命令: kubectl exec pod名称 -n 命名空间 -it -c 容器名称 /bin/sh 在容器内部执行命令 # 使用这个命令就可以进入某个容器的内部，然后进行相关操作了 # 比如，可以查看txt文件的内容 [root@k8s-master01 pod]# kubectl exec pod-command -n dev -it -c busybox /bin/sh / # tail -f /tmp/hello.txt 14:44:19 14:44:22 14:44:25 特别说明： 通过上面发现command已经可以完成启动命令和传递参数的功能，为什么这里还要提供一个args选项，用于传递参数呢?这其实跟docker有点关系，kubernetes中的command、args两项其实是实现覆盖Dockerfile中ENTRYPOINT的功能。 1 如果command和args均没有写，那么用Dockerfile的配置。 2 如果command写了，但args没有写，那么Dockerfile默认的配置会被忽略，执行输入的command 3 如果command没写，但args写了，那么Dockerfile中配置的ENTRYPOINT的命令会被执行，使用当前args的参数 4 如果command和args都写了，那么Dockerfile的配置被忽略，执行command并追加上args参数 5.2.4 环境变量 创建pod-env.yaml文件，内容如下：\napiVersion: v1 kind: Pod metadata: name: pod-env namespace: dev spec: containers: - name: busybox image: busybox:1.30 command: [\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;while true;do /bin/echo $(date +%T);sleep 60; done;\u0026#34;] env: # 设置环境变量列表 - name: \u0026#34;username\u0026#34; value: \u0026#34;admin\u0026#34; - name: \u0026#34;password\u0026#34; value: \u0026#34;123456\u0026#34; env，环境变量，用于在pod中的容器设置环境变量。\n# 创建Pod [root@k8s-master01 ~]# kubectl create -f pod-env.yaml pod/pod-env created # 进入容器，输出环境变量 [root@k8s-master01 ~]# kubectl exec pod-env -n dev -c busybox -it /bin/sh / # echo $username admin / # echo $password 123456 这种方式不是很推荐，推荐将这些配置单独存储在配置文件中，这种方式将在后面介绍。\n5.2.5 端口设置 本小节来介绍容器的端口设置，也就是containers的ports选项。\n首先看下ports支持的子选项：\n[root@k8s-master01 ~]# kubectl explain pod.spec.containers.ports KIND: Pod VERSION: v1 RESOURCE: ports \u0026lt;[]Object\u0026gt; FIELDS: name \u0026lt;string\u0026gt; # 端口名称，如果指定，必须保证name在pod中是唯一的\tcontainerPort\u0026lt;integer\u0026gt; # 容器要监听的端口(0\u0026lt;x\u0026lt;65536) hostPort \u0026lt;integer\u0026gt; # 容器要在主机上公开的端口，如果设置，主机上只能运行容器的一个副本(一般省略) hostIP \u0026lt;string\u0026gt; # 要将外部端口绑定到的主机IP(一般省略) protocol \u0026lt;string\u0026gt; # 端口协议。必须是UDP、TCP或SCTP。默认为“TCP”。 接下来，编写一个测试案例，创建pod-ports.yaml\napiVersion: v1 kind: Pod metadata: name: pod-ports namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 ports: # 设置容器暴露的端口列表 - name: nginx-port containerPort: 80 protocol: TCP # 创建Pod [root@k8s-master01 ~]# kubectl create -f pod-ports.yaml pod/pod-ports created # 查看pod # 在下面可以明显看到配置信息 [root@k8s-master01 ~]# kubectl get pod pod-ports -n dev -o yaml ...... spec: containers: - image: nginx:1.17.1 imagePullPolicy: IfNotPresent name: nginx ports: - containerPort: 80 name: nginx-port protocol: TCP ...... 访问容器中的程序需要使用的是Podip:containerPort\n5.2.6 资源配额 容器中的程序要运行，肯定是要占用一定资源的，比如cpu和内存等，如果不对某个容器的资源做限制，那么它就可能吃掉大量资源，导致其它容器无法运行。针对这种情况，kubernetes提供了对内存和cpu的资源进行配额的机制，这种机制主要通过resources选项实现，他有两个子选项：\nlimits：用于限制运行时容器的最大占用资源，当容器占用资源超过limits时会被终止，并进行重启 requests ：用于设置容器需要的最小资源，如果环境资源不够，容器将无法启动 可以通过上面两个选项设置资源的上下限。\n接下来，编写一个测试案例，创建pod-resources.yaml\napiVersion: v1 kind: Pod metadata: name: pod-resources namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 resources: # 资源配额 limits: # 限制资源（上限） cpu: \u0026#34;2\u0026#34; # CPU限制，单位是core数 memory: \u0026#34;10Gi\u0026#34; # 内存限制 requests: # 请求资源（下限） cpu: \u0026#34;1\u0026#34; # CPU限制，单位是core数 memory: \u0026#34;10Mi\u0026#34; # 内存限制 在这对cpu和memory的单位做一个说明：\ncpu：core数，可以为整数或小数 memory： 内存大小，可以使用Gi、Mi、G、M等形式 # 运行Pod [root@k8s-master01 ~]# kubectl create -f pod-resources.yaml pod/pod-resources created # 查看发现pod运行正常 [root@k8s-master01 ~]# kubectl get pod pod-resources -n dev NAME READY STATUS RESTARTS AGE pod-resources 1/1 Running 0 39s # 接下来，停止Pod [root@k8s-master01 ~]# kubectl delete -f pod-resources.yaml pod \u0026#34;pod-resources\u0026#34; deleted # 编辑pod，修改resources.requests.memory的值为10Gi [root@k8s-master01 ~]# vim pod-resources.yaml # 再次启动pod [root@k8s-master01 ~]# kubectl create -f pod-resources.yaml pod/pod-resources created # 查看Pod状态，发现Pod启动失败 [root@k8s-master01 ~]# kubectl get pod pod-resources -n dev -o wide NAME READY STATUS RESTARTS AGE pod-resources 0/1 Pending 0 20s # 查看pod详情会发现，如下提示 [root@k8s-master01 ~]# kubectl describe pod pod-resources -n dev ...... Warning FailedScheduling 35s default-scheduler 0/3 nodes are available: 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn\u0026#39;t tolerate, 2 Insufficient memory.(内存不足) 5.3 Pod生命周期 我们一般将pod对象从创建至终的这段时间范围称为pod的生命周期，它主要包含下面的过程：\npod创建过程 运行初始化容器（init container）过程 运行主容器（main container） 容器启动后钩子（post start）、容器终止前钩子（pre stop） 容器的存活性探测（liveness probe）、就绪性探测（readiness probe） pod终止过程 在整个生命周期中，Pod会出现5种状态（相位），分别如下：\n挂起（Pending）：apiserver已经创建了pod资源对象，但它尚未被调度完成或者仍处于下载镜像的过程中 运行中（Running）：pod已经被调度至某节点，并且所有容器都已经被kubelet创建完成 成功（Succeeded）：pod中的所有容器都已经成功终止并且不会被重启 失败（Failed）：所有容器都已经终止，但至少有一个容器终止失败，即容器返回了非0值的退出状态 未知（Unknown）：apiserver无法正常获取到pod对象的状态信息，通常由网络通信失败所导致 5.3.1 创建和终止 pod的创建过程\n用户通过kubectl或其他api客户端提交需要创建的pod信息给apiServer\napiServer开始生成pod对象的信息，并将信息存入etcd，然后返回确认信息至客户端\napiServer开始反映etcd中的pod对象的变化，其它组件使用watch机制来跟踪检查apiServer上的变动\nscheduler发现有新的pod对象要创建，开始为Pod分配主机并将结果信息更新至apiServer\nnode节点上的kubelet发现有pod调度过来，尝试调用docker启动容器，并将结果回送至apiServer\napiServer将接收到的pod状态信息存入etcd中\npod的终止过程\n用户向apiServer发送删除pod对象的命令 apiServcer中的pod对象信息会随着时间的推移而更新，在宽限期内（默认30s），pod被视为dead 将pod标记为terminating状态 kubelet在监控到pod对象转为terminating状态的同时启动pod关闭过程 端点控制器监控到pod对象的关闭行为时将其从所有匹配到此端点的service资源的端点列表中移除 如果当前pod对象定义了preStop钩子处理器，则在其标记为terminating后即会以同步的方式启动执行 pod对象中的容器进程收到停止信号 宽限期结束后，若pod中还存在仍在运行的进程，那么pod对象会收到立即终止的信号 kubelet请求apiServer将此pod资源的宽限期设置为0从而完成删除操作，此时pod对于用户已不可见 5.3.2 初始化容器 初始化容器是在pod的主容器启动之前要运行的容器，主要是做一些主容器的前置工作，它具有两大特征：\n初始化容器必须运行完成直至结束，若某初始化容器运行失败，那么kubernetes需要重启它直到成功完成 初始化容器必须按照定义的顺序执行，当且仅当前一个成功之后，后面的一个才能运行 初始化容器有很多的应用场景，下面列出的是最常见的几个：\n提供主容器镜像中不具备的工具程序或自定义代码 初始化容器要先于应用容器串行启动并运行完成，因此可用于延后应用容器的启动直至其依赖的条件得到满足 接下来做一个案例，模拟下面这个需求：\n假设要以主容器来运行nginx，但是要求在运行nginx之前先要能够连接上mysql和redis所在服务器\n为了简化测试，事先规定好mysql(192.168.90.14)和redis(192.168.90.15)服务器的地址\n创建pod-initcontainer.yaml，内容如下：\napiVersion: v1 kind: Pod metadata: name: pod-initcontainer namespace: dev spec: containers: - name: main-container image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 initContainers: - name: test-mysql image: busybox:1.30 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;until ping 192.168.90.14 -c 1 ; do echo waiting for mysql...; sleep 2; done;\u0026#39;] - name: test-redis image: busybox:1.30 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;until ping 192.168.90.15 -c 1 ; do echo waiting for reids...; sleep 2; done;\u0026#39;] # 创建pod [root@k8s-master01 ~]# kubectl create -f pod-initcontainer.yaml pod/pod-initcontainer created # 查看pod状态 # 发现pod卡在启动第一个初始化容器过程中，后面的容器不会运行 root@k8s-master01 ~]# kubectl describe pod pod-initcontainer -n dev ........ Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 49s default-scheduler Successfully assigned dev/pod-initcontainer to node1 Normal Pulled 48s kubelet, node1 Container image \u0026#34;busybox:1.30\u0026#34; already present on machine Normal Created 48s kubelet, node1 Created container test-mysql Normal Started 48s kubelet, node1 Started container test-mysql # 动态查看pod [root@k8s-master01 ~]# kubectl get pods pod-initcontainer -n dev -w NAME READY STATUS RESTARTS AGE pod-initcontainer 0/1 Init:0/2 0 15s pod-initcontainer 0/1 Init:1/2 0 52s pod-initcontainer 0/1 Init:1/2 0 53s pod-initcontainer 0/1 PodInitializing 0 89s pod-initcontainer 1/1 Running 0 90s # 接下来新开一个shell，为当前服务器新增两个ip，观察pod的变化 [root@k8s-master01 ~]# ifconfig ens33:1 192.168.90.14 netmask 255.255.255.0 up [root@k8s-master01 ~]# ifconfig ens33:2 192.168.90.15 netmask 255.255.255.0 up 5.3.3 钩子函数 钩子函数能够感知自身生命周期中的事件，并在相应的时刻到来时运行用户指定的程序代码。\nkubernetes在主容器的启动之后和停止之前提供了两个钩子函数：\npost start：容器创建之后执行，如果失败了会重启容器 pre stop ：容器终止之前执行，执行完成之后容器将成功终止，在其完成之前会阻塞删除容器的操作 钩子处理器支持使用下面三种方式定义动作：\nExec命令：在容器内执行一次命令\n…… lifecycle: postStart: exec: command: - cat - /tmp/healthy …… TCPSocket：在当前容器尝试访问指定的socket\n…… lifecycle: postStart: tcpSocket: port: 8080 …… HTTPGet：在当前容器中向某url发起http请求\n…… lifecycle: postStart: httpGet: path: / #URI地址 port: 80 #端口号 host: 192.168.5.3 #主机地址 scheme: HTTP #支持的协议，http或者https …… 接下来，以exec方式为例，演示下钩子函数的使用，创建pod-hook-exec.yaml文件，内容如下：\napiVersion: v1 kind: Pod metadata: name: pod-hook-exec namespace: dev spec: containers: - name: main-container image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 lifecycle: postStart: exec: # 在容器启动的时候执行一个命令，修改掉nginx的默认首页内容 command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo postStart... \u0026gt; /usr/share/nginx/html/index.html\u0026#34;] preStop: exec: # 在容器停止之前停止nginx服务 command: [\u0026#34;/usr/sbin/nginx\u0026#34;,\u0026#34;-s\u0026#34;,\u0026#34;quit\u0026#34;] # 创建pod [root@k8s-master01 ~]# kubectl create -f pod-hook-exec.yaml pod/pod-hook-exec created # 查看pod [root@k8s-master01 ~]# kubectl get pods pod-hook-exec -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE pod-hook-exec 1/1 Running 0 29s 10.244.2.48 node2 # 访问pod [root@k8s-master01 ~]# curl 10.244.2.48 postStart... 5.3.4 容器探测 容器探测用于检测容器中的应用实例是否正常工作，是保障业务可用性的一种传统机制。如果经过探测，实例的状态不符合预期，那么kubernetes就会把该问题实例\u0026rdquo; 摘除 \u0026ldquo;，不承担业务流量。kubernetes提供了两种探针来实现容器探测，分别是：\nliveness probes：存活性探针，用于检测应用实例当前是否处于正常运行状态，如果不是，k8s会重启容器 readiness probes：就绪性探针，用于检测应用实例当前是否可以接收请求，如果不能，k8s不会转发流量 livenessProbe 决定是否重启容器，readinessProbe 决定是否将请求转发给容器。\n上面两种探针目前均支持三种探测方式：\nExec命令：在容器内执行一次命令，如果命令执行的退出码为0，则认为程序正常，否则不正常\n…… livenessProbe: exec: command: - cat - /tmp/healthy …… TCPSocket：将会尝试访问一个用户容器的端口，如果能够建立这条连接，则认为程序正常，否则不正常\n…… livenessProbe: tcpSocket: port: 8080 …… HTTPGet：调用容器内Web应用的URL，如果返回的状态码在200和399之间，则认为程序正常，否则不正常\n…… livenessProbe: httpGet: path: / #URI地址 port: 80 #端口号 host: 127.0.0.1 #主机地址 scheme: HTTP #支持的协议，http或者https …… 下面以liveness probes为例，做几个演示：\n方式一：Exec\n创建pod-liveness-exec.yaml\napiVersion: v1 kind: Pod metadata: name: pod-liveness-exec namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 livenessProbe: exec: command: [\u0026#34;/bin/cat\u0026#34;,\u0026#34;/tmp/hello.txt\u0026#34;] # 执行一个查看文件的命令 创建pod，观察效果\n# 创建Pod [root@k8s-master01 ~]# kubectl create -f pod-liveness-exec.yaml pod/pod-liveness-exec created # 查看Pod详情 [root@k8s-master01 ~]# kubectl describe pods pod-liveness-exec -n dev ...... Normal Created 20s (x2 over 50s) kubelet, node1 Created container nginx Normal Started 20s (x2 over 50s) kubelet, node1 Started container nginx Normal Killing 20s kubelet, node1 Container nginx failed liveness probe, will be restarted Warning Unhealthy 0s (x5 over 40s) kubelet, node1 Liveness probe failed: cat: can\u0026#39;t open \u0026#39;/tmp/hello11.txt\u0026#39;: No such file or directory # 观察上面的信息就会发现nginx容器启动之后就进行了健康检查 # 检查失败之后，容器被kill掉，然后尝试进行重启（这是重启策略的作用，后面讲解） # 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长 [root@k8s-master01 ~]# kubectl get pods pod-liveness-exec -n dev NAME READY STATUS RESTARTS AGE pod-liveness-exec 0/1 CrashLoopBackOff 2 3m19s # 当然接下来，可以修改成一个存在的文件，比如/tmp/hello.txt，再试，结果就正常了...... 方式二：TCPSocket\n创建pod-liveness-tcpsocket.yaml\napiVersion: v1 kind: Pod metadata: name: pod-liveness-tcpsocket namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 livenessProbe: tcpSocket: port: 8080 # 尝试访问8080端口 创建pod，观察效果\n# 创建Pod [root@k8s-master01 ~]# kubectl create -f pod-liveness-tcpsocket.yaml pod/pod-liveness-tcpsocket created # 查看Pod详情 [root@k8s-master01 ~]# kubectl describe pods pod-liveness-tcpsocket -n dev ...... Normal Scheduled 31s default-scheduler Successfully assigned dev/pod-liveness-tcpsocket to node2 Normal Pulled \u0026lt;invalid\u0026gt; kubelet, node2 Container image \u0026#34;nginx:1.17.1\u0026#34; already present on machine Normal Created \u0026lt;invalid\u0026gt; kubelet, node2 Created container nginx Normal Started \u0026lt;invalid\u0026gt; kubelet, node2 Started container nginx Warning Unhealthy \u0026lt;invalid\u0026gt; (x2 over \u0026lt;invalid\u0026gt;) kubelet, node2 Liveness probe failed: dial tcp 10.244.2.44:8080: connect: connection refused # 观察上面的信息，发现尝试访问8080端口,但是失败了 # 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长 [root@k8s-master01 ~]# kubectl get pods pod-liveness-tcpsocket -n dev NAME READY STATUS RESTARTS AGE pod-liveness-tcpsocket 0/1 CrashLoopBackOff 2 3m19s # 当然接下来，可以修改成一个可以访问的端口，比如80，再试，结果就正常了...... 方式三：HTTPGet\n创建pod-liveness-httpget.yaml\napiVersion: v1 kind: Pod metadata: name: pod-liveness-httpget namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 livenessProbe: httpGet: # 其实就是访问http://127.0.0.1:80/hello scheme: HTTP #支持的协议，http或者https port: 80 #端口号 path: /hello #URI地址 创建pod，观察效果\n# 创建Pod [root@k8s-master01 ~]# kubectl create -f pod-liveness-httpget.yaml pod/pod-liveness-httpget created # 查看Pod详情 [root@k8s-master01 ~]# kubectl describe pod pod-liveness-httpget -n dev ....... Normal Pulled 6s (x3 over 64s) kubelet, node1 Container image \u0026#34;nginx:1.17.1\u0026#34; already present on machine Normal Created 6s (x3 over 64s) kubelet, node1 Created container nginx Normal Started 6s (x3 over 63s) kubelet, node1 Started container nginx Warning Unhealthy 6s (x6 over 56s) kubelet, node1 Liveness probe failed: HTTP probe failed with statuscode: 404 Normal Killing 6s (x2 over 36s) kubelet, node1 Container nginx failed liveness probe, will be restarted # 观察上面信息，尝试访问路径，但是未找到,出现404错误 # 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长 [root@k8s-master01 ~]# kubectl get pod pod-liveness-httpget -n dev NAME READY STATUS RESTARTS AGE pod-liveness-httpget 1/1 Running 5 3m17s # 当然接下来，可以修改成一个可以访问的路径path，比如/，再试，结果就正常了...... 至此，已经使用liveness Probe演示了三种探测方式，但是查看livenessProbe的子属性，会发现除了这三种方式，还有一些其他的配置，在这里一并解释下：\n[root@k8s-master01 ~]# kubectl explain pod.spec.containers.livenessProbe FIELDS: exec \u0026lt;Object\u0026gt; tcpSocket \u0026lt;Object\u0026gt; httpGet \u0026lt;Object\u0026gt; initialDelaySeconds \u0026lt;integer\u0026gt; # 容器启动后等待多少秒执行第一次探测 timeoutSeconds \u0026lt;integer\u0026gt; # 探测超时时间。默认1秒，最小1秒 periodSeconds \u0026lt;integer\u0026gt; # 执行探测的频率。默认是10秒，最小1秒 failureThreshold \u0026lt;integer\u0026gt; # 连续探测失败多少次才被认定为失败。默认是3。最小值是1 successThreshold \u0026lt;integer\u0026gt; # 连续探测成功多少次才被认定为成功。默认是1 下面稍微配置两个，演示下效果即可：\n[root@k8s-master01 ~]# more pod-liveness-httpget.yaml apiVersion: v1 kind: Pod metadata: name: pod-liveness-httpget namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 livenessProbe: httpGet: scheme: HTTP port: 80 path: / initialDelaySeconds: 30 # 容器启动后30s开始探测 timeoutSeconds: 5 # 探测超时时间为5s 5.3.5 重启策略 在上一节中，一旦容器探测出现了问题，kubernetes就会对容器所在的Pod进行重启，其实这是由pod的重启策略决定的，pod的重启策略有 3 种，分别如下：\nAlways ：容器失效时，自动重启该容器，这也是默认值。 OnFailure ： 容器终止运行且退出码不为0时重启 Never ： 不论状态为何，都不重启该容器 重启策略适用于pod对象中的所有容器，首次需要重启的容器，将在其需要时立即进行重启，随后再次需要重启的操作将由kubelet延迟一段时间后进行，且反复的重启操作的延迟时长以此为10s、20s、40s、80s、160s和300s，300s是最大延迟时长。\n创建pod-restartpolicy.yaml：\napiVersion: v1 kind: Pod metadata: name: pod-restartpolicy namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 livenessProbe: httpGet: scheme: HTTP port: 80 path: /hello restartPolicy: Never # 设置重启策略为Never 运行Pod测试\n# 创建Pod [root@k8s-master01 ~]# kubectl create -f pod-restartpolicy.yaml pod/pod-restartpolicy created # 查看Pod详情，发现nginx容器失败 [root@k8s-master01 ~]# kubectl describe pods pod-restartpolicy -n dev ...... Warning Unhealthy 15s (x3 over 35s) kubelet, node1 Liveness probe failed: HTTP probe failed with statuscode: 404 Normal Killing 15s kubelet, node1 Container nginx failed liveness probe # 多等一会，再观察pod的重启次数，发现一直是0，并未重启 [root@k8s-master01 ~]# kubectl get pods pod-restartpolicy -n dev NAME READY STATUS RESTARTS AGE pod-restartpolicy 0/1 Running 0 5min42s 5.4 Pod调度 在默认情况下，一个Pod在哪个Node节点上运行，是由Scheduler组件采用相应的算法计算出来的，这个过程是不受人工控制的。但是在实际使用中，这并不满足的需求，因为很多情况下，我们想控制某些Pod到达某些节点上，那么应该怎么做呢？这就要求了解kubernetes对Pod的调度规则，kubernetes提供了四大类调度方式：\n自动调度：运行在哪个节点上完全由Scheduler经过一系列的算法计算得出 定向调度：NodeName、NodeSelector 亲和性调度：NodeAffinity、PodAffinity、PodAntiAffinity 污点（容忍）调度：Taints、Toleration 5.4.1 定向调度 定向调度，指的是利用在pod上声明nodeName或者nodeSelector，以此将Pod调度到期望的node节点上。注意，这里的调度是强制的，这就意味着即使要调度的目标Node不存在，也会向上面进行调度，只不过pod运行失败而已。\nNodeName\nNodeName用于强制约束将Pod调度到指定的Name的Node节点上。这种方式，其实是直接跳过Scheduler的调度逻辑，直接将Pod调度到指定名称的节点。\n接下来，实验一下：创建一个pod-nodename.yaml文件\napiVersion: v1 kind: Pod metadata: name: pod-nodename namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 nodeName: node1 # 指定调度到node1节点上 #创建Pod [root@k8s-master01 ~]# kubectl create -f pod-nodename.yaml pod/pod-nodename created #查看Pod调度到NODE属性，确实是调度到了node1节点上 [root@k8s-master01 ~]# kubectl get pods pod-nodename -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE ...... pod-nodename 1/1 Running 0 56s 10.244.1.87 node1 ...... # 接下来，删除pod，修改nodeName的值为node3（并没有node3节点） [root@k8s-master01 ~]# kubectl delete -f pod-nodename.yaml pod \u0026#34;pod-nodename\u0026#34; deleted [root@k8s-master01 ~]# vim pod-nodename.yaml [root@k8s-master01 ~]# kubectl create -f pod-nodename.yaml pod/pod-nodename created #再次查看，发现已经向Node3节点调度，但是由于不存在node3节点，所以pod无法正常运行 [root@k8s-master01 ~]# kubectl get pods pod-nodename -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE ...... pod-nodename 0/1 Pending 0 6s \u0026lt;none\u0026gt; node3 ...... NodeSelector\nNodeSelector用于将pod调度到添加了指定标签的node节点上。它是通过kubernetes的label-selector机制实现的，也就是说，在pod创建之前，会由scheduler使用MatchNodeSelector调度策略进行label匹配，找出目标node，然后将pod调度到目标节点，该匹配规则是强制约束。\n接下来，实验一下：\n1 首先分别为node节点添加标签\n[root@k8s-master01 ~]# kubectl label nodes node1 nodeenv=pro node/node2 labeled [root@k8s-master01 ~]# kubectl label nodes node2 nodeenv=test node/node2 labeled 2 创建一个pod-nodeselector.yaml文件，并使用它创建Pod\napiVersion: v1 kind: Pod metadata: name: pod-nodeselector namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 nodeSelector: nodeenv: pro # 指定调度到具有nodeenv=pro标签的节点上 #创建Pod [root@k8s-master01 ~]# kubectl create -f pod-nodeselector.yaml pod/pod-nodeselector created #查看Pod调度到NODE属性，确实是调度到了node1节点上 [root@k8s-master01 ~]# kubectl get pods pod-nodeselector -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE ...... pod-nodeselector 1/1 Running 0 47s 10.244.1.87 node1 ...... # 接下来，删除pod，修改nodeSelector的值为nodeenv: xxxx（不存在打有此标签的节点） [root@k8s-master01 ~]# kubectl delete -f pod-nodeselector.yaml pod \u0026#34;pod-nodeselector\u0026#34; deleted [root@k8s-master01 ~]# vim pod-nodeselector.yaml [root@k8s-master01 ~]# kubectl create -f pod-nodeselector.yaml pod/pod-nodeselector created #再次查看，发现pod无法正常运行,Node的值为none [root@k8s-master01 ~]# kubectl get pods -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE pod-nodeselector 0/1 Pending 0 2m20s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # 查看详情,发现node selector匹配失败的提示 [root@k8s-master01 ~]# kubectl describe pods pod-nodeselector -n dev ....... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling \u0026lt;unknown\u0026gt; default-scheduler 0/3 nodes are available: 3 node(s) didn\u0026#39;t match node selector. 5.4.2 亲和性调度 上一节，介绍了两种定向调度的方式，使用起来非常方便，但是也有一定的问题，那就是如果没有满足条件的Node，那么Pod将不会被运行，即使在集群中还有可用Node列表也不行，这就限制了它的使用场景。\n基于上面的问题，kubernetes还提供了一种亲和性调度（Affinity）。它在NodeSelector的基础之上的进行了扩展，可以通过配置的形式，实现优先选择满足条件的Node进行调度，如果没有，也可以调度到不满足条件的节点上，使调度更加灵活。\nAffinity主要分为三类：\nnodeAffinity(node亲和性）: 以node为目标，解决pod可以调度到哪些node的问题 podAffinity(pod亲和性) : 以pod为目标，解决pod可以和哪些已存在的pod部署在同一个拓扑域中的问题 podAntiAffinity(pod反亲和性) : 以pod为目标，解决pod不能和哪些已存在pod部署在同一个拓扑域中的问题 关于亲和性(反亲和性)使用场景的说明：\n亲和性：如果两个应用频繁交互，那就有必要利用亲和性让两个应用的尽可能的靠近，这样可以减少因网络通信而带来的性能损耗。\n反亲和性：当应用的采用多副本部署时，有必要采用反亲和性让各个应用实例打散分布在各个node上，这样可以提高服务的高可用性。\nNodeAffinity\n首先来看一下NodeAffinity的可配置项：\npod.spec.affinity.nodeAffinity requiredDuringSchedulingIgnoredDuringExecution Node节点必须满足指定的所有规则才可以，相当于硬限制 nodeSelectorTerms 节点选择列表 matchFields 按节点字段列出的节点选择器要求列表 matchExpressions 按节点标签列出的节点选择器要求列表(推荐) key 键 values 值 operat or 关系符 支持Exists, DoesNotExist, In, NotIn, Gt, Lt preferredDuringSchedulingIgnoredDuringExecution 优先调度到满足指定的规则的Node，相当于软限制 (倾向) preference 一个节点选择器项，与相应的权重相关联 matchFields 按节点字段列出的节点选择器要求列表 matchExpressions 按节点标签列出的节点选择器要求列表(推荐) key 键 values 值 operator 关系符 支持In, NotIn, Exists, DoesNotExist, Gt, Lt weight 倾向权重，在范围1-100。 关系符的使用说明: - matchExpressions: - key: nodeenv # 匹配存在标签的key为nodeenv的节点 operator: Exists - key: nodeenv # 匹配标签的key为nodeenv,且value是\u0026#34;xxx\u0026#34;或\u0026#34;yyy\u0026#34;的节点 operator: In values: [\u0026#34;xxx\u0026#34;,\u0026#34;yyy\u0026#34;] - key: nodeenv # 匹配标签的key为nodeenv,且value大于\u0026#34;xxx\u0026#34;的节点 operator: Gt values: \u0026#34;xxx\u0026#34; 接下来首先演示一下requiredDuringSchedulingIgnoredDuringExecution ,\n创建pod-nodeaffinity-required.yaml\napiVersion: v1 kind: Pod metadata: name: pod-nodeaffinity-required namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 affinity: #亲和性设置 nodeAffinity: #设置node亲和性 requiredDuringSchedulingIgnoredDuringExecution: # 硬限制 nodeSelectorTerms: - matchExpressions: # 匹配env的值在[\u0026#34;xxx\u0026#34;,\u0026#34;yyy\u0026#34;]中的标签 - key: nodeenv operator: In values: [\u0026#34;xxx\u0026#34;,\u0026#34;yyy\u0026#34;] # 创建pod [root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-required.yaml pod/pod-nodeaffinity-required created # 查看pod状态 （运行失败） [root@k8s-master01 ~]# kubectl get pods pod-nodeaffinity-required -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE ...... pod-nodeaffinity-required 0/1 Pending 0 16s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ...... # 查看Pod的详情 # 发现调度失败，提示node选择失败 [root@k8s-master01 ~]# kubectl describe pod pod-nodeaffinity-required -n dev ...... Warning FailedScheduling \u0026lt;unknown\u0026gt; default-scheduler 0/3 nodes are available: 3 node(s) didn\u0026#39;t match node selector. Warning FailedScheduling \u0026lt;unknown\u0026gt; default-scheduler 0/3 nodes are available: 3 node(s) didn\u0026#39;t match node selector. #接下来，停止pod [root@k8s-master01 ~]# kubectl delete -f pod-nodeaffinity-required.yaml pod \u0026#34;pod-nodeaffinity-required\u0026#34; deleted # 修改文件，将values: [\u0026#34;xxx\u0026#34;,\u0026#34;yyy\u0026#34;]------\u0026gt; [\u0026#34;pro\u0026#34;,\u0026#34;yyy\u0026#34;] [root@k8s-master01 ~]# vim pod-nodeaffinity-required.yaml # 再次启动 [root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-required.yaml pod/pod-nodeaffinity-required created # 此时查看，发现调度成功，已经将pod调度到了node1上 [root@k8s-master01 ~]# kubectl get pods pod-nodeaffinity-required -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE ...... pod-nodeaffinity-required 1/1 Running 0 11s 10.244.1.89 node1 ...... 接下来再演示一下requiredDuringSchedulingIgnoredDuringExecution ,\n创建pod-nodeaffinity-preferred.yaml\napiVersion: v1 kind: Pod metadata: name: pod-nodeaffinity-preferred namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 affinity: #亲和性设置 nodeAffinity: #设置node亲和性 preferredDuringSchedulingIgnoredDuringExecution: # 软限制 - weight: 1 preference: matchExpressions: # 匹配env的值在[\u0026#34;xxx\u0026#34;,\u0026#34;yyy\u0026#34;]中的标签(当前环境没有) - key: nodeenv operator: In values: [\u0026#34;xxx\u0026#34;,\u0026#34;yyy\u0026#34;] # 创建pod [root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-preferred.yaml pod/pod-nodeaffinity-preferred created # 查看pod状态 （运行成功） [root@k8s-master01 ~]# kubectl get pod pod-nodeaffinity-preferred -n dev NAME READY STATUS RESTARTS AGE pod-nodeaffinity-preferred 1/1 Running 0 40s NodeAffinity规则设置的注意事项： 1 如果同时定义了nodeSelector和nodeAffinity，那么必须两个条件都得到满足，Pod才能运行在指定的Node上 2 如果nodeAffinity指定了多个nodeSelectorTerms，那么只需要其中一个能够匹配成功即可 3 如果一个nodeSelectorTerms中有多个matchExpressions ，则一个节点必须满足所有的才能匹配成功 4 如果一个pod所在的Node在Pod运行期间其标签发生了改变，不再符合该Pod的节点亲和性需求，则系统将忽略此变化 PodAffinity\nPodAffinity主要实现以运行的Pod为参照，实现让新创建的Pod跟参照pod在一个区域的功能。\n首先来看一下PodAffinity的可配置项：\npod.spec.affinity.podAffinity requiredDuringSchedulingIgnoredDuringExecution 硬限制 namespaces 指定参照pod的namespace topologyKey 指定调度作用域 labelSelector 标签选择器 matchExpressions 按节点标签列出的节点选择器要求列表(推荐) key 键 values 值 operator 关系符 支持In, NotIn, Exists, DoesNotExist. matchLabels 指多个matchExpressions映射的内容 preferredDuringSchedulingIgnoredDuringExecution 软限制 podAffinityTerm 选项 namespaces topologyKey labelSelector matchExpressions key 键 values 值 operator matchLabels weight 倾向权重，在范围1-100 topologyKey用于指定调度时作用域,例如: 如果指定为kubernetes.io/hostname，那就是以Node节点为区分范围 如果指定为beta.kubernetes.io/os,则以Node节点的操作系统类型来区分 接下来，演示下requiredDuringSchedulingIgnoredDuringExecution,\n1）首先创建一个参照Pod，pod-podaffinity-target.yaml：\napiVersion: v1 kind: Pod metadata: name: pod-podaffinity-target namespace: dev labels: podenv: pro #设置标签 spec: containers: - name: nginx image: nginx:1.17.1 nodeName: node1 # 将目标pod名确指定到node1上 # 启动目标pod [root@k8s-master01 ~]# kubectl create -f pod-podaffinity-target.yaml pod/pod-podaffinity-target created # 查看pod状况 [root@k8s-master01 ~]# kubectl get pods pod-podaffinity-target -n dev NAME READY STATUS RESTARTS AGE pod-podaffinity-target 1/1 Running 0 4s 2）创建pod-podaffinity-required.yaml，内容如下：\napiVersion: v1 kind: Pod metadata: name: pod-podaffinity-required namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 affinity: #亲和性设置 podAffinity: #设置pod亲和性 requiredDuringSchedulingIgnoredDuringExecution: # 硬限制 - labelSelector: matchExpressions: # 匹配env的值在[\u0026#34;xxx\u0026#34;,\u0026#34;yyy\u0026#34;]中的标签 - key: podenv operator: In values: [\u0026#34;xxx\u0026#34;,\u0026#34;yyy\u0026#34;] topologyKey: kubernetes.io/hostname 上面配置表达的意思是：新Pod必须要与拥有标签nodeenv=xxx或者nodeenv=yyy的pod在同一Node上，显然现在没有这样pod，接下来，运行测试一下。\n# 启动pod [root@k8s-master01 ~]# kubectl create -f pod-podaffinity-required.yaml pod/pod-podaffinity-required created # 查看pod状态，发现未运行 [root@k8s-master01 ~]# kubectl get pods pod-podaffinity-required -n dev NAME READY STATUS RESTARTS AGE pod-podaffinity-required 0/1 Pending 0 9s # 查看详细信息 [root@k8s-master01 ~]# kubectl describe pods pod-podaffinity-required -n dev ...... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling \u0026lt;unknown\u0026gt; default-scheduler 0/3 nodes are available: 2 node(s) didn\u0026#39;t match pod affinity rules, 1 node(s) had taints that the pod didn\u0026#39;t tolerate. # 接下来修改 values: [\u0026#34;xxx\u0026#34;,\u0026#34;yyy\u0026#34;]-----\u0026gt;values:[\u0026#34;pro\u0026#34;,\u0026#34;yyy\u0026#34;] # 意思是：新Pod必须要与拥有标签nodeenv=xxx或者nodeenv=yyy的pod在同一Node上 [root@k8s-master01 ~]# vim pod-podaffinity-required.yaml # 然后重新创建pod，查看效果 [root@k8s-master01 ~]# kubectl delete -f pod-podaffinity-required.yaml pod \u0026#34;pod-podaffinity-required\u0026#34; de leted [root@k8s-master01 ~]# kubectl create -f pod-podaffinity-required.yaml pod/pod-podaffinity-required created # 发现此时Pod运行正常 [root@k8s-master01 ~]# kubectl get pods pod-podaffinity-required -n dev NAME READY STATUS RESTARTS AGE LABELS pod-podaffinity-required 1/1 Running 0 6s \u0026lt;none\u0026gt; 关于PodAffinity的 preferredDuringSchedulingIgnoredDuringExecution，这里不再演示。\nPodAntiAffinity\nPodAntiAffinity主要实现以运行的Pod为参照，让新创建的Pod跟参照pod不在一个区域中的功能。\n它的配置方式和选项跟PodAffinty是一样的，这里不再做详细解释，直接做一个测试案例。\n1）继续使用上个案例中目标pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide --show-labels NAME READY STATUS RESTARTS AGE IP NODE LABELS pod-podaffinity-required 1/1 Running 0 3m29s 10.244.1.38 node1 \u0026lt;none\u0026gt; pod-podaffinity-target 1/1 Running 0 9m25s 10.244.1.37 node1 podenv=pro 2）创建pod-podantiaffinity-required.yaml，内容如下：\napiVersion: v1 kind: Pod metadata: name: pod-podantiaffinity-required namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 affinity: #亲和性设置 podAntiAffinity: #设置pod亲和性 requiredDuringSchedulingIgnoredDuringExecution: # 硬限制 - labelSelector: matchExpressions: # 匹配podenv的值在[\u0026#34;pro\u0026#34;]中的标签 - key: podenv operator: In values: [\u0026#34;pro\u0026#34;] topologyKey: kubernetes.io/hostname 上面配置表达的意思是：新Pod必须要与拥有标签nodeenv=pro的pod不在同一Node上，运行测试一下。\n# 创建pod [root@k8s-master01 ~]# kubectl create -f pod-podantiaffinity-required.yaml pod/pod-podantiaffinity-required created # 查看pod # 发现调度到了node2上 [root@k8s-master01 ~]# kubectl get pods pod-podantiaffinity-required -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE .. pod-podantiaffinity-required 1/1 Running 0 30s 10.244.1.96 node2 .. 5.4.3 污点和容忍 污点（Taints）\n前面的调度方式都是站在Pod的角度上，通过在Pod上添加属性，来确定Pod是否要调度到指定的Node上，其实我们也可以站在Node的角度上，通过在Node上添加污点属性，来决定是否允许Pod调度过来。\nNode被设置上污点之后就和Pod之间存在了一种相斥的关系，进而拒绝Pod调度进来，甚至可以将已经存在的Pod驱逐出去。\n污点的格式为：key=value:effect, key和value是污点的标签，effect描述污点的作用，支持如下三个选项：\nPreferNoSchedule：kubernetes将尽量避免把Pod调度到具有该污点的Node上，除非没有其他节点可调度 NoSchedule：kubernetes将不会把Pod调度到具有该污点的Node上，但不会影响当前Node上已存在的Pod NoExecute：kubernetes将不会把Pod调度到具有该污点的Node上，同时也会将Node上已存在的Pod驱离 使用kubectl设置和去除污点的命令示例如下：\n# 设置污点 kubectl taint nodes node1 key=value:effect # 去除污点 kubectl taint nodes node1 key:effect- # 去除所有污点 kubectl taint nodes node1 key- 接下来，演示下污点的效果：\n准备节点node1（为了演示效果更加明显，暂时停止node2节点） 为node1节点设置一个污点: tag=heima:PreferNoSchedule；然后创建pod1( pod1 可以 ) 修改为node1节点设置一个污点: tag=heima:NoSchedule；然后创建pod2( pod1 正常 pod2 失败 ) 修改为node1节点设置一个污点: tag=heima:NoExecute；然后创建pod3 ( 3个pod都失败 ) # 为node1设置污点(PreferNoSchedule) [root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:PreferNoSchedule # 创建pod1 [root@k8s-master01 ~]# kubectl run taint1 --image=nginx:1.17.1 -n dev [root@k8s-master01 ~]# kubectl get pods -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE taint1-7665f7fd85-574h4 1/1 Running 0 2m24s 10.244.1.59 node1 # 为node1设置污点(取消PreferNoSchedule，设置NoSchedule) [root@k8s-master01 ~]# kubectl taint nodes node1 tag:PreferNoSchedule- [root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:NoSchedule # 创建pod2 [root@k8s-master01 ~]# kubectl run taint2 --image=nginx:1.17.1 -n dev [root@k8s-master01 ~]# kubectl get pods taint2 -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE taint1-7665f7fd85-574h4 1/1 Running 0 2m24s 10.244.1.59 node1 taint2-544694789-6zmlf 0/1 Pending 0 21s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # 为node1设置污点(取消NoSchedule，设置NoExecute) [root@k8s-master01 ~]# kubectl taint nodes node1 tag:NoSchedule- [root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:NoExecute # 创建pod3 [root@k8s-master01 ~]# kubectl run taint3 --image=nginx:1.17.1 -n dev [root@k8s-master01 ~]# kubectl get pods -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED taint1-7665f7fd85-htkmp 0/1 Pending 0 35s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; taint2-544694789-bn7wb 0/1 Pending 0 35s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; taint3-6d78dbd749-tktkq 0/1 Pending 0 6s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; 小提示： 使用kubeadm搭建的集群，默认就会给master节点添加一个污点标记,所以pod就不会调度到master节点上. 容忍（Toleration）\n上面介绍了污点的作用，我们可以在node上添加污点用于拒绝pod调度上来，但是如果就是想将一个pod调度到一个有污点的node上去，这时候应该怎么做呢？这就要使用到容忍。\n污点就是拒绝，容忍就是忽略，Node通过污点拒绝pod调度上去，Pod通过容忍忽略拒绝\n下面先通过一个案例看下效果：\n上一小节，已经在node1节点上打上了NoExecute的污点，此时pod是调度不上去的 本小节，可以通过给pod添加容忍，然后将其调度上去 创建pod-toleration.yaml,内容如下\napiVersion: v1 kind: Pod metadata: name: pod-toleration namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 tolerations: # 添加容忍 - key: \u0026#34;tag\u0026#34; # 要容忍的污点的key operator: \u0026#34;Equal\u0026#34; # 操作符 value: \u0026#34;heima\u0026#34; # 容忍的污点的value effect: \u0026#34;NoExecute\u0026#34; # 添加容忍的规则，这里必须和标记的污点规则相同 # 添加容忍之前的pod [root@k8s-master01 ~]# kubectl get pods -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED pod-toleration 0/1 Pending 0 3s \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; # 添加容忍之后的pod [root@k8s-master01 ~]# kubectl get pods -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED pod-toleration 1/1 Running 0 3s 10.244.1.62 node1 \u0026lt;none\u0026gt; 下面看一下容忍的详细配置:\n[root@k8s-master01 ~]# kubectl explain pod.spec.tolerations ...... FIELDS: key # 对应着要容忍的污点的键，空意味着匹配所有的键 value # 对应着要容忍的污点的值 operator # key-value的运算符，支持Equal和Exists（默认） effect # 对应污点的effect，空意味着匹配所有影响 tolerationSeconds # 容忍时间, 当effect为NoExecute时生效，表示pod在Node上的停留时间 6. Pod控制器详解 6.1 Pod控制器介绍 Pod是kubernetes的最小管理单元，在kubernetes中，按照pod的创建方式可以将其分为两类：\n自主式pod：kubernetes直接创建出来的Pod，这种pod删除后就没有了，也不会重建 控制器创建的pod：kubernetes通过控制器创建的pod，这种pod删除了之后还会自动重建 什么是Pod控制器\nPod控制器是管理pod的中间层，使用Pod控制器之后，只需要告诉Pod控制器，想要多少个什么样的Pod就可以了，它会创建出满足条件的Pod并确保每一个Pod资源处于用户期望的目标状态。如果Pod资源在运行中出现故障，它会基于指定策略重新编排Pod。\n在kubernetes中，有很多类型的pod控制器，每种都有自己的适合的场景，常见的有下面这些：\nReplicationController：比较原始的pod控制器，已经被废弃，由ReplicaSet替代 ReplicaSet：保证副本数量一直维持在期望值，并支持pod数量扩缩容，镜像版本升级 Deployment：通过控制ReplicaSet来控制Pod，并支持滚动升级、回退版本 Horizontal Pod Autoscaler：可以根据集群负载自动水平调整Pod的数量，实现削峰填谷 DaemonSet：在集群中的指定Node上运行且仅运行一个副本，一般用于守护进程类的任务 Job：它创建出来的pod只要完成任务就立即退出，不需要重启或重建，用于执行一次性任务 Cronjob：它创建的Pod负责周期性任务控制，不需要持续后台运行 StatefulSet：管理有状态应用 6.2 ReplicaSet(RS) ReplicaSet的主要作用是保证一定数量的pod正常运行，它会持续监听这些Pod的运行状态，一旦Pod发生故障，就会重启或重建。同时它还支持对pod数量的扩缩容和镜像版本的升降级。\nReplicaSet的资源清单文件：\napiVersion: apps/v1 # 版本号 kind: ReplicaSet # 类型 metadata: # 元数据 name: # rs名称 namespace: # 所属命名空间 labels: #标签 controller: rs spec: # 详情描述 replicas: 3 # 副本数量 selector: # 选择器，通过它指定该控制器管理哪些pod matchLabels: # Labels匹配规则 app: nginx-pod matchExpressions: # Expressions匹配规则 - {key: app, operator: In, values: [nginx-pod]} template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本 metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 在这里面，需要新了解的配置项就是spec下面几个选项：\nreplicas：指定副本数量，其实就是当前rs创建出来的pod的数量，默认为1\nselector：选择器，它的作用是建立pod控制器和pod之间的关联关系，采用的Label Selector机制\n在pod模板上定义label，在控制器上定义选择器，就可以表明当前控制器能管理哪些pod了\ntemplate：模板，就是当前控制器创建pod所使用的模板板，里面其实就是前一章学过的pod的定义\n创建ReplicaSet\n创建pc-replicaset.yaml文件，内容如下：\napiVersion: apps/v1 kind: ReplicaSet metadata: name: pc-replicaset namespace: dev spec: replicas: 3 selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 # 创建rs [root@k8s-master01 ~]# kubectl create -f pc-replicaset.yaml replicaset.apps/pc-replicaset created # 查看rs # DESIRED:期望副本数量 # CURRENT:当前副本数量 # READY:已经准备好提供服务的副本数量 [root@k8s-master01 ~]# kubectl get rs pc-replicaset -n dev -o wide NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTOR pc-replicaset 3 3 3 22s nginx nginx:1.17.1 app=nginx-pod # 查看当前控制器创建出来的pod # 这里发现控制器创建出来的pod的名称是在控制器名称后面拼接了-xxxxx随机码 [root@k8s-master01 ~]# kubectl get pod -n dev NAME READY STATUS RESTARTS AGE pc-replicaset-6vmvt 1/1 Running 0 54s pc-replicaset-fmb8f 1/1 Running 0 54s pc-replicaset-snrk2 1/1 Running 0 54s 扩缩容\n# 编辑rs的副本数量，修改spec:replicas: 6即可 [root@k8s-master01 ~]# kubectl edit rs pc-replicaset -n dev replicaset.apps/pc-replicaset edited # 查看pod [root@k8s-master01 ~]# kubectl get pods -n dev NAME READY STATUS RESTARTS AGE pc-replicaset-6vmvt 1/1 Running 0 114m pc-replicaset-cftnp 1/1 Running 0 10s pc-replicaset-fjlm6 1/1 Running 0 10s pc-replicaset-fmb8f 1/1 Running 0 114m pc-replicaset-s2whj 1/1 Running 0 10s pc-replicaset-snrk2 1/1 Running 0 114m # 当然也可以直接使用命令实现 # 使用scale命令实现扩缩容， 后面--replicas=n直接指定目标数量即可 [root@k8s-master01 ~]# kubectl scale rs pc-replicaset --replicas=2 -n dev replicaset.apps/pc-replicaset scaled # 命令运行完毕，立即查看，发现已经有4个开始准备退出了 [root@k8s-master01 ~]# kubectl get pods -n dev NAME READY STATUS RESTARTS AGE pc-replicaset-6vmvt 0/1 Terminating 0 118m pc-replicaset-cftnp 0/1 Terminating 0 4m17s pc-replicaset-fjlm6 0/1 Terminating 0 4m17s pc-replicaset-fmb8f 1/1 Running 0 118m pc-replicaset-s2whj 0/1 Terminating 0 4m17s pc-replicaset-snrk2 1/1 Running 0 118m #稍等片刻，就只剩下2个了 [root@k8s-master01 ~]# kubectl get pods -n dev NAME READY STATUS RESTARTS AGE pc-replicaset-fmb8f 1/1 Running 0 119m pc-replicaset-snrk2 1/1 Running 0 119m 镜像升级\n# 编辑rs的容器镜像 - image: nginx:1.17.2 [root@k8s-master01 ~]# kubectl edit rs pc-replicaset -n dev replicaset.apps/pc-replicaset edited # 再次查看，发现镜像版本已经变更了 [root@k8s-master01 ~]# kubectl get rs -n dev -o wide NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES ... pc-replicaset 2 2 2 140m nginx nginx:1.17.2 ... # 同样的道理，也可以使用命令完成这个工作 # kubectl set image rs rs名称 容器=镜像版本 -n namespace [root@k8s-master01 ~]# kubectl set image rs pc-replicaset nginx=nginx:1.17.1 -n dev replicaset.apps/pc-replicaset image updated # 再次查看，发现镜像版本已经变更了 [root@k8s-master01 ~]# kubectl get rs -n dev -o wide NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES ... pc-replicaset 2 2 2 145m nginx nginx:1.17.1 ... 删除ReplicaSet\n# 使用kubectl delete命令会删除此RS以及它管理的Pod # 在kubernetes删除RS前，会将RS的replicasclear调整为0，等待所有的Pod被删除后，在执行RS对象的删除 [root@k8s-master01 ~]# kubectl delete rs pc-replicaset -n dev replicaset.apps \u0026#34;pc-replicaset\u0026#34; deleted [root@k8s-master01 ~]# kubectl get pod -n dev -o wide No resources found in dev namespace. # 如果希望仅仅删除RS对象（保留Pod），可以使用kubectl delete命令时添加--cascade=false选项（不推荐）。 [root@k8s-master01 ~]# kubectl delete rs pc-replicaset -n dev --cascade=false replicaset.apps \u0026#34;pc-replicaset\u0026#34; deleted [root@k8s-master01 ~]# kubectl get pods -n dev NAME READY STATUS RESTARTS AGE pc-replicaset-cl82j 1/1 Running 0 75s pc-replicaset-dslhb 1/1 Running 0 75s # 也可以使用yaml直接删除(推荐) [root@k8s-master01 ~]# kubectl delete -f pc-replicaset.yaml replicaset.apps \u0026#34;pc-replicaset\u0026#34; deleted 6.3 Deployment(Deploy) 为了更好的解决服务编排的问题，kubernetes在V1.2版本开始，引入了Deployment控制器。值得一提的是，这种控制器并不直接管理pod，而是通过管理ReplicaSet来简介管理Pod，即：Deployment管理ReplicaSet，ReplicaSet管理Pod。所以Deployment比ReplicaSet功能更加强大。\nDeployment主要功能有下面几个：\n支持ReplicaSet的所有功能 支持发布的停止、继续 支持滚动升级和回滚版本 Deployment的资源清单文件：\napiVersion: apps/v1 # 版本号 kind: Deployment # 类型 metadata: # 元数据 name: # rs名称 namespace: # 所属命名空间 labels: #标签 controller: deploy spec: # 详情描述 replicas: 3 # 副本数量 revisionHistoryLimit: 3 # 保留历史版本 paused: false # 暂停部署，默认是false progressDeadlineSeconds: 600 # 部署超时时间（s），默认是600 strategy: # 策略 type: RollingUpdate # 滚动更新策略 rollingUpdate: # 滚动更新 max违规词汇: 30% # 最大额外可以存在的副本数，可以为百分比，也可以为整数 maxUnavailable: 30% # 最大不可用状态的 Pod 的最大值，可以为百分比，也可以为整数 selector: # 选择器，通过它指定该控制器管理哪些pod matchLabels: # Labels匹配规则 app: nginx-pod matchExpressions: # Expressions匹配规则 - {key: app, operator: In, values: [nginx-pod]} template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本 metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 6.3.1 创建deployment 创建pc-deployment.yaml，内容如下：\napiVersion: apps/v1 kind: Deployment metadata: name: pc-deployment namespace: dev spec: replicas: 3 selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 # 创建deployment [root@k8s-master01 ~]# kubectl create -f pc-deployment.yaml --record=true deployment.apps/pc-deployment created # 查看deployment # UP-TO-DATE 最新版本的pod的数量 # AVAILABLE 当前可用的pod的数量 [root@k8s-master01 ~]# kubectl get deploy pc-deployment -n dev NAME READY UP-TO-DATE AVAILABLE AGE pc-deployment 3/3 3 3 15s # 查看rs # 发现rs的名称是在原来deployment的名字后面添加了一个10位数的随机串 [root@k8s-master01 ~]# kubectl get rs -n dev NAME DESIRED CURRENT READY AGE pc-deployment-6696798b78 3 3 3 23s # 查看pod [root@k8s-master01 ~]# kubectl get pods -n dev NAME READY STATUS RESTARTS AGE pc-deployment-6696798b78-d2c8n 1/1 Running 0 107s pc-deployment-6696798b78-smpvp 1/1 Running 0 107s pc-deployment-6696798b78-wvjd8 1/1 Running 0 107s 6.3.2 扩缩容 # 变更副本数量为5个 [root@k8s-master01 ~]# kubectl scale deploy pc-deployment --replicas=5 -n dev deployment.apps/pc-deployment scaled # 查看deployment [root@k8s-master01 ~]# kubectl get deploy pc-deployment -n dev NAME READY UP-TO-DATE AVAILABLE AGE pc-deployment 5/5 5 5 2m # 查看pod [root@k8s-master01 ~]# kubectl get pods -n dev NAME READY STATUS RESTARTS AGE pc-deployment-6696798b78-d2c8n 1/1 Running 0 4m19s pc-deployment-6696798b78-jxmdq 1/1 Running 0 94s pc-deployment-6696798b78-mktqv 1/1 Running 0 93s pc-deployment-6696798b78-smpvp 1/1 Running 0 4m19s pc-deployment-6696798b78-wvjd8 1/1 Running 0 4m19s # 编辑deployment的副本数量，修改spec:replicas: 4即可 [root@k8s-master01 ~]# kubectl edit deploy pc-deployment -n dev deployment.apps/pc-deployment edited # 查看pod [root@k8s-master01 ~]# kubectl get pods -n dev NAME READY STATUS RESTARTS AGE pc-deployment-6696798b78-d2c8n 1/1 Running 0 5m23s pc-deployment-6696798b78-jxmdq 1/1 Running 0 2m38s pc-deployment-6696798b78-smpvp 1/1 Running 0 5m23s pc-deployment-6696798b78-wvjd8 1/1 Running 0 5m23s 镜像更新\ndeployment支持两种更新策略:重建更新和滚动更新,可以通过strategy指定策略类型,支持两个属性:\nstrategy：指定新的Pod替换旧的Pod的策略， 支持两个属性： type：指定策略类型，支持两种策略 Recreate：在创建出新的Pod之前会先杀掉所有已存在的Pod RollingUpdate：滚动更新，就是杀死一部分，就启动一部分，在更新过程中，存在两个版本Pod rollingUpdate：当type为RollingUpdate时生效，用于为RollingUpdate设置参数，支持两个属性： maxUnavailable：用来指定在升级过程中不可用Pod的最大数量，默认为25%。 max违规词汇： 用来指定在升级过程中可以超过期望的Pod的最大数量，默认为25%。 重建更新\n编辑pc-deployment.yaml,在spec节点下添加更新策略 spec: strategy: # 策略 type: Recreate # 重建更新 创建deploy进行验证 # 变更镜像 [root@k8s-master01 ~]# kubectl set image deployment pc-deployment nginx=nginx:1.17.2 -n dev deployment.apps/pc-deployment image updated # 观察升级过程 [root@k8s-master01 ~]# kubectl get pods -n dev -w NAME READY STATUS RESTARTS AGE pc-deployment-5d89bdfbf9-65qcw 1/1 Running 0 31s pc-deployment-5d89bdfbf9-w5nzv 1/1 Running 0 31s pc-deployment-5d89bdfbf9-xpt7w 1/1 Running 0 31s pc-deployment-5d89bdfbf9-xpt7w 1/1 Terminating 0 41s pc-deployment-5d89bdfbf9-65qcw 1/1 Terminating 0 41s pc-deployment-5d89bdfbf9-w5nzv 1/1 Terminating 0 41s pc-deployment-675d469f8b-grn8z 0/1 Pending 0 0s pc-deployment-675d469f8b-hbl4v 0/1 Pending 0 0s pc-deployment-675d469f8b-67nz2 0/1 Pending 0 0s pc-deployment-675d469f8b-grn8z 0/1 ContainerCreating 0 0s pc-deployment-675d469f8b-hbl4v 0/1 ContainerCreating 0 0s pc-deployment-675d469f8b-67nz2 0/1 ContainerCreating 0 0s pc-deployment-675d469f8b-grn8z 1/1 Running 0 1s pc-deployment-675d469f8b-67nz2 1/1 Running 0 1s pc-deployment-675d469f8b-hbl4v 1/1 Running 0 2s 滚动更新\n编辑pc-deployment.yaml,在spec节点下添加更新策略 spec: strategy: # 策略 type: RollingUpdate # 滚动更新策略 rollingUpdate: max违规词汇: 25% maxUnavailable: 25% 创建deploy进行验证 # 变更镜像 [root@k8s-master01 ~]# kubectl set image deployment pc-deployment nginx=nginx:1.17.3 -n dev deployment.apps/pc-deployment image updated # 观察升级过程 [root@k8s-master01 ~]# kubectl get pods -n dev -w NAME READY STATUS RESTARTS AGE pc-deployment-c848d767-8rbzt 1/1 Running 0 31m pc-deployment-c848d767-h4p68 1/1 Running 0 31m pc-deployment-c848d767-hlmz4 1/1 Running 0 31m pc-deployment-c848d767-rrqcn 1/1 Running 0 31m pc-deployment-966bf7f44-226rx 0/1 Pending 0 0s pc-deployment-966bf7f44-226rx 0/1 ContainerCreating 0 0s pc-deployment-966bf7f44-226rx 1/1 Running 0 1s pc-deployment-c848d767-h4p68 0/1 Terminating 0 34m pc-deployment-966bf7f44-cnd44 0/1 Pending 0 0s pc-deployment-966bf7f44-cnd44 0/1 ContainerCreating 0 0s pc-deployment-966bf7f44-cnd44 1/1 Running 0 2s pc-deployment-c848d767-hlmz4 0/1 Terminating 0 34m pc-deployment-966bf7f44-px48p 0/1 Pending 0 0s pc-deployment-966bf7f44-px48p 0/1 ContainerCreating 0 0s pc-deployment-966bf7f44-px48p 1/1 Running 0 0s pc-deployment-c848d767-8rbzt 0/1 Terminating 0 34m pc-deployment-966bf7f44-dkmqp 0/1 Pending 0 0s pc-deployment-966bf7f44-dkmqp 0/1 ContainerCreating 0 0s pc-deployment-966bf7f44-dkmqp 1/1 Running 0 2s pc-deployment-c848d767-rrqcn 0/1 Terminating 0 34m # 至此，新版本的pod创建完毕，就版本的pod销毁完毕 # 中间过程是滚动进行的，也就是边销毁边创建 滚动更新的过程：\n镜像更新中rs的变化\n# 查看rs,发现原来的rs的依旧存在，只是pod数量变为了0，而后又新产生了一个rs，pod数量为4 # 其实这就是deployment能够进行版本回退的奥妙所在，后面会详细解释 [root@k8s-master01 ~]# kubectl get rs -n dev NAME DESIRED CURRENT READY AGE pc-deployment-6696798b78 0 0 0 7m37s pc-deployment-6696798b11 0 0 0 5m37s pc-deployment-c848d76789 4 4 4 72s 6.3.3 版本回退 deployment支持版本升级过程中的暂停、继续功能以及版本回退等诸多功能，下面具体来看.\nkubectl rollout： 版本升级相关功能，支持下面的选项：\nstatus\t显示当前升级状态 history 显示 升级历史记录 pause 暂停版本升级过程 resume 继续已经暂停的版本升级过程 restart 重启版本升级过程 undo 回滚到上一级版本（可以使用\u0026ndash;to-revision回滚到指定版本） # 查看当前升级版本的状态 [root@k8s-master01 ~]# kubectl rollout status deploy pc-deployment -n dev deployment \u0026#34;pc-deployment\u0026#34; successfully rolled out # 查看升级历史记录 [root@k8s-master01 ~]# kubectl rollout history deploy pc-deployment -n dev deployment.apps/pc-deployment REVISION CHANGE-CAUSE 1 kubectl create --filename=pc-deployment.yaml --record=true 2 kubectl create --filename=pc-deployment.yaml --record=true 3 kubectl create --filename=pc-deployment.yaml --record=true # 可以发现有三次版本记录，说明完成过两次升级 # 版本回滚 # 这里直接使用--to-revision=1回滚到了1版本， 如果省略这个选项，就是回退到上个版本，就是2版本 [root@k8s-master01 ~]# kubectl rollout undo deployment pc-deployment --to-revision=1 -n dev deployment.apps/pc-deployment rolled back # 查看发现，通过nginx镜像版本可以发现到了第一版 [root@k8s-master01 ~]# kubectl get deploy -n dev -o wide NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES pc-deployment 4/4 4 4 74m nginx nginx:1.17.1 # 查看rs，发现第一个rs中有4个pod运行，后面两个版本的rs中pod为运行 # 其实deployment之所以可是实现版本的回滚，就是通过记录下历史rs来实现的， # 一旦想回滚到哪个版本，只需要将当前版本pod数量降为0，然后将回滚版本的pod提升为目标数量就可以了 [root@k8s-master01 ~]# kubectl get rs -n dev NAME DESIRED CURRENT READY AGE pc-deployment-6696798b78 4 4 4 78m pc-deployment-966bf7f44 0 0 0 37m pc-deployment-c848d767 0 0 0 71m 6.3.4 金丝雀发布 Deployment控制器支持控制更新过程中的控制，如“暂停(pause)”或“继续(resume)”更新操作。\n比如有一批新的Pod资源创建完成后立即暂停更新过程，此时，仅存在一部分新版本的应用，主体部分还是旧的版本。然后，再筛选一小部分的用户请求路由到新版本的Pod应用，继续观察能否稳定地按期望的方式运行。确定没问题之后再继续完成余下的Pod资源滚动更新，否则立即回滚更新操作。这就是所谓的金丝雀发布。\n# 更新deployment的版本，并配置暂停deployment [root@k8s-master01 ~]# kubectl set image deploy pc-deployment nginx=nginx:1.17.4 -n dev \u0026amp;\u0026amp; kubectl rollout pause deployment pc-deployment -n dev deployment.apps/pc-deployment image updated deployment.apps/pc-deployment paused #观察更新状态 [root@k8s-master01 ~]# kubectl rollout status deploy pc-deployment -n dev　Waiting for deployment \u0026#34;pc-deployment\u0026#34; rollout to finish: 2 out of 4 new replicas have been updated... # 监控更新的过程，可以看到已经新增了一个资源，但是并未按照预期的状态去删除一个旧的资源，就是因为使用了pause暂停命令 [root@k8s-master01 ~]# kubectl get rs -n dev -o wide NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES pc-deployment-5d89bdfbf9 3 3 3 19m nginx nginx:1.17.1 pc-deployment-675d469f8b 0 0 0 14m nginx nginx:1.17.2 pc-deployment-6c9f56fcfb 2 2 2 3m16s nginx nginx:1.17.4 [root@k8s-master01 ~]# kubectl get pods -n dev NAME READY STATUS RESTARTS AGE pc-deployment-5d89bdfbf9-rj8sq 1/1 Running 0 7m33s pc-deployment-5d89bdfbf9-ttwgg 1/1 Running 0 7m35s pc-deployment-5d89bdfbf9-v4wvc 1/1 Running 0 7m34s pc-deployment-6c9f56fcfb-996rt 1/1 Running 0 3m31s pc-deployment-6c9f56fcfb-j2gtj 1/1 Running 0 3m31s # 确保更新的pod没问题了，继续更新 [root@k8s-master01 ~]# kubectl rollout resume deploy pc-deployment -n dev deployment.apps/pc-deployment resumed # 查看最后的更新情况 [root@k8s-master01 ~]# kubectl get rs -n dev -o wide NAME DESIRED CURRENT READY AGE CONTAINERS IMAGES pc-deployment-5d89bdfbf9 0 0 0 21m nginx nginx:1.17.1 pc-deployment-675d469f8b 0 0 0 16m nginx nginx:1.17.2 pc-deployment-6c9f56fcfb 4 4 4 5m11s nginx nginx:1.17.4 [root@k8s-master01 ~]# kubectl get pods -n dev NAME READY STATUS RESTARTS AGE pc-deployment-6c9f56fcfb-7bfwh 1/1 Running 0 37s pc-deployment-6c9f56fcfb-996rt 1/1 Running 0 5m27s pc-deployment-6c9f56fcfb-j2gtj 1/1 Running 0 5m27s pc-deployment-6c9f56fcfb-rf84v 1/1 Running 0 37s 删除Deployment\n# 删除deployment，其下的rs和pod也将被删除 [root@k8s-master01 ~]# kubectl delete -f pc-deployment.yaml deployment.apps \u0026#34;pc-deployment\u0026#34; deleted 6.4 Horizontal Pod Autoscaler(HPA) 在前面的课程中，我们已经可以实现通过手工执行kubectl scale命令实现Pod扩容或缩容，但是这显然不符合Kubernetes的定位目标\u0026ndash;自动化、智能化。 Kubernetes期望可以实现通过监测Pod的使用情况，实现pod数量的自动调整，于是就产生了Horizontal Pod Autoscaler（HPA）这种控制器。\nHPA可以获取每个Pod利用率，然后和HPA中定义的指标进行对比，同时计算出需要伸缩的具体值，最后实现Pod的数量的调整。其实HPA与之前的Deployment一样，也属于一种Kubernetes资源对象，它通过追踪分析RC控制的所有目标Pod的负载变化情况，来确定是否需要针对性地调整目标Pod的副本数，这是HPA的实现原理。\n接下来，我们来做一个实验\n6.4.1 安装metrics-server metrics-server可以用来收集集群中的资源使用情况\n# 安装git [root@k8s-master01 ~]# yum install git -y # 获取metrics-server, 注意使用的版本 [root@k8s-master01 ~]# git clone -b v0.3.6 https://github.com/kubernetes-incubator/metrics-server # 修改deployment, 注意修改的是镜像和初始化参数 [root@k8s-master01 ~]# cd /root/metrics-server/deploy/1.8+/ [root@k8s-master01 1.8+]# vim metrics-server-deployment.yaml 按图中添加下面选项 hostNetwork: true image: registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6 args: - --kubelet-insecure-tls - --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP # 安装metrics-server [root@k8s-master01 1.8+]# kubectl apply -f ./ # 查看pod运行情况 [root@k8s-master01 1.8+]# kubectl get pod -n kube-system metrics-server-6b976979db-2xwbj 1/1 Running 0 90s # 使用kubectl top node 查看资源使用情况 [root@k8s-master01 1.8+]# kubectl top node NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-master01 289m 14% 1582Mi 54% k8s-node01 81m 4% 1195Mi 40% k8s-node02 72m 3% 1211Mi 41% [root@k8s-master01 1.8+]# kubectl top pod -n kube-system NAME CPU(cores) MEMORY(bytes) coredns-6955765f44-7ptsb 3m 9Mi coredns-6955765f44-vcwr5 3m 8Mi etcd-master 14m 145Mi ... # 至此,metrics-server安装完成 6.4.2 准备deployment和servie 创建pc-hpa-pod.yaml文件，内容如下：\napiVersion: apps/v1 kind: Deployment metadata: name: nginx namespace: dev spec: strategy: # 策略 type: RollingUpdate # 滚动更新策略 replicas: 1 selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 resources: # 资源配额 limits: # 限制资源（上限） cpu: \u0026#34;1\u0026#34; # CPU限制，单位是core数 requests: # 请求资源（下限） cpu: \u0026#34;100m\u0026#34; # CPU限制，单位是core数 # 创建deployment [root@k8s-master01 1.8+]# kubectl run nginx --image=nginx:1.17.1 --requests=cpu=100m -n dev # 创建service [root@k8s-master01 1.8+]# kubectl expose deployment nginx --type=NodePort --port=80 -n dev # 查看 [root@k8s-master01 1.8+]# kubectl get deployment,pod,svc -n dev NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx 1/1 1 1 47s NAME READY STATUS RESTARTS AGE pod/nginx-7df9756ccc-bh8dr 1/1 Running 0 47s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/nginx NodePort 10.101.18.29 \u0026lt;none\u0026gt; 80:31830/TCP 35s 6.4.3 部署HPA 创建pc-hpa.yaml文件，内容如下：\napiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata: name: pc-hpa namespace: dev spec: minReplicas: 1 #最小pod数量 maxReplicas: 10 #最大pod数量 targetCPUUtilizationPercentage: 3 # CPU使用率指标 scaleTargetRef: # 指定要控制的nginx信息 apiVersion: /v1 kind: Deployment name: nginx # 创建hpa [root@k8s-master01 1.8+]# kubectl create -f pc-hpa.yaml horizontalpodautoscaler.autoscaling/pc-hpa created # 查看hpa [root@k8s-master01 1.8+]# kubectl get hpa -n dev NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE pc-hpa Deployment/nginx 0%/3% 1 10 1 62s 6.4.4 测试 使用压测工具对service地址192.168.5.4:31830进行压测，然后通过控制台查看hpa和pod的变化\nhpa变化\n[root@k8s-master01 ~]# kubectl get hpa -n dev -w NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE pc-hpa Deployment/nginx 0%/3% 1 10 1 4m11s pc-hpa Deployment/nginx 0%/3% 1 10 1 5m19s pc-hpa Deployment/nginx 22%/3% 1 10 1 6m50s pc-hpa Deployment/nginx 22%/3% 1 10 4 7m5s pc-hpa Deployment/nginx 22%/3% 1 10 8 7m21s pc-hpa Deployment/nginx 6%/3% 1 10 8 7m51s pc-hpa Deployment/nginx 0%/3% 1 10 8 9m6s pc-hpa Deployment/nginx 0%/3% 1 10 8 13m pc-hpa Deployment/nginx 0%/3% 1 10 1 14m deployment变化\n[root@k8s-master01 ~]# kubectl get deployment -n dev -w NAME READY UP-TO-DATE AVAILABLE AGE nginx 1/1 1 1 11m nginx 1/4 1 1 13m nginx 1/4 1 1 13m nginx 1/4 1 1 13m nginx 1/4 4 1 13m nginx 1/8 4 1 14m nginx 1/8 4 1 14m nginx 1/8 4 1 14m nginx 1/8 8 1 14m nginx 2/8 8 2 14m nginx 3/8 8 3 14m nginx 4/8 8 4 14m nginx 5/8 8 5 14m nginx 6/8 8 6 14m nginx 7/8 8 7 14m nginx 8/8 8 8 15m nginx 8/1 8 8 20m nginx 8/1 8 8 20m nginx 1/1 1 1 20m pod变化\n[root@k8s-master01 ~]# kubectl get pods -n dev -w NAME READY STATUS RESTARTS AGE nginx-7df9756ccc-bh8dr 1/1 Running 0 11m nginx-7df9756ccc-cpgrv 0/1 Pending 0 0s nginx-7df9756ccc-8zhwk 0/1 Pending 0 0s nginx-7df9756ccc-rr9bn 0/1 Pending 0 0s nginx-7df9756ccc-cpgrv 0/1 ContainerCreating 0 0s nginx-7df9756ccc-8zhwk 0/1 ContainerCreating 0 0s nginx-7df9756ccc-rr9bn 0/1 ContainerCreating 0 0s nginx-7df9756ccc-m9gsj 0/1 Pending 0 0s nginx-7df9756ccc-g56qb 0/1 Pending 0 0s nginx-7df9756ccc-sl9c6 0/1 Pending 0 0s nginx-7df9756ccc-fgst7 0/1 Pending 0 0s nginx-7df9756ccc-g56qb 0/1 ContainerCreating 0 0s nginx-7df9756ccc-m9gsj 0/1 ContainerCreating 0 0s nginx-7df9756ccc-sl9c6 0/1 ContainerCreating 0 0s nginx-7df9756ccc-fgst7 0/1 ContainerCreating 0 0s nginx-7df9756ccc-8zhwk 1/1 Running 0 19s nginx-7df9756ccc-rr9bn 1/1 Running 0 30s nginx-7df9756ccc-m9gsj 1/1 Running 0 21s nginx-7df9756ccc-cpgrv 1/1 Running 0 47s nginx-7df9756ccc-sl9c6 1/1 Running 0 33s nginx-7df9756ccc-g56qb 1/1 Running 0 48s nginx-7df9756ccc-fgst7 1/1 Running 0 66s nginx-7df9756ccc-fgst7 1/1 Terminating 0 6m50s nginx-7df9756ccc-8zhwk 1/1 Terminating 0 7m5s nginx-7df9756ccc-cpgrv 1/1 Terminating 0 7m5s nginx-7df9756ccc-g56qb 1/1 Terminating 0 6m50s nginx-7df9756ccc-rr9bn 1/1 Terminating 0 7m5s nginx-7df9756ccc-m9gsj 1/1 Terminating 0 6m50s nginx-7df9756ccc-sl9c6 1/1 Terminating 0 6m50s 6.5 DaemonSet(DS) DaemonSet类型的控制器可以保证在集群中的每一台（或指定）节点上都运行一个副本。一般适用于日志收集、节点监控等场景。也就是说，如果一个Pod提供的功能是节点级别的（每个节点都需要且只需要一个），那么这类Pod就适合使用DaemonSet类型的控制器创建。\nDaemonSet控制器的特点：\n每当向集群中添加一个节点时，指定的 Pod 副本也将添加到该节点上 当节点从集群中移除时，Pod 也就被垃圾回收了 下面先来看下DaemonSet的资源清单文件\napiVersion: apps/v1 # 版本号 kind: DaemonSet # 类型 metadata: # 元数据 name: # rs名称 namespace: # 所属命名空间 labels: #标签 controller: daemonset spec: # 详情描述 revisionHistoryLimit: 3 # 保留历史版本 updateStrategy: # 更新策略 type: RollingUpdate # 滚动更新策略 rollingUpdate: # 滚动更新 maxUnavailable: 1 # 最大不可用状态的 Pod 的最大值，可以为百分比，也可以为整数 selector: # 选择器，通过它指定该控制器管理哪些pod matchLabels: # Labels匹配规则 app: nginx-pod matchExpressions: # Expressions匹配规则 - {key: app, operator: In, values: [nginx-pod]} template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本 metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 创建pc-daemonset.yaml，内容如下：\napiVersion: apps/v1 kind: DaemonSet metadata: name: pc-daemonset namespace: dev spec: selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 # 创建daemonset [root@k8s-master01 ~]# kubectl create -f pc-daemonset.yaml daemonset.apps/pc-daemonset created # 查看daemonset [root@k8s-master01 ~]# kubectl get ds -n dev -o wide NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES pc-daemonset 2 2 2 2 2 24s nginx nginx:1.17.1 # 查看pod,发现在每个Node上都运行一个pod [root@k8s-master01 ~]# kubectl get pods -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE pc-daemonset-9bck8 1/1 Running 0 37s 10.244.1.43 node1 pc-daemonset-k224w 1/1 Running 0 37s 10.244.2.74 node2 # 删除daemonset [root@k8s-master01 ~]# kubectl delete -f pc-daemonset.yaml daemonset.apps \u0026#34;pc-daemonset\u0026#34; deleted 6.6 Job Job，主要用于负责**批量处理(一次要处理指定数量任务)短暂的一次性(每个任务仅运行一次就结束)**任务。Job特点如下：\n当Job创建的pod执行成功结束时，Job将记录成功结束的pod数量 当成功结束的pod达到指定的数量时，Job将完成执行 Job的资源清单文件：\napiVersion: batch/v1 # 版本号 kind: Job # 类型 metadata: # 元数据 name: # rs名称 namespace: # 所属命名空间 labels: #标签 controller: job spec: # 详情描述 completions: 1 # 指定job需要成功运行Pods的次数。默认值: 1 parallelism: 1 # 指定job在任一时刻应该并发运行Pods的数量。默认值: 1 activeDeadlineSeconds: 30 # 指定job可运行的时间期限，超过时间还未结束，系统将会尝试进行终止。 backoffLimit: 6 # 指定job失败后进行重试的次数。默认是6 manualSelector: true # 是否可以使用selector选择器选择pod，默认是false selector: # 选择器，通过它指定该控制器管理哪些pod matchLabels: # Labels匹配规则 app: counter-pod matchExpressions: # Expressions匹配规则 - {key: app, operator: In, values: [counter-pod]} template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本 metadata: labels: app: counter-pod spec: restartPolicy: Never # 重启策略只能设置为Never或者OnFailure containers: - name: counter image: busybox:1.30 command: [\u0026#34;bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 2;done\u0026#34;] 关于重启策略设置的说明： 如果指定为OnFailure，则job会在pod出现故障时重启容器，而不是创建pod，failed次数不变 如果指定为Never，则job会在pod出现故障时创建新的pod，并且故障pod不会消失，也不会重启，failed次数加1 如果指定为Always的话，就意味着一直重启，意味着job任务会重复去执行了，当然不对，所以不能设置为Always 创建pc-job.yaml，内容如下：\napiVersion: batch/v1 kind: Job metadata: name: pc-job namespace: dev spec: manualSelector: true selector: matchLabels: app: counter-pod template: metadata: labels: app: counter-pod spec: restartPolicy: Never containers: - name: counter image: busybox:1.30 command: [\u0026#34;bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 3;done\u0026#34;] # 创建job [root@k8s-master01 ~]# kubectl create -f pc-job.yaml job.batch/pc-job created # 查看job [root@k8s-master01 ~]# kubectl get job -n dev -o wide -w NAME COMPLETIONS DURATION AGE CONTAINERS IMAGES SELECTOR pc-job 0/1 21s 21s counter busybox:1.30 app=counter-pod pc-job 1/1 31s 79s counter busybox:1.30 app=counter-pod # 通过观察pod状态可以看到，pod在运行完毕任务后，就会变成Completed状态 [root@k8s-master01 ~]# kubectl get pods -n dev -w NAME READY STATUS RESTARTS AGE pc-job-rxg96 1/1 Running 0 29s pc-job-rxg96 0/1 Completed 0 33s # 接下来，调整下pod运行的总数量和并行数量 即：在spec下设置下面两个选项 # completions: 6 # 指定job需要成功运行Pods的次数为6 # parallelism: 3 # 指定job并发运行Pods的数量为3 # 然后重新运行job，观察效果，此时会发现，job会每次运行3个pod，总共执行了6个pod [root@k8s-master01 ~]# kubectl get pods -n dev -w NAME READY STATUS RESTARTS AGE pc-job-684ft 1/1 Running 0 5s pc-job-jhj49 1/1 Running 0 5s pc-job-pfcvh 1/1 Running 0 5s pc-job-684ft 0/1 Completed 0 11s pc-job-v7rhr 0/1 Pending 0 0s pc-job-v7rhr 0/1 Pending 0 0s pc-job-v7rhr 0/1 ContainerCreating 0 0s pc-job-jhj49 0/1 Completed 0 11s pc-job-fhwf7 0/1 Pending 0 0s pc-job-fhwf7 0/1 Pending 0 0s pc-job-pfcvh 0/1 Completed 0 11s pc-job-5vg2j 0/1 Pending 0 0s pc-job-fhwf7 0/1 ContainerCreating 0 0s pc-job-5vg2j 0/1 Pending 0 0s pc-job-5vg2j 0/1 ContainerCreating 0 0s pc-job-fhwf7 1/1 Running 0 2s pc-job-v7rhr 1/1 Running 0 2s pc-job-5vg2j 1/1 Running 0 3s pc-job-fhwf7 0/1 Completed 0 12s pc-job-v7rhr 0/1 Completed 0 12s pc-job-5vg2j 0/1 Completed 0 12s # 删除job [root@k8s-master01 ~]# kubectl delete -f pc-job.yaml job.batch \u0026#34;pc-job\u0026#34; deleted 6.7 CronJob(CJ) CronJob控制器以 Job控制器资源为其管控对象，并借助它管理pod资源对象，Job控制器定义的作业任务在其控制器资源创建之后便会立即执行，但CronJob可以以类似于Linux操作系统的周期性任务作业计划的方式控制其运行时间点及重复运行的方式。也就是说，CronJob可以在特定的时间点(反复的)去运行job任务。\nCronJob的资源清单文件：\napiVersion: batch/v1beta1 # 版本号 kind: CronJob # 类型 metadata: # 元数据 name: # rs名称 namespace: # 所属命名空间 labels: #标签 controller: cronjob spec: # 详情描述 schedule: # cron格式的作业调度运行时间点,用于控制任务在什么时间执行 concurrencyPolicy: # 并发执行策略，用于定义前一次作业运行尚未完成时是否以及如何运行后一次的作业 failedJobHistoryLimit: # 为失败的任务执行保留的历史记录数，默认为1 successfulJobHistoryLimit: # 为成功的任务执行保留的历史记录数，默认为3 startingDeadlineSeconds: # 启动作业错误的超时时长 jobTemplate: # job控制器模板，用于为cronjob控制器生成job对象;下面其实就是job的定义 metadata: spec: completions: 1 parallelism: 1 activeDeadlineSeconds: 30 backoffLimit: 6 manualSelector: true selector: matchLabels: app: counter-pod matchExpressions: 规则 - {key: app, operator: In, values: [counter-pod]} template: metadata: labels: app: counter-pod spec: restartPolicy: Never containers: - name: counter image: busybox:1.30 command: [\u0026#34;bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 20;done\u0026#34;] 需要重点解释的几个选项： schedule: cron表达式，用于指定任务的执行时间 */1 * * * * \u0026lt;分钟\u0026gt; \u0026lt;小时\u0026gt; \u0026lt;日\u0026gt; \u0026lt;月份\u0026gt; \u0026lt;星期\u0026gt; 分钟 值从 0 到 59. 小时 值从 0 到 23. 日 值从 1 到 31. 月 值从 1 到 12. 星期 值从 0 到 6, 0 代表星期日 多个时间可以用逗号隔开； 范围可以用连字符给出；*可以作为通配符； /表示每... concurrencyPolicy: Allow: 允许Jobs并发运行(默认) Forbid: 禁止并发运行，如果上一次运行尚未完成，则跳过下一次运行 Replace: 替换，取消当前正在运行的作业并用新作业替换它 创建pc-cronjob.yaml，内容如下：\napiVersion: batch/v1beta1 kind: CronJob metadata: name: pc-cronjob namespace: dev labels: controller: cronjob spec: schedule: \u0026#34;*/1 * * * *\u0026#34; jobTemplate: metadata: spec: template: spec: restartPolicy: Never containers: - name: counter image: busybox:1.30 command: [\u0026#34;bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 3;done\u0026#34;] # 创建cronjob [root@k8s-master01 ~]# kubectl create -f pc-cronjob.yaml cronjob.batch/pc-cronjob created # 查看cronjob [root@k8s-master01 ~]# kubectl get cronjobs -n dev NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE pc-cronjob */1 * * * * False 0 \u0026lt;none\u0026gt; 6s # 查看job [root@k8s-master01 ~]# kubectl get jobs -n dev NAME COMPLETIONS DURATION AGE pc-cronjob-1592587800 1/1 28s 3m26s pc-cronjob-1592587860 1/1 28s 2m26s pc-cronjob-1592587920 1/1 28s 86s # 查看pod [root@k8s-master01 ~]# kubectl get pods -n dev pc-cronjob-1592587800-x4tsm 0/1 Completed 0 2m24s pc-cronjob-1592587860-r5gv4 0/1 Completed 0 84s pc-cronjob-1592587920-9dxxq 1/1 Running 0 24s # 删除cronjob [root@k8s-master01 ~]# kubectl delete -f pc-cronjob.yaml cronjob.batch \u0026#34;pc-cronjob\u0026#34; deleted 7. Service详解 7.1 Service介绍 在kubernetes中，pod是应用程序的载体，我们可以通过pod的ip来访问应用程序，但是pod的ip地址不是固定的，这也就意味着不方便直接采用pod的ip对服务进行访问。\n为了解决这个问题，kubernetes提供了Service资源，Service会对提供同一个服务的多个pod进行聚合，并且提供一个统一的入口地址。通过访问Service的入口地址就能访问到后面的pod服务。\nService在很多情况下只是一个概念，真正起作用的其实是kube-proxy服务进程，每个Node节点上都运行着一个kube-proxy服务进程。当创建Service的时候会通过api-server向etcd写入创建的service的信息，而kube-proxy会基于监听的机制发现这种Service的变动，然后它会将最新的Service信息转换成对应的访问规则。\n# 10.97.97.97:80 是service提供的访问入口 # 当访问这个入口的时候，可以发现后面有三个pod的服务在等待调用， # kube-proxy会基于rr（轮询）的策略，将请求分发到其中一个pod上去 # 这个规则会同时在集群内的所有节点上都生成，所以在任何一个节点上，都可以访问。 [root@node1 ~]# ipvsadm -Ln IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u0026gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.97.97.97:80 rr -\u0026gt; 10.244.1.39:80 Masq 1 0 0 -\u0026gt; 10.244.1.40:80 Masq 1 0 0 -\u0026gt; 10.244.2.33:80 Masq 1 0 0 kube-proxy目前支持三种工作模式:\n7.1.1 userspace 模式 userspace模式下，kube-proxy会为每一个Service创建一个监听端口，发向Cluster IP的请求被Iptables规则重定向到kube-proxy监听的端口上，kube-proxy根据LB算法选择一个提供服务的Pod并和其建立链接，以将请求转发到Pod上。 该模式下，kube-proxy充当了一个四层负责均衡器的角色。由于kube-proxy运行在userspace中，在进行转发处理时会增加内核和用户空间之间的数据拷贝，虽然比较稳定，但是效率比较低。\n7.1.2 iptables 模式 iptables模式下，kube-proxy为service后端的每个Pod创建对应的iptables规则，直接将发向Cluster IP的请求重定向到一个Pod IP。 该模式下kube-proxy不承担四层负责均衡器的角色，只负责创建iptables规则。该模式的优点是较userspace模式效率更高，但不能提供灵活的LB策略，当后端Pod不可用时也无法进行重试。\n7.1.3 ipvs 模式 ipvs模式和iptables类似，kube-proxy监控Pod的变化并创建相应的ipvs规则。ipvs相对iptables转发效率更高。除此以外，ipvs支持更多的LB算法。\n# 此模式必须安装ipvs内核模块，否则会降级为iptables # 开启ipvs [root@k8s-master01 ~]# kubectl edit cm kube-proxy -n kube-system # 修改mode: \u0026#34;ipvs\u0026#34; [root@k8s-master01 ~]# kubectl delete pod -l k8s-app=kube-proxy -n kube-system [root@node1 ~]# ipvsadm -Ln IP Virtual Server version 1.2.1 (size=4096) Prot LocalAddress:Port Scheduler Flags -\u0026gt; RemoteAddress:Port Forward Weight ActiveConn InActConn TCP 10.97.97.97:80 rr -\u0026gt; 10.244.1.39:80 Masq 1 0 0 -\u0026gt; 10.244.1.40:80 Masq 1 0 0 -\u0026gt; 10.244.2.33:80 Masq 1 0 0 7.2 Service类型 Service的资源清单文件：\nkind: Service # 资源类型 apiVersion: v1 # 资源版本 metadata: # 元数据 name: service # 资源名称 namespace: dev # 命名空间 spec: # 描述 selector: # 标签选择器，用于确定当前service代理哪些pod app: nginx type: # Service类型，指定service的访问方式 clusterIP: # 虚拟服务的ip地址 sessionAffinity: # session亲和性，支持ClientIP、None两个选项 ports: # 端口信息 - protocol: TCP port: 3017 # service端口 targetPort: 5003 # pod端口 nodePort: 31122 # 主机端口 ClusterIP：默认值，它是Kubernetes系统自动分配的虚拟IP，只能在集群内部访问 NodePort：将Service通过指定的Node上的端口暴露给外部，通过此方法，就可以在集群外部访问服务 LoadBalancer：使用外接负载均衡器完成到服务的负载分发，注意此模式需要外部云环境支持 ExternalName： 把集群外部的服务引入集群内部，直接使用 7.3 Service使用 7.3.1 实验环境准备 在使用service之前，首先利用Deployment创建出3个pod，注意要为pod设置app=nginx-pod的标签\n创建deployment.yaml，内容如下：\napiVersion: apps/v1 kind: Deployment metadata: name: pc-deployment namespace: dev spec: replicas: 3 selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 [root@k8s-master01 ~]# kubectl create -f deployment.yaml deployment.apps/pc-deployment created # 查看pod详情 [root@k8s-master01 ~]# kubectl get pods -n dev -o wide --show-labels NAME READY STATUS IP NODE LABELS pc-deployment-66cb59b984-8p84h 1/1 Running 10.244.1.39 node1 app=nginx-pod pc-deployment-66cb59b984-vx8vx 1/1 Running 10.244.2.33 node2 app=nginx-pod pc-deployment-66cb59b984-wnncx 1/1 Running 10.244.1.40 node1 app=nginx-pod # 为了方便后面的测试，修改下三台nginx的index.html页面（三台修改的IP地址不一致） # kubectl exec -it pc-deployment-66cb59b984-8p84h -n dev /bin/sh # echo \u0026#34;10.244.1.39\u0026#34; \u0026gt; /usr/share/nginx/html/index.html #修改完毕之后，访问测试 [root@k8s-master01 ~]# curl 10.244.1.39 10.244.1.39 [root@k8s-master01 ~]# curl 10.244.2.33 10.244.2.33 [root@k8s-master01 ~]# curl 10.244.1.40 10.244.1.40 7.3.2 ClusterIP类型的Service 创建service-clusterip.yaml文件\napiVersion: v1 kind: Service metadata: name: service-clusterip namespace: dev spec: selector: app: nginx-pod clusterIP: 10.97.97.97 # service的ip地址，如果不写，默认会生成一个 type: ClusterIP ports: - port: 80 # Service端口 targetPort: 80 # pod端口 # 创建service [root@k8s-master01 ~]# kubectl create -f service-clusterip.yaml service/service-clusterip created # 查看service [root@k8s-master01 ~]# kubectl get svc -n dev -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service-clusterip ClusterIP 10.97.97.97 \u0026lt;none\u0026gt; 80/TCP 13s app=nginx-pod # 查看service的详细信息 # 在这里有一个Endpoints列表，里面就是当前service可以负载到的服务入口 [root@k8s-master01 ~]# kubectl describe svc service-clusterip -n dev Name: service-clusterip Namespace: dev Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Selector: app=nginx-pod Type: ClusterIP IP: 10.97.97.97 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 80/TCP Endpoints: 10.244.1.39:80,10.244.1.40:80,10.244.2.33:80 Session Affinity: None Events: \u0026lt;none\u0026gt; # 查看ipvs的映射规则 [root@k8s-master01 ~]# ipvsadm -Ln TCP 10.97.97.97:80 rr -\u0026gt; 10.244.1.39:80 Masq 1 0 0 -\u0026gt; 10.244.1.40:80 Masq 1 0 0 -\u0026gt; 10.244.2.33:80 Masq 1 0 0 # 访问10.97.97.97:80观察效果 [root@k8s-master01 ~]# curl 10.97.97.97:80 10.244.2.33 7.3.3 Endpoint Endpoint是kubernetes中的一个资源对象，存储在etcd中，用来记录一个service对应的所有pod的访问地址，它是根据service配置文件中selector描述产生的。\n一个Service由一组Pod组成，这些Pod通过Endpoints暴露出来，Endpoints是实现实际服务的端点集合。换句话说，service和pod之间的联系是通过endpoints实现的。\n负载分发策略\n对Service的访问被分发到了后端的Pod上去，目前kubernetes提供了两种负载分发策略：\n如果不定义，默认使用kube-proxy的策略，比如随机、轮询\n基于客户端地址的会话保持模式，即来自同一个客户端发起的所有请求都会转发到固定的一个Pod上\n此模式可以使在spec中添加sessionAffinity:ClientIP选项\n# 查看ipvs的映射规则【rr 轮询】 [root@k8s-master01 ~]# ipvsadm -Ln TCP 10.97.97.97:80 rr -\u0026gt; 10.244.1.39:80 Masq 1 0 0 -\u0026gt; 10.244.1.40:80 Masq 1 0 0 -\u0026gt; 10.244.2.33:80 Masq 1 0 0 # 循环访问测试 [root@k8s-master01 ~]# while true;do curl 10.97.97.97:80; sleep 5; done; 10.244.1.40 10.244.1.39 10.244.2.33 10.244.1.40 10.244.1.39 10.244.2.33 # 修改分发策略----sessionAffinity:ClientIP # 查看ipvs规则【persistent 代表持久】 [root@k8s-master01 ~]# ipvsadm -Ln TCP 10.97.97.97:80 rr persistent 10800 -\u0026gt; 10.244.1.39:80 Masq 1 0 0 -\u0026gt; 10.244.1.40:80 Masq 1 0 0 -\u0026gt; 10.244.2.33:80 Masq 1 0 0 # 循环访问测试 [root@k8s-master01 ~]# while true;do curl 10.97.97.97; sleep 5; done; 10.244.2.33 10.244.2.33 10.244.2.33 # 删除service [root@k8s-master01 ~]# kubectl delete -f service-clusterip.yaml service \u0026#34;service-clusterip\u0026#34; deleted 7.3.4 HeadLiness类型的Service 在某些场景中，开发人员可能不想使用Service提供的负载均衡功能，而希望自己来控制负载均衡策略，针对这种情况，kubernetes提供了HeadLiness Service，这类Service不会分配Cluster IP，如果想要访问service，只能通过service的域名进行查询。\n创建service-headliness.yaml\napiVersion: v1 kind: Service metadata: name: service-headliness namespace: dev spec: selector: app: nginx-pod clusterIP: None # 将clusterIP设置为None，即可创建headliness Service type: ClusterIP ports: - port: 80 targetPort: 80 # 创建service [root@k8s-master01 ~]# kubectl create -f service-headliness.yaml service/service-headliness created # 获取service， 发现CLUSTER-IP未分配 [root@k8s-master01 ~]# kubectl get svc service-headliness -n dev -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service-headliness ClusterIP None \u0026lt;none\u0026gt; 80/TCP 11s app=nginx-pod # 查看service详情 [root@k8s-master01 ~]# kubectl describe svc service-headliness -n dev Name: service-headliness Namespace: dev Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Selector: app=nginx-pod Type: ClusterIP IP: None Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 80/TCP Endpoints: 10.244.1.39:80,10.244.1.40:80,10.244.2.33:80 Session Affinity: None Events: \u0026lt;none\u0026gt; # 查看域名的解析情况 [root@k8s-master01 ~]# kubectl exec -it pc-deployment-66cb59b984-8p84h -n dev /bin/sh / # cat /etc/resolv.conf nameserver 10.96.0.10 search dev.svc.cluster.local svc.cluster.local cluster.local [root@k8s-master01 ~]# dig @10.96.0.10 service-headliness.dev.svc.cluster.local service-headliness.dev.svc.cluster.local. 30 IN A 10.244.1.40 service-headliness.dev.svc.cluster.local. 30 IN A 10.244.1.39 service-headliness.dev.svc.cluster.local. 30 IN A 10.244.2.33 7.3.5 NodePort类型的Service 在之前的样例中，创建的Service的ip地址只有集群内部才可以访问，如果希望将Service暴露给集群外部使用，那么就要使用到另外一种类型的Service，称为NodePort类型。NodePort的工作原理其实就是将service的端口映射到Node的一个端口上，然后就可以通过NodeIp:NodePort来访问service了。\n创建service-nodeport.yaml\napiVersion: v1 kind: Service metadata: name: service-nodeport namespace: dev spec: selector: app: nginx-pod type: NodePort # service类型 ports: - port: 80 nodePort: 30002 # 指定绑定的node的端口(默认的取值范围是：30000-32767), 如果不指定，会默认分配 targetPort: 80 # 创建service [root@k8s-master01 ~]# kubectl create -f service-nodeport.yaml service/service-nodeport created # 查看service [root@k8s-master01 ~]# kubectl get svc -n dev -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) SELECTOR service-nodeport NodePort 10.105.64.191 \u0026lt;none\u0026gt; 80:30002/TCP app=nginx-pod # 接下来可以通过电脑主机的浏览器去访问集群中任意一个nodeip的30002端口，即可访问到pod 7.3.6 LoadBalancer类型的Service LoadBalancer和NodePort很相似，目的都是向外部暴露一个端口，区别在于LoadBalancer会在集群的外部再来做一个负载均衡设备，而这个设备需要外部环境支持的，外部服务发送到这个设备上的请求，会被设备负载之后转发到集群中。\n7.3.7 ExternalName类型的Service ExternalName类型的Service用于引入集群外部的服务，它通过externalName属性指定外部一个服务的地址，然后在集群内部访问此service就可以访问到外部的服务了。\napiVersion: v1 kind: Service metadata: name: service-externalname namespace: dev spec: type: ExternalName # service类型 externalName: www.baidu.com #改成ip地址也可以 # 创建service [root@k8s-master01 ~]# kubectl create -f service-externalname.yaml service/service-externalname created # 域名解析 [root@k8s-master01 ~]# dig @10.96.0.10 service-externalname.dev.svc.cluster.local service-externalname.dev.svc.cluster.local. 30 IN CNAME www.baidu.com. www.baidu.com. 30 IN CNAME www.a.shifen.com. www.a.shifen.com. 30 IN A 39.156.66.18 www.a.shifen.com. 30 IN A 39.156.66.14 7.4 Ingress介绍 在前面课程中已经提到，Service对集群之外暴露服务的主要方式有两种：NotePort和LoadBalancer，但是这两种方式，都有一定的缺点：\nNodePort方式的缺点是会占用很多集群机器的端口，那么当集群服务变多的时候，这个缺点就愈发明显 LB方式的缺点是每个service需要一个LB，浪费、麻烦，并且需要kubernetes之外设备的支持 基于这种现状，kubernetes提供了Ingress资源对象，Ingress只需要一个NodePort或者一个LB就可以满足暴露多个Service的需求。工作机制大致如下图表示：\n实际上，Ingress相当于一个7层的负载均衡器，是kubernetes对反向代理的一个抽象，它的工作原理类似于Nginx，可以理解成在Ingress里建立诸多映射规则，Ingress Controller通过监听这些配置规则并转化成Nginx的反向代理配置 , 然后对外部提供服务。在这里有两个核心概念：\ningress：kubernetes中的一个对象，作用是定义请求如何转发到service的规则 ingress controller：具体实现反向代理及负载均衡的程序，对ingress定义的规则进行解析，根据配置的规则来实现请求转发，实现方式有很多，比如Nginx, Contour, Haproxy等等 Ingress（以Nginx为例）的工作原理如下：\n用户编写Ingress规则，说明哪个域名对应kubernetes集群中的哪个Service Ingress控制器动态感知Ingress服务规则的变化，然后生成一段对应的Nginx反向代理配置 Ingress控制器会将生成的Nginx配置写入到一个运行着的Nginx服务中，并动态更新 到此为止，其实真正在工作的就是一个Nginx了，内部配置了用户定义的请求转发规则 7.5 Ingress使用 7.5.1 环境准备 搭建ingress环境 # 创建文件夹 [root@k8s-master01 ~]# mkdir ingress-controller [root@k8s-master01 ~]# cd ingress-controller/ # 获取ingress-nginx，本次案例使用的是0.30版本 [root@k8s-master01 ingress-controller]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/mandatory.yaml [root@k8s-master01 ingress-controller]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/provider/baremetal/service-nodeport.yaml # 修改mandatory.yaml文件中的仓库 # 修改quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0 # 为quay-mirror.qiniu.com/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0 # 创建ingress-nginx [root@k8s-master01 ingress-controller]# kubectl apply -f ./ # 查看ingress-nginx [root@k8s-master01 ingress-controller]# kubectl get pod -n ingress-nginx NAME READY STATUS RESTARTS AGE pod/nginx-ingress-controller-fbf967dd5-4qpbp 1/1 Running 0 12h # 查看service [root@k8s-master01 ingress-controller]# kubectl get svc -n ingress-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ingress-nginx NodePort 10.98.75.163 \u0026lt;none\u0026gt; 80:32240/TCP,443:31335/TCP 11h 7.5.2 准备service和pod 为了后面的实验比较方便，创建如下图所示的模型\n创建tomcat-nginx.yaml\napiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment namespace: dev spec: replicas: 3 selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 --- apiVersion: apps/v1 kind: Deployment metadata: name: tomcat-deployment namespace: dev spec: replicas: 3 selector: matchLabels: app: tomcat-pod template: metadata: labels: app: tomcat-pod spec: containers: - name: tomcat image: tomcat:8.5-jre10-slim ports: - containerPort: 8080 --- apiVersion: v1 kind: Service metadata: name: nginx-service namespace: dev spec: selector: app: nginx-pod clusterIP: None type: ClusterIP ports: - port: 80 targetPort: 80 --- apiVersion: v1 kind: Service metadata: name: tomcat-service namespace: dev spec: selector: app: tomcat-pod clusterIP: None type: ClusterIP ports: - port: 8080 targetPort: 8080 # 创建 [root@k8s-master01 ~]# kubectl create -f tomcat-nginx.yaml # 查看 [root@k8s-master01 ~]# kubectl get svc -n dev NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-service ClusterIP None \u0026lt;none\u0026gt; 80/TCP 48s tomcat-service ClusterIP None \u0026lt;none\u0026gt; 8080/TCP 48s 7.5.3 Http代理 创建ingress-http.yaml\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: ingress-http namespace: dev spec: rules: - host: nginx.itheima.com http: paths: - path: / backend: serviceName: nginx-service servicePort: 80 - host: tomcat.itheima.com http: paths: - path: / backend: serviceName: tomcat-service servicePort: 8080 # 创建 [root@k8s-master01 ~]# kubectl create -f ingress-http.yaml ingress.extensions/ingress-http created # 查看 [root@k8s-master01 ~]# kubectl get ing ingress-http -n dev NAME HOSTS ADDRESS PORTS AGE ingress-http nginx.itheima.com,tomcat.itheima.com 80 22s # 查看详情 [root@k8s-master01 ~]# kubectl describe ing ingress-http -n dev ... Rules: Host Path Backends ---- ---- -------- nginx.itheima.com / nginx-service:80 (10.244.1.96:80,10.244.1.97:80,10.244.2.112:80) tomcat.itheima.com / tomcat-service:8080(10.244.1.94:8080,10.244.1.95:8080,10.244.2.111:8080) ... # 接下来,在本地电脑上配置host文件,解析上面的两个域名到192.168.109.100(master)上 # 然后,就可以分别访问tomcat.itheima.com:32240 和 nginx.itheima.com:32240 查看效果了 7.5.4 Https代理 创建证书\n# 生成证书 openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/C=CN/ST=BJ/L=BJ/O=nginx/CN=itheima.com\u0026#34; # 创建密钥 kubectl create secret tls tls-secret --key tls.key --cert tls.crt 创建ingress-https.yaml\napiVersion: extensions/v1beta1 kind: Ingress metadata: name: ingress-https namespace: dev spec: tls: - hosts: - nginx.itheima.com - tomcat.itheima.com secretName: tls-secret # 指定秘钥 rules: - host: nginx.itheima.com http: paths: - path: / backend: serviceName: nginx-service servicePort: 80 - host: tomcat.itheima.com http: paths: - path: / backend: serviceName: tomcat-service servicePort: 8080 # 创建 [root@k8s-master01 ~]# kubectl create -f ingress-https.yaml ingress.extensions/ingress-https created # 查看 [root@k8s-master01 ~]# kubectl get ing ingress-https -n dev NAME HOSTS ADDRESS PORTS AGE ingress-https nginx.itheima.com,tomcat.itheima.com 10.104.184.38 80, 443 2m42s # 查看详情 [root@k8s-master01 ~]# kubectl describe ing ingress-https -n dev ... TLS: tls-secret terminates nginx.itheima.com,tomcat.itheima.com Rules: Host Path Backends ---- ---- -------- nginx.itheima.com / nginx-service:80 (10.244.1.97:80,10.244.1.98:80,10.244.2.119:80) tomcat.itheima.com / tomcat-service:8080(10.244.1.99:8080,10.244.2.117:8080,10.244.2.120:8080) ... # 下面可以通过浏览器访问https://nginx.itheima.com:31335 和 https://tomcat.itheima.com:31335来查看了 8. 数据存储 在前面已经提到，容器的生命周期可能很短，会被频繁地创建和销毁。那么容器在销毁时，保存在容器中的数据也会被清除。这种结果对用户来说，在某些情况下是不乐意看到的。为了持久化保存容器的数据，kubernetes引入了Volume的概念。\nVolume是Pod中能够被多个容器访问的共享目录，它被定义在Pod上，然后被一个Pod里的多个容器挂载到具体的文件目录下，kubernetes通过Volume实现同一个Pod中不同容器之间的数据共享以及数据的持久化存储。Volume的生命容器不与Pod中单个容器的生命周期相关，当容器终止或者重启时，Volume中的数据也不会丢失。\nkubernetes的Volume支持多种类型，比较常见的有下面几个：\n简单存储：EmptyDir、HostPath、NFS 高级存储：PV、PVC 配置存储：ConfigMap、Secret 8.1 基本存储 8.1.1 EmptyDir EmptyDir是最基础的Volume类型，一个EmptyDir就是Host上的一个空目录。\nEmptyDir是在Pod被分配到Node时创建的，它的初始内容为空，并且无须指定宿主机上对应的目录文件，因为kubernetes会自动分配一个目录，当Pod销毁时， EmptyDir中的数据也会被永久删除。 EmptyDir用途如下：\n临时空间，例如用于某些应用程序运行时所需的临时目录，且无须永久保留 一个容器需要从另一个容器中获取数据的目录（多容器共享目录） 接下来，通过一个容器之间文件共享的案例来使用一下EmptyDir。\n在一个Pod中准备两个容器nginx和busybox，然后声明一个Volume分别挂在到两个容器的目录中，然后nginx容器负责向Volume中写日志，busybox中通过命令将日志内容读到控制台。\n创建一个volume-emptydir.yaml\napiVersion: v1 kind: Pod metadata: name: volume-emptydir namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 volumeMounts: # 将logs-volume挂在到nginx容器中，对应的目录为 /var/log/nginx - name: logs-volume mountPath: /var/log/nginx - name: busybox image: busybox:1.30 command: [\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;tail -f /logs/access.log\u0026#34;] # 初始命令，动态读取指定文件中内容 volumeMounts: # 将logs-volume 挂在到busybox容器中，对应的目录为 /logs - name: logs-volume mountPath: /logs volumes: # 声明volume， name为logs-volume，类型为emptyDir - name: logs-volume emptyDir: {} # 创建Pod [root@k8s-master01 ~]# kubectl create -f volume-emptydir.yaml pod/volume-emptydir created # 查看pod [root@k8s-master01 ~]# kubectl get pods volume-emptydir -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE ...... volume-emptydir 2/2 Running 0 97s 10.42.2.9 node1 ...... # 通过podIp访问nginx [root@k8s-master01 ~]# curl 10.42.2.9 ...... # 通过kubectl logs命令查看指定容器的标准输出 [root@k8s-master01 ~]# kubectl logs -f volume-emptydir -n dev -c busybox 10.42.1.0 - - [27/Jun/2021:15:08:54 +0000] \u0026#34;GET / HTTP/1.1\u0026#34; 200 612 \u0026#34;-\u0026#34; \u0026#34;curl/7.29.0\u0026#34; \u0026#34;-\u0026#34; 8.1.2 HostPath 上节课提到，EmptyDir中数据不会被持久化，它会随着Pod的结束而销毁，如果想简单的将数据持久化到主机中，可以选择HostPath。\nHostPath就是将Node主机中一个实际目录挂在到Pod中，以供容器使用，这样的设计就可以保证Pod销毁了，但是数据依据可以存在于Node主机上。\n创建一个volume-hostpath.yaml：\napiVersion: v1 kind: Pod metadata: name: volume-hostpath namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 volumeMounts: - name: logs-volume mountPath: /var/log/nginx - name: busybox image: busybox:1.30 command: [\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;tail -f /logs/access.log\u0026#34;] volumeMounts: - name: logs-volume mountPath: /logs volumes: - name: logs-volume hostPath: path: /root/logs type: DirectoryOrCreate # 目录存在就使用，不存在就先创建后使用 关于type的值的一点说明： DirectoryOrCreate 目录存在就使用，不存在就先创建后使用 Directory 目录必须存在 FileOrCreate 文件存在就使用，不存在就先创建后使用 File 文件必须存在 Socket unix套接字必须存在 CharDevice 字符设备必须存在 BlockDevice 块设备必须存在 # 创建Pod [root@k8s-master01 ~]# kubectl create -f volume-hostpath.yaml pod/volume-hostpath created # 查看Pod [root@k8s-master01 ~]# kubectl get pods volume-hostpath -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE ...... pod-volume-hostpath 2/2 Running 0 16s 10.42.2.10 node1 ...... #访问nginx [root@k8s-master01 ~]# curl 10.42.2.10 [root@k8s-master01 ~]# kubectl logs -f volume-emptydir -n dev -c busybox # 接下来就可以去host的/root/logs目录下查看存储的文件了 ### 注意: 下面的操作需要到Pod所在的节点运行（案例中是node1） [root@node1 ~]# ls /root/logs/ access.log error.log # 同样的道理，如果在此目录下创建一个文件，到容器中也是可以看到的 8.1.3 NFS HostPath可以解决数据持久化的问题，但是一旦Node节点故障了，Pod如果转移到了别的节点，又会出现问题了，此时需要准备单独的网络存储系统，比较常用的用NFS、CIFS。\nNFS是一个网络文件存储系统，可以搭建一台NFS服务器，然后将Pod中的存储直接连接到NFS系统上，这样的话，无论Pod在节点上怎么转移，只要Node跟NFS的对接没问题，数据就可以成功访问。\n1）首先要准备nfs的服务器，这里为了简单，直接是master节点做nfs服务器\n# 在nfs上安装nfs服务 [root@nfs ~]# yum install nfs-utils -y # 准备一个共享目录 [root@nfs ~]# mkdir /root/data/nfs -pv # 将共享目录以读写权限暴露给192.168.5.0/24网段中的所有主机 [root@nfs ~]# vim /etc/exports [root@nfs ~]# more /etc/exports /root/data/nfs 192.168.5.0/24(rw,no_root_squash) # 启动nfs服务 [root@nfs ~]# systemctl restart nfs 2）接下来，要在的每个node节点上都安装下nfs，这样的目的是为了node节点可以驱动nfs设备\n# 在node上安装nfs服务，注意不需要启动 [root@k8s-master01 ~]# yum install nfs-utils -y 3）接下来，就可以编写pod的配置文件了，创建volume-nfs.yaml\napiVersion: v1 kind: Pod metadata: name: volume-nfs namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 volumeMounts: - name: logs-volume mountPath: /var/log/nginx - name: busybox image: busybox:1.30 command: [\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;tail -f /logs/access.log\u0026#34;] volumeMounts: - name: logs-volume mountPath: /logs volumes: - name: logs-volume nfs: server: 192.168.5.6 #nfs服务器地址 path: /root/data/nfs #共享文件路径 4）最后，运行下pod，观察结果\n# 创建pod [root@k8s-master01 ~]# kubectl create -f volume-nfs.yaml pod/volume-nfs created # 查看pod [root@k8s-master01 ~]# kubectl get pods volume-nfs -n dev NAME READY STATUS RESTARTS AGE volume-nfs 2/2 Running 0 2m9s # 查看nfs服务器上的共享目录，发现已经有文件了 [root@k8s-master01 ~]# ls /root/data/ access.log error.log 8.2 高级存储 前面已经学习了使用NFS提供存储，此时就要求用户会搭建NFS系统，并且会在yaml配置nfs。由于kubernetes支持的存储系统有很多，要求客户全都掌握，显然不现实。为了能够屏蔽底层存储实现的细节，方便用户使用， kubernetes引入PV和PVC两种资源对象。\nPV（Persistent Volume）是持久化卷的意思，是对底层的共享存储的一种抽象。一般情况下PV由kubernetes管理员进行创建和配置，它与底层具体的共享存储技术有关，并通过插件完成与共享存储的对接。\nPVC（Persistent Volume Claim）是持久卷声明的意思，是用户对于存储需求的一种声明。换句话说，PVC其实就是用户向kubernetes系统发出的一种资源需求申请。\n使用了PV和PVC之后，工作可以得到进一步的细分：\n存储：存储工程师维护 PV： kubernetes管理员维护 PVC：kubernetes用户维护 8.2.1 PV PV是存储资源的抽象，下面是资源清单文件:\napiVersion: v1 kind: PersistentVolume metadata: name: pv2 spec: nfs: # 存储类型，与底层真正存储对应 capacity: # 存储能力，目前只支持存储空间的设置 storage: 2Gi accessModes: # 访问模式 storageClassName: # 存储类别 persistentVolumeReclaimPolicy: # 回收策略 PV 的关键配置参数说明：\n存储类型\n底层实际存储的类型，kubernetes支持多种存储类型，每种存储类型的配置都有所差异\n存储能力（capacity）\n目前只支持存储空间的设置( storage=1Gi )，不过未来可能会加入IOPS、吞吐量等指标的配置\n访问模式（accessModes）\n用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式：\nReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载 ReadOnlyMany（ROX）： 只读权限，可以被多个节点挂载 ReadWriteMany（RWX）：读写权限，可以被多个节点挂载 需要注意的是，底层不同的存储类型可能支持的访问模式不同\n回收策略（persistentVolumeReclaimPolicy）\n当PV不再被使用了之后，对其的处理方式。目前支持三种策略：\nRetain （保留） 保留数据，需要管理员手工清理数据 Recycle（回收） 清除 PV 中的数据，效果相当于执行 rm -rf /thevolume/* Delete （删除） 与 PV 相连的后端存储完成 volume 的删除操作，当然这常见于云服务商的存储服务 需要注意的是，底层不同的存储类型可能支持的回收策略不同\n存储类别\nPV可以通过storageClassName参数指定一个存储类别\n具有特定类别的PV只能与请求了该类别的PVC进行绑定 未设定类别的PV则只能与不请求任何类别的PVC进行绑定 状态（status）\n一个 PV 的生命周期中，可能会处于4中不同的阶段：\nAvailable（可用）： 表示可用状态，还未被任何 PVC 绑定 Bound（已绑定）： 表示 PV 已经被 PVC 绑定 Released（已释放）： 表示 PVC 被删除，但是资源还未被集群重新声明 Failed（失败）： 表示该 PV 的自动回收失败 实验\n使用NFS作为存储，来演示PV的使用，创建3个PV，对应NFS中的3个暴露的路径。\n准备NFS环境 # 创建目录 [root@nfs ~]# mkdir /root/data/{pv1,pv2,pv3} -pv # 暴露服务 [root@nfs ~]# more /etc/exports /root/data/pv1 192.168.5.0/24(rw,no_root_squash) /root/data/pv2 192.168.5.0/24(rw,no_root_squash) /root/data/pv3 192.168.5.0/24(rw,no_root_squash) # 重启服务 [root@nfs ~]# systemctl restart nfs 创建pv.yaml apiVersion: v1 kind: PersistentVolume metadata: name: pv1 spec: capacity: storage: 1Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain nfs: path: /root/data/pv1 server: 192.168.5.6 --- apiVersion: v1 kind: PersistentVolume metadata: name: pv2 spec: capacity: storage: 2Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain nfs: path: /root/data/pv2 server: 192.168.5.6 --- apiVersion: v1 kind: PersistentVolume metadata: name: pv3 spec: capacity: storage: 3Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain nfs: path: /root/data/pv3 server: 192.168.5.6 # 创建 pv [root@k8s-master01 ~]# kubectl create -f pv.yaml persistentvolume/pv1 created persistentvolume/pv2 created persistentvolume/pv3 created # 查看pv [root@k8s-master01 ~]# kubectl get pv -o wide NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS AGE VOLUMEMODE pv1 1Gi RWX Retain Available 10s Filesystem pv2 2Gi RWX Retain Available 10s Filesystem pv3 3Gi RWX Retain Available 9s Filesystem 8.2.2 PVC PVC是资源的申请，用来声明对存储空间、访问模式、存储类别需求信息。下面是资源清单文件:\napiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc namespace: dev spec: accessModes: # 访问模式 selector: # 采用标签对PV选择 storageClassName: # 存储类别 resources: # 请求空间 requests: storage: 5Gi PVC 的关键配置参数说明：\n访问模式（accessModes） 用于描述用户应用对存储资源的访问权限\n选择条件（selector）\n通过Label Selector的设置，可使PVC对于系统中己存在的PV进行筛选\n存储类别（storageClassName）\nPVC在定义时可以设定需要的后端存储的类别，只有设置了该class的pv才能被系统选出\n资源请求（Resources ）\n描述对存储资源的请求\n实验\n创建pvc.yaml，申请pv apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc1 namespace: dev spec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc2 namespace: dev spec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: pvc3 namespace: dev spec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi # 创建pvc [root@k8s-master01 ~]# kubectl create -f pvc.yaml persistentvolumeclaim/pvc1 created persistentvolumeclaim/pvc2 created persistentvolumeclaim/pvc3 created # 查看pvc [root@k8s-master01 ~]# kubectl get pvc -n dev -o wide NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODE pvc1 Bound pv1 1Gi RWX 15s Filesystem pvc2 Bound pv2 2Gi RWX 15s Filesystem pvc3 Bound pv3 3Gi RWX 15s Filesystem # 查看pv [root@k8s-master01 ~]# kubectl get pv -o wide NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM AGE VOLUMEMODE pv1 1Gi RWx Retain Bound dev/pvc1 3h37m Filesystem pv2 2Gi RWX Retain Bound dev/pvc2 3h37m Filesystem pv3 3Gi RWX Retain Bound dev/pvc3 3h37m Filesystem 创建pods.yaml, 使用pv apiVersion: v1 kind: Pod metadata: name: pod1 namespace: dev spec: containers: - name: busybox image: busybox:1.30 command: [\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;while true;do echo pod1 \u0026gt;\u0026gt; /root/out.txt; sleep 10; done;\u0026#34;] volumeMounts: - name: volume mountPath: /root/ volumes: - name: volume persistentVolumeClaim: claimName: pvc1 readOnly: false --- apiVersion: v1 kind: Pod metadata: name: pod2 namespace: dev spec: containers: - name: busybox image: busybox:1.30 command: [\u0026#34;/bin/sh\u0026#34;,\u0026#34;-c\u0026#34;,\u0026#34;while true;do echo pod2 \u0026gt;\u0026gt; /root/out.txt; sleep 10; done;\u0026#34;] volumeMounts: - name: volume mountPath: /root/ volumes: - name: volume persistentVolumeClaim: claimName: pvc2 readOnly: false # 创建pod [root@k8s-master01 ~]# kubectl create -f pods.yaml pod/pod1 created pod/pod2 created # 查看pod [root@k8s-master01 ~]# kubectl get pods -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE pod1 1/1 Running 0 14s 10.244.1.69 node1 pod2 1/1 Running 0 14s 10.244.1.70 node1 # 查看pvc [root@k8s-master01 ~]# kubectl get pvc -n dev -o wide NAME STATUS VOLUME CAPACITY ACCESS MODES AGE VOLUMEMODE pvc1 Bound pv1 1Gi RWX 94m Filesystem pvc2 Bound pv2 2Gi RWX 94m Filesystem pvc3 Bound pv3 3Gi RWX 94m Filesystem # 查看pv [root@k8s-master01 ~]# kubectl get pv -n dev -o wide NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM AGE VOLUMEMODE pv1 1Gi RWX Retain Bound dev/pvc1 5h11m Filesystem pv2 2Gi RWX Retain Bound dev/pvc2 5h11m Filesystem pv3 3Gi RWX Retain Bound dev/pvc3 5h11m Filesystem # 查看nfs中的文件存储 [root@nfs ~]# more /root/data/pv1/out.txt node1 node1 [root@nfs ~]# more /root/data/pv2/out.txt node2 node2 8.2.3 生命周期 PVC和PV是一一对应的，PV和PVC之间的相互作用遵循以下生命周期：\n资源供应：管理员手动创建底层存储和PV\n资源绑定：用户创建PVC，kubernetes负责根据PVC的声明去寻找PV，并绑定\n在用户定义好PVC之后，系统将根据PVC对存储资源的请求在已存在的PV中选择一个满足条件的\n一旦找到，就将该PV与用户定义的PVC进行绑定，用户的应用就可以使用这个PVC了 如果找不到，PVC则会无限期处于Pending状态，直到等到系统管理员创建了一个符合其要求的PV PV一旦绑定到某个PVC上，就会被这个PVC独占，不能再与其他PVC进行绑定了\n资源使用：用户可在pod中像volume一样使用pvc\nPod使用Volume的定义，将PVC挂载到容器内的某个路径进行使用。\n资源释放：用户删除pvc来释放pv\n当存储资源使用完毕后，用户可以删除PVC，与该PVC绑定的PV将会被标记为“已释放”，但还不能立刻与其他PVC进行绑定。通过之前PVC写入的数据可能还被留在存储设备上，只有在清除之后该PV才能再次使用。\n资源回收：kubernetes根据pv设置的回收策略进行资源的回收\n对于PV，管理员可以设定回收策略，用于设置与之绑定的PVC释放资源之后如何处理遗留数据的问题。只有PV的存储空间完成回收，才能供新的PVC绑定和使用\n8.3 配置存储 8.3.1 ConfigMap ConfigMap是一种比较特殊的存储卷，它的主要作用是用来存储配置信息的。\n创建configmap.yaml，内容如下：\napiVersion: v1 kind: ConfigMap metadata: name: configmap namespace: dev data: info: | username:admin password:123456 接下来，使用此配置文件创建configmap\n# 创建configmap [root@k8s-master01 ~]# kubectl create -f configmap.yaml configmap/configmap created # 查看configmap详情 [root@k8s-master01 ~]# kubectl describe cm configmap -n dev Name: configmap Namespace: dev Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Data ==== info: ---- username:admin password:123456 Events: \u0026lt;none\u0026gt; 接下来创建一个pod-configmap.yaml，将上面创建的configmap挂载进去\napiVersion: v1 kind: Pod metadata: name: pod-configmap namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 volumeMounts: # 将configmap挂载到目录 - name: config mountPath: /configmap/config volumes: # 引用configmap - name: config configMap: name: configmap # 创建pod [root@k8s-master01 ~]# kubectl create -f pod-configmap.yaml pod/pod-configmap created # 查看pod [root@k8s-master01 ~]# kubectl get pod pod-configmap -n dev NAME READY STATUS RESTARTS AGE pod-configmap 1/1 Running 0 6s #进入容器 [root@k8s-master01 ~]# kubectl exec -it pod-configmap -n dev /bin/sh # cd /configmap/config/ # ls info # more info username:admin password:123456 # 可以看到映射已经成功，每个configmap都映射成了一个目录 # key---\u0026gt;文件 value----\u0026gt;文件中的内容 # 此时如果更新configmap的内容, 容器中的值也会动态更新 8.3.2 Secret 在kubernetes中，还存在一种和ConfigMap非常类似的对象，称为Secret对象。它主要用于存储敏感信息，例如密码、秘钥、证书等等。\n首先使用base64对数据进行编码 [root@k8s-master01 ~]# echo -n \u0026#39;admin\u0026#39; | base64 #准备username YWRtaW4= [root@k8s-master01 ~]# echo -n \u0026#39;123456\u0026#39; | base64 #准备password MTIzNDU2 接下来编写secret.yaml，并创建Secret apiVersion: v1 kind: Secret metadata: name: secret namespace: dev type: Opaque data: username: YWRtaW4= password: MTIzNDU2 # 创建secret [root@k8s-master01 ~]# kubectl create -f secret.yaml secret/secret created # 查看secret详情 [root@k8s-master01 ~]# kubectl describe secret secret -n dev Name: secret Namespace: dev Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Type: Opaque Data ==== password: 6 bytes username: 5 bytes 创建pod-secret.yaml，将上面创建的secret挂载进去： apiVersion: v1 kind: Pod metadata: name: pod-secret namespace: dev spec: containers: - name: nginx image: nginx:1.17.1 volumeMounts: # 将secret挂载到目录 - name: config mountPath: /secret/config volumes: - name: config secret: secretName: secret # 创建pod [root@k8s-master01 ~]# kubectl create -f pod-secret.yaml pod/pod-secret created # 查看pod [root@k8s-master01 ~]# kubectl get pod pod-secret -n dev NAME READY STATUS RESTARTS AGE pod-secret 1/1 Running 0 2m28s # 进入容器，查看secret信息，发现已经自动解码了 [root@k8s-master01 ~]# kubectl exec -it pod-secret /bin/sh -n dev / # ls /secret/config/ password username / # more /secret/config/username admin / # more /secret/config/password 123456 至此，已经实现了利用secret实现了信息的编码。\n9. 安全认证 9.1 访问控制概述 Kubernetes作为一个分布式集群的管理工具，保证集群的安全性是其一个重要的任务。所谓的安全性其实就是保证对Kubernetes的各种客户端进行认证和鉴权操作。\n客户端\n在Kubernetes集群中，客户端通常有两类：\nUser Account：一般是独立于kubernetes之外的其他服务管理的用户账号。 Service Account：kubernetes管理的账号，用于为Pod中的服务进程在访问Kubernetes时提供身份标识。 认证、授权与准入控制\nApiServer是访问及管理资源对象的唯一入口。任何一个请求访问ApiServer，都要经过下面三个流程：\nAuthentication（认证）：身份鉴别，只有正确的账号才能够通过认证 Authorization（授权）： 判断用户是否有权限对访问的资源执行特定的动作 Admission Control（准入控制）：用于补充授权机制以实现更加精细的访问控制功能。 9.2 认证管理 Kubernetes集群安全的最关键点在于如何识别并认证客户端身份，它提供了3种客户端身份认证方式：\nHTTP Base认证：通过用户名+密码的方式认证\n这种认证方式是把“用户名:密码”用BASE64算法进行编码后的字符串放在HTTP请求中的Header Authorization域里发送给服务端。服务端收到后进行解码，获取用户名及密码，然后进行用户身份认证的过程。 HTTP Token认证：通过一个Token来识别合法用户\n这种认证方式是用一个很长的难以被模仿的字符串--Token来表明客户身份的一种方式。每个Token对应一个用户名，当客户端发起API调用请求时，需要在HTTP Header里放入Token，API Server接到Token后会跟服务器中保存的token进行比对，然后进行用户身份认证的过程。 HTTPS证书认证：基于CA根证书签名的双向数字证书认证方式\n这种认证方式是安全性最高的一种方式，但是同时也是操作起来最麻烦的一种方式。 HTTPS认证大体分为3个过程：\n证书申请和下发\nHTTPS通信双方的服务器向CA机构申请证书，CA机构下发根证书、服务端证书及私钥给申请者 客户端和服务端的双向认证\n1\u0026gt; 客户端向服务器端发起请求，服务端下发自己的证书给客户端， 客户端接收到证书后，通过私钥解密证书，在证书中获得服务端的公钥， 客户端利用服务器端的公钥认证证书中的信息，如果一致，则认可这个服务器 2\u0026gt; 客户端发送自己的证书给服务器端，服务端接收到证书后，通过私钥解密证书， 在证书中获得客户端的公钥，并用该公钥认证证书信息，确认客户端是否合法 服务器端和客户端进行通信\n服务器端和客户端协商好加密方案后，客户端会产生一个随机的秘钥并加密，然后发送到服务器端。 服务器端接收这个秘钥后，双方接下来通信的所有内容都通过该随机秘钥加密 注意: Kubernetes允许同时配置多种认证方式，只要其中任意一个方式认证通过即可\n9.3 授权管理 授权发生在认证成功之后，通过认证就可以知道请求用户是谁， 然后Kubernetes会根据事先定义的授权策略来决定用户是否有权限访问，这个过程就称为授权。\n每个发送到ApiServer的请求都带上了用户和资源的信息：比如发送请求的用户、请求的路径、请求的动作等，授权就是根据这些信息和授权策略进行比较，如果符合策略，则认为授权通过，否则会返回错误。\nAPI Server目前支持以下几种授权策略：\nAlwaysDeny：表示拒绝所有请求，一般用于测试 AlwaysAllow：允许接收所有请求，相当于集群不需要授权流程（Kubernetes默认的策略） ABAC：基于属性的访问控制，表示使用用户配置的授权规则对用户请求进行匹配和控制 Webhook：通过调用外部REST服务对用户进行授权 Node：是一种专用模式，用于对kubelet发出的请求进行访问控制 RBAC：基于角色的访问控制（kubeadm安装方式下的默认选项） RBAC(Role-Based Access Control) 基于角色的访问控制，主要是在描述一件事情：给哪些对象授予了哪些权限\n其中涉及到了下面几个概念：\n对象：User、Groups、ServiceAccount 角色：代表着一组定义在资源上的可操作动作(权限)的集合 绑定：将定义好的角色跟用户绑定在一起 RBAC引入了4个顶级资源对象：\nRole、ClusterRole：角色，用于指定一组权限 RoleBinding、ClusterRoleBinding：角色绑定，用于将角色（权限）赋予给对象 Role、ClusterRole\n一个角色就是一组权限的集合，这里的权限都是许可形式的（白名单）。\n# Role只能对命名空间内的资源进行授权，需要指定nameapce kind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: namespace: dev name: authorization-role rules: - apiGroups: [\u0026#34;\u0026#34;] # 支持的API组列表,\u0026#34;\u0026#34; 空字符串，表示核心API群 resources: [\u0026#34;pods\u0026#34;] # 支持的资源对象列表 verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] # 允许的对资源对象的操作方法列表 # ClusterRole可以对集群范围内资源、跨namespaces的范围资源、非资源类型进行授权 kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: authorization-clusterrole rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] 需要详细说明的是，rules中的参数：\napiGroups: 支持的API组列表\n\u0026#34;\u0026#34;,\u0026#34;apps\u0026#34;, \u0026#34;autoscaling\u0026#34;, \u0026#34;batch\u0026#34; resources：支持的资源对象列表\n\u0026#34;services\u0026#34;, \u0026#34;endpoints\u0026#34;, \u0026#34;pods\u0026#34;,\u0026#34;secrets\u0026#34;,\u0026#34;configmaps\u0026#34;,\u0026#34;crontabs\u0026#34;,\u0026#34;deployments\u0026#34;,\u0026#34;jobs\u0026#34;, \u0026#34;nodes\u0026#34;,\u0026#34;rolebindings\u0026#34;,\u0026#34;clusterroles\u0026#34;,\u0026#34;daemonsets\u0026#34;,\u0026#34;replicasets\u0026#34;,\u0026#34;statefulsets\u0026#34;, \u0026#34;horizontalpodautoscalers\u0026#34;,\u0026#34;replicationcontrollers\u0026#34;,\u0026#34;cronjobs\u0026#34; verbs：对资源对象的操作方法列表\n\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;, \u0026#34;exec\u0026#34; RoleBinding、ClusterRoleBinding\n角色绑定用来把一个角色绑定到一个目标对象上，绑定目标可以是User、Group或者ServiceAccount。\n# RoleBinding可以将同一namespace中的subject绑定到某个Role下，则此subject即具有该Role定义的权限 kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: authorization-role-binding namespace: dev subjects: - kind: User name: heima apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: authorization-role apiGroup: rbac.authorization.k8s.io # ClusterRoleBinding在整个集群级别和所有namespaces将特定的subject与ClusterRole绑定，授予权限 kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: authorization-clusterrole-binding subjects: - kind: User name: heima apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: authorization-clusterrole apiGroup: rbac.authorization.k8s.io RoleBinding引用ClusterRole进行授权\nRoleBinding可以引用ClusterRole，对属于同一命名空间内ClusterRole定义的资源主体进行授权。\n一种很常用的做法就是，集群管理员为集群范围预定义好一组角色（ClusterRole），然后在多个命名空间中重复使用这些ClusterRole。这样可以大幅提高授权管理工作效率，也使得各个命名空间下的基础性授权规则与使用体验保持一致。 # 虽然authorization-clusterrole是一个集群角色，但是因为使用了RoleBinding # 所以heima只能读取dev命名空间中的资源 kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: authorization-role-binding-ns namespace: dev subjects: - kind: User name: heima apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: authorization-clusterrole apiGroup: rbac.authorization.k8s.io 实战：创建一个只能管理dev空间下Pods资源的账号\n创建账号 # 1) 创建证书 [root@k8s-master01 pki]# cd /etc/kubernetes/pki/ [root@k8s-master01 pki]# (umask 077;openssl genrsa -out devman.key 2048) # 2) 用apiserver的证书去签署 # 2-1) 签名申请，申请的用户是devman,组是devgroup [root@k8s-master01 pki]# openssl req -new -key devman.key -out devman.csr -subj \u0026#34;/CN=devman/O=devgroup\u0026#34; # 2-2) 签署证书 [root@k8s-master01 pki]# openssl x509 -req -in devman.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out devman.crt -days 3650 # 3) 设置集群、用户、上下文信息 [root@k8s-master01 pki]# kubectl config set-cluster kubernetes --embed-certs=true --certificate-authority=/etc/kubernetes/pki/ca.crt --server=https://192.168.109.100:6443 [root@k8s-master01 pki]# kubectl config set-credentials devman --embed-certs=true --client-certificate=/etc/kubernetes/pki/devman.crt --client-key=/etc/kubernetes/pki/devman.key [root@k8s-master01 pki]# kubectl config set-context devman@kubernetes --cluster=kubernetes --user=devman # 切换账户到devman [root@k8s-master01 pki]# kubectl config use-context devman@kubernetes Switched to context \u0026#34;devman@kubernetes\u0026#34;. # 查看dev下pod，发现没有权限 [root@k8s-master01 pki]# kubectl get pods -n dev Error from server (Forbidden): pods is forbidden: User \u0026#34;devman\u0026#34; cannot list resource \u0026#34;pods\u0026#34; in API group \u0026#34;\u0026#34; in the namespace \u0026#34;dev\u0026#34; # 切换到admin账户 [root@k8s-master01 pki]# kubectl config use-context kubernetes-admin@kubernetes Switched to context \u0026#34;kubernetes-admin@kubernetes\u0026#34;. 2） 创建Role和RoleBinding，为devman用户授权\nkind: Role apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: namespace: dev name: dev-role rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: authorization-role-binding namespace: dev subjects: - kind: User name: devman apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: dev-role apiGroup: rbac.authorization.k8s.io [root@k8s-master01 pki]# kubectl create -f dev-role.yaml role.rbac.authorization.k8s.io/dev-role created rolebinding.rbac.authorization.k8s.io/authorization-role-binding created 切换账户，再次验证 # 切换账户到devman [root@k8s-master01 pki]# kubectl config use-context devman@kubernetes Switched to context \u0026#34;devman@kubernetes\u0026#34;. # 再次查看 [root@k8s-master01 pki]# kubectl get pods -n dev NAME READY STATUS RESTARTS AGE nginx-deployment-66cb59b984-8wp2k 1/1 Running 0 4d1h nginx-deployment-66cb59b984-dc46j 1/1 Running 0 4d1h nginx-deployment-66cb59b984-thfck 1/1 Running 0 4d1h # 为了不影响后面的学习,切回admin账户 [root@k8s-master01 pki]# kubectl config use-context kubernetes-admin@kubernetes Switched to context \u0026#34;kubernetes-admin@kubernetes\u0026#34;. 9.4 准入控制 通过了前面的认证和授权之后，还需要经过准入控制处理通过之后，apiserver才会处理这个请求。\n准入控制是一个可配置的控制器列表，可以通过在Api-Server上通过命令行设置选择执行哪些准入控制器：\n--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel, DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds 只有当所有的准入控制器都检查通过之后，apiserver才执行该请求，否则返回拒绝。\n当前可配置的Admission Control准入控制如下：\nAlwaysAdmit：允许所有请求 AlwaysDeny：禁止所有请求，一般用于测试 AlwaysPullImages：在启动容器之前总去下载镜像 DenyExecOnPrivileged：它会拦截所有想在Privileged Container上执行命令的请求 ImagePolicyWebhook：这个插件将允许后端的一个Webhook程序来完成admission controller的功能。 Service Account：实现ServiceAccount实现了自动化 SecurityContextDeny：这个插件将使用SecurityContext的Pod中的定义全部失效 ResourceQuota：用于资源配额管理目的，观察所有请求，确保在namespace上的配额不会超标 LimitRanger：用于资源限制管理，作用于namespace上，确保对Pod进行资源限制 InitialResources：为未设置资源请求与限制的Pod，根据其镜像的历史资源的使用情况进行设置 NamespaceLifecycle：如果尝试在一个不存在的namespace中创建资源对象，则该创建请求将被拒绝。当删除一个namespace时，系统将会删除该namespace中所有对象。 DefaultStorageClass：为了实现共享存储的动态供应，为未指定StorageClass或PV的PVC尝试匹配默认的StorageClass，尽可能减少用户在申请PVC时所需了解的后端存储细节 DefaultTolerationSeconds：这个插件为那些没有设置forgiveness tolerations并具有notready:NoExecute和unreachable:NoExecute两种taints的Pod设置默认的“容忍”时间，为5min PodSecurityPolicy：这个插件用于在创建或修改Pod时决定是否根据Pod的security context和可用的PodSecurityPolicy对Pod的安全策略进行控制 10. DashBoard 之前在kubernetes中完成的所有操作都是通过命令行工具kubectl完成的。其实，为了提供更丰富的用户体验，kubernetes还开发了一个基于web的用户界面（Dashboard）。用户可以使用Dashboard部署容器化的应用，还可以监控应用的状态，执行故障排查以及管理kubernetes中各种资源。\n10.1 部署Dashboard 下载yaml，并运行Dashboard # 下载yaml [root@k8s-master01 ~]# wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml # 修改kubernetes-dashboard的Service类型 kind: Service apiVersion: v1 metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboard spec: type: NodePort # 新增 ports: - port: 443 targetPort: 8443 nodePort: 30009 # 新增 selector: k8s-app: kubernetes-dashboard # 部署 [root@k8s-master01 ~]# kubectl create -f recommended.yaml # 查看namespace下的kubernetes-dashboard下的资源 [root@k8s-master01 ~]# kubectl get pod,svc -n kubernetes-dashboard NAME READY STATUS RESTARTS AGE pod/dashboard-metrics-scraper-c79c65bb7-zwfvw 1/1 Running 0 111s pod/kubernetes-dashboard-56484d4c5-z95z5 1/1 Running 0 111s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/dashboard-metrics-scraper ClusterIP 10.96.89.218 \u0026lt;none\u0026gt; 8000/TCP 111s service/kubernetes-dashboard NodePort 10.104.178.171 \u0026lt;none\u0026gt; 443:30009/TCP 111s 2）创建访问账户，获取token\n# 创建账号 [root@k8s-master01-1 ~]# kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard # 授权 [root@k8s-master01-1 ~]# kubectl create clusterrolebinding dashboard-admin-rb --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin # 获取账号token [root@k8s-master01 ~]# kubectl get secrets -n kubernetes-dashboard | grep dashboard-admin dashboard-admin-token-xbqhh kubernetes.io/service-account-token 3 2m35s [root@k8s-master01 ~]# kubectl describe secrets dashboard-admin-token-xbqhh -n kubernetes-dashboard Name: dashboard-admin-token-xbqhh Namespace: kubernetes-dashboard Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: dashboard-admin kubernetes.io/service-account.uid: 95d84d80-be7a-4d10-a2e0-68f90222d039 Type: kubernetes.io/service-account-token Data ==== namespace: 20 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6ImJrYkF4bW5XcDhWcmNGUGJtek5NODFuSXl1aWptMmU2M3o4LTY5a2FKS2cifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4teGJxaGgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiOTVkODRkODAtYmU3YS00ZDEwLWEyZTAtNjhmOTAyMjJkMDM5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.NAl7e8ZfWWdDoPxkqzJzTB46sK9E8iuJYnUI9vnBaY3Jts7T1g1msjsBnbxzQSYgAG--cV0WYxjndzJY_UWCwaGPrQrt_GunxmOK9AUnzURqm55GR2RXIZtjsWVP2EBatsDgHRmuUbQvTFOvdJB4x3nXcYLN2opAaMqg3rnU2rr-A8zCrIuX_eca12wIp_QiuP3SF-tzpdLpsyRfegTJZl6YnSGyaVkC9id-cxZRb307qdCfXPfCHR_2rt5FVfxARgg_C0e3eFHaaYQO7CitxsnIoIXpOFNAR8aUrmopJyODQIPqBWUehb7FhlU1DCduHnIIXVC_UICZ-MKYewBDLw ca.crt: 1025 bytes 3）通过浏览器访问Dashboard的UI\n在登录页面上输入上面的token\n出现下面的页面代表成功\n10.2 使用DashBoard 本章节以Deployment为例演示DashBoard的使用\n查看\n选择指定的命名空间dev，然后点击Deployments，查看dev空间下的所有deployment\n扩缩容\n在Deployment上点击规模，然后指定目标副本数量，点击确定\n编辑\n在Deployment上点击编辑，然后修改yaml文件，点击确定\n查看Pod\n点击Pods, 查看pods列表\n操作Pod\n选中某个Pod，可以对其执行日志（logs）、进入执行（exec）、编辑、删除操作\nDashboard提供了kubectl的绝大部分功能，这里不再一一演示\n","permalink":"http://hugo.itshare.work/posts/kubernetes/kubernetes/","summary":"Kubernetes详细教程 1. Kubernetes介绍 1.1 应用部署方式演变 在部署应用程序的方式上，主要经历了三个时代： 传统部署：互联网早期，会直接将应用程序部署在物理机上 优点：简单，不需要其它技术的参与 缺点：不能为应用程序定义资源使用边界，很难合理地分配计算资源，而且程序之间容易产","title":"Kubernetes详细教程"},{"content":"Nginx 简介 一、Nginx概述 1.1 概述 Nginx（“engine x”）是一个高性能的 HTTP /反向代理的服务器及电子邮件（IMAP/POP3)代理服务器。官方测试nginx能够支撑5万并发，并且cpu，内存等资源消耗却非常低，运行非常稳定。最重要的是开源，免费，可商用的。\nNginx还支持热部署，几乎可以做到7 * 24 小时不间断运行，即时运行数个月也不需要重启，还能够在不间断服务的情况下对软件进行升级维护。\n1.2 Nginx应用场景 虚拟主机：一台服务器虚拟出多个网站。\n静态资源服务：提供http资源访问服务。\n反向代理，负载均衡。当网站的访问量达到一定程度后，单台服务器不能满足用户的请求时，需要yo哪个多台服务器集群可以使用nginx做反向代理。并且多台服务器可以平均分担负载，不会因为某台服务器负载高宕机而某台服务器闲置的情况。\n1.3 正向代理 正向代理：一般的访问流程是客户端直接向目标服务器发送请求并获取内容，使用正向代理后，客户端通过配置或其他方式改为向代理服务器发送请求，并指定目标服务器（原始服务器），然后由代理服务器和原始服务器通信，转交请求并获得的内容，再返回给客户端。正向代理隐藏了真实的客户端，为客户端收发请求，使真实客户端对服务器不可见；\n1.4 反向代理 **反向代理：**正好相反。对于客户端来说，反向代理就好像目标服务器。并且客户端不需要进行任何设置。客户端向反向代理发送请求，接着反向代理判断请求走向何处，并将请求转交给客户端，使得这些内容就好像它自己的一样，一次客户端并会并会不感知到反向代理后面的服务，因此不需要客户端做任何设置，只需要把反向代理服务器当成真正的服务器就好了。\n1.5 负载均衡 负载均衡建立在现有网络结构之上，它提供一种链家有效透明的方法扩展网络设备和服务器的宽带、增加吞吐量，加强网络数据处理能力，提高网络的灵活性和可用性。\n1.6 动静分离 为了加快网站的解析速度，可以把动态页面和静态页面由不同的服务器来解析，加快解析速度，降低原来单个服务器的压力。一般来说，都需要将动态资源和静态资源分开，由于Nginx的高并发和静态资源缓存等特性，经常将静态资源部署在Nginx上。如果请求的是静态资源，直接到静态资源目录获取资源，如果是童泰资源的请求，则利用反向代理的原理，把请求转发给对应后台应用去处理，从而实现动静分离。\n二、Nginx安装 2.1 进入官网下载 2.2 安装相关依赖 2.2.1 第一步 1、 安装pcre\nwget http://downloads.sourceforge.net/project/pcre/pcre/8.37/pcre-8.37.tar.gz 2、解压文件 tar -zxvf 路径\n3、pcre主目录执行命令 :\n./configure ​ 可能遇到的情况：没有c++支持\n安装c++支持：\nyum install -y gcc gcc-c++ 4、完成后、回到pcre目录下执行\nmake \u0026amp;\u0026amp; make install 5、查看版本 :\npcre-config --version 2.2.2 第二步，安装其他依赖 zlib openssl\nyum -y install make zlib zlib-devel gcc-c++ libtool openssl openssl-devel 2.3 安装nginx 解压nginx-xx.tar.gz包\ntar -zxvf nginx-xxx.tar.gz 进入解压目录，执行./configure\n./configure make\u0026amp;\u0026amp;make install\nmake \u0026amp;\u0026amp; make install 2.3 开放端口 #查看开放的端口号 firewall-cmd --list-all #设置开放的端口号 firewall-cmd --add-service=http –permanent sudo firewall-cmd --add-port=80/tcp --permanent #重启防火墙 firewall-cmd –reload 2.4 访问 三、nginx常用命令和配置文件 3.1 常用命令 #查看版本 在/usr/local/nginx/sbin 目录下执行 ./nginx -v #启动nginx 在/usr/local/nginx/sbin 目录下执行 ./nginx #关闭nginx 在/usr/local/nginx/sbin 目录下执行 ./nginx -s stop #重加载nginx 在/usr/local/nginx/sbin 目录下执行 ./nginx -s reload 3.2 配置文件详细讲解 #配置文件位置 位置：/usr/local/nginx/conf/nginx.conf ######Nginx配置文件nginx.conf中文详解##### #定义Nginx运行的用户和用户组 user www www; #nginx进程数，建议设置为等于CPU总核心数。 worker_processes 8; #全局错误日志定义类型，[ debug | info | notice | warn | error | crit ] error_log /usr/local/nginx/logs/error.log info; #进程pid文件 pid /usr/local/nginx/logs/nginx.pid; #指定进程可以打开的最大描述符：数目 #工作模式与连接数上限 #这个指令是指当一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（ulimit -n）与nginx进程数相除，但是nginx分配请求并不是那么均匀，所以最好与ulimit -n 的值保持一致。 #现在在linux 2.6内核下开启文件打开数为65535，worker_rlimit_nofile就相应应该填写65535。 #这是因为nginx调度时分配请求到进程并不是那么的均衡，所以假如填写10240，总并发量达到3-4万时就有进程可能超过10240了，这时会返回502错误。 worker_rlimit_nofile 65535; events { #参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; epoll模型 #是Linux 2.6以上版本内核中的高性能网络I/O模型，linux建议epoll，如果跑在FreeBSD上面，就用kqueue模型。 #补充说明： #与apache相类，nginx针对不同的操作系统，有不同的事件模型 #A）标准事件模型 #Select、poll属于标准事件模型，如果当前系统不存在更有效的方法，nginx会选择select或poll #B）高效事件模型 #Kqueue：使用于FreeBSD 4.1+, OpenBSD 2.9+, NetBSD 2.0 和 MacOS X.使用双处理器的MacOS X系统使用kqueue可能会造成内核崩溃。 #Epoll：使用于Linux内核2.6版本及以后的系统。 #/dev/poll：使用于Solaris 7 11/99+，HP/UX 11.22+ (eventport)，IRIX 6.5.15+ 和 Tru64 UNIX 5.1A+。 #Eventport：使用于Solaris 10。 为了防止出现内核崩溃的问题， 有必要安装安全补丁。 use epoll; #单个进程最大连接数（最大连接数=连接数*进程数） #根据硬件调整，和前面工作进程配合起来用，尽量大，但是别把cpu跑到100%就行。每个进程允许的最多连接数，理论上每台nginx服务器的最大连接数为。 worker_connections 65535; #keepalive超时时间。 keepalive_timeout 60; #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求头的大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。 #分页大小可以用命令getconf PAGESIZE 取得。 #[root@web001 ~]# getconf PAGESIZE #4096 #但也有client_header_buffer_size超过4k的情况，但是client_header_buffer_size该值必须设置为“系统分页大小”的整倍数。 client_header_buffer_size 4k; #这个将为打开文件指定缓存，默认是没有启用的，max指定缓存数量，建议和打开文件数一致，inactive是指经过多长时间文件没被请求后删除缓存。 open_file_cache max=65535 inactive=60s; #这个是指多长时间检查一次缓存的有效信息。 #语法:open_file_cache_valid time 默认值:open_file_cache_valid 60 使用字段:http, server, location 这个指令指定了何时需要检查open_file_cache中缓存项目的有效信息. open_file_cache_valid 80s; #open_file_cache指令中的inactive参数时间内文件的最少使用次数，如果超过这个数字，文件描述符一直是在缓存中打开的，如上例，如果有一个文件在inactive时间内一次没被使用，它将被移除。 #语法:open_file_cache_min_uses number 默认值:open_file_cache_min_uses 1 使用字段:http, server, location 这个指令指定了在open_file_cache指令无效的参数中一定的时间范围内可以使用的最小文件数,如果使用更大的值,文件描述符在cache中总是打开状态. open_file_cache_min_uses 1; #语法:open_file_cache_errors on | off 默认值:open_file_cache_errors off 使用字段:http, server, location 这个指令指定是否在搜索一个文件是记录cache错误. open_file_cache_errors on; } #设定http服务器，利用它的反向代理功能提供负载均衡支持 http { #文件扩展名与文件类型映射表 include mime.types; #默认文件类型 default_type application/octet-stream; #默认编码 #charset utf-8; #服务器名字的hash表大小 #保存服务器名字的hash表是由指令server_names_hash_max_size 和server_names_hash_bucket_size所控制的。参数hash bucket size总是等于hash表的大小，并且是一路处理器缓存大小的倍数。在减少了在内存中的存取次数后，使在处理器中加速查找hash表键值成为可能。如果hash bucket size等于一路处理器缓存的大小，那么在查找键的时候，最坏的情况下在内存中查找的次数为2。第一次是确定存储单元的地址，第二次是在存储单元中查找键 值。因此，如果Nginx给出需要增大hash max size 或 hash bucket size的提示，那么首要的是增大前一个参数的大小. server_names_hash_bucket_size 128; #客户端请求头部的缓冲区大小。这个可以根据你的系统分页大小来设置，一般一个请求的头部大小不会超过1k，不过由于一般系统分页都要大于1k，所以这里设置为分页大小。分页大小可以用命令getconf PAGESIZE取得。 client_header_buffer_size 32k; #客户请求头缓冲大小。nginx默认会用client_header_buffer_size这个buffer来读取header值，如果header过大，它会使用large_client_header_buffers来读取。 large_client_header_buffers 4 64k; #设定通过nginx上传文件的大小 client_max_body_size 8m; #开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成off。 #sendfile指令指定 nginx 是否调用sendfile 函数（zero copy 方式）来输出文件，对于普通应用，必须设为on。如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络IO处理速度，降低系统uptime。 sendfile on; #开启目录列表访问，合适下载服务器，默认关闭。 autoindex on; #此选项允许或禁止使用socke的TCP_CORK的选项，此选项仅在使用sendfile的时候使用 tcp_nopush on; tcp_nodelay on; #长连接超时时间，单位是秒 keepalive_timeout 120; #FastCGI相关参数是为了改善网站的性能：减少资源占用，提高访问速度。下面参数看字面意思都能理解。 fastcgi_connect_timeout 300; fastcgi_send_timeout 300; fastcgi_read_timeout 300; fastcgi_buffer_size 64k; fastcgi_buffers 4 64k; fastcgi_busy_buffers_size 128k; fastcgi_temp_file_write_size 128k; #gzip模块设置 gzip on; #开启gzip压缩输出 gzip_min_length 1k; #最小压缩文件大小 gzip_buffers 4 16k; #压缩缓冲区 gzip_http_version 1.0; #压缩版本（默认1.1，前端如果是squid2.5请使用1.0） gzip_comp_level 2; #压缩等级 gzip_types text/plain application/x-javascript text/css application/xml; #压缩类型，默认就已经包含textml，所以下面就不用再写了，写上去也不会有问题，但是会有一个warn。 gzip_vary on; #开启限制IP连接数的时候需要使用 #limit_zone crawler $binary_remote_addr 10m; #负载均衡配置 upstream jh.w3cschool.cn { #upstream的负载均衡，weight是权重，可以根据机器配置定义权重。weigth参数表示权值，权值越高被分配到的几率越大。 server 192.168.80.121:80 weight=3; server 192.168.80.122:80 weight=2; server 192.168.80.123:80 weight=3; #nginx的upstream目前支持4种方式的分配 #1、轮询（默认） #每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 #2、weight #指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。 #例如： #upstream bakend { # server 192.168.0.14 weight=10; # server 192.168.0.15 weight=10; #} #2、ip_hash #每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。 #例如： #upstream bakend { # ip_hash; # server 192.168.0.14:88; # server 192.168.0.15:80; #} #3、fair（第三方） #按后端服务器的响应时间来分配请求，响应时间短的优先分配。 #upstream backend { # server server1; # server server2; # fair; #} #4、url_hash（第三方） #按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 #例：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法 #upstream backend { # server squid1:3128; # server squid2:3128; # hash $request_uri; # hash_method crc32; #} #tips: #upstream bakend{#定义负载均衡设备的Ip及设备状态}{ # ip_hash; # server 127.0.0.1:9090 down; # server 127.0.0.1:8080 weight=2; # server 127.0.0.1:6060; # server 127.0.0.1:7070 backup; #} #在需要使用负载均衡的server中增加 proxy_pass http://bakend/; #每个设备的状态设置为: #1.down表示单前的server暂时不参与负载 #2.weight为weight越大，负载的权重就越大。 #3.max_fails：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream模块定义的错误 #4.fail_timeout:max_fails次失败后，暂停的时间。 #5.backup： 其它所有的非backup机器down或者忙的时候，请求backup机器。所以这台机器压力会最轻。 #nginx支持同时设置多组的负载均衡，用来给不用的server来使用。 #client_body_in_file_only设置为On 可以讲client post过来的数据记录到文件中用来做debug #client_body_temp_path设置记录文件的目录 可以设置最多3层目录 #location对URL进行匹配.可以进行重定向或者进行新的代理 负载均衡 } #虚拟主机的配置 server { #监听端口 listen 80; #域名可以有多个，用空格隔开 server_name www.w3cschool.cn w3cschool.cn; index index.html index.htm index.php; root /data/www/w3cschool; #对******进行负载均衡 location ~ .*.(php|php5)?$ { fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fastcgi.conf; } #图片缓存时间设置 location ~ .*.(gif|jpg|jpeg|png|bmp|swf)$ { expires 10d; } #JS和CSS缓存时间设置 location ~ .*.(js|css)?$ { expires 1h; } #日志格式设定 #$remote_addr与$http_x_forwarded_for用以记录客户端的ip地址； #$remote_user：用来记录客户端用户名称； #$time_local： 用来记录访问时间与时区； #$request： 用来记录请求的url与http协议； #$status： 用来记录请求状态；成功是200， #$body_bytes_sent ：记录发送给客户端文件主体内容大小； #$http_referer：用来记录从那个页面链接访问过来的； #$http_user_agent：记录客户浏览器的相关信息； #通常web服务器放在反向代理的后面，这样就不能获取到客户的IP地址了，通过$remote_add拿到的IP地址是反向代理服务器的iP地址。反向代理服务器在转发请求的http头信息中，可以增加x_forwarded_for信息，用以记录原有客户端的IP地址和原来客户端的请求的服务器地址。 log_format access \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39; \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; \u0026#39;\u0026#34;$http_user_agent\u0026#34; $http_x_forwarded_for\u0026#39;; #定义本虚拟主机的访问日志 access_log /usr/local/nginx/logs/host.access.log main; access_log /usr/local/nginx/logs/host.access.404.log log404; #对 \u0026#34;/\u0026#34; 启用反向代理 location / { proxy_pass http://127.0.0.1:88; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; #以下是一些反向代理的配置，可选。 proxy_set_header Host $host; #允许客户端请求的最大单文件字节数 client_max_body_size 10m; #缓冲区代理缓冲用户端请求的最大字节数， #如果把它设置为比较大的数值，例如256k，那么，无论使用firefox还是IE浏览器，来提交任意小于256k的图片，都很正常。如果注释该指令，使用默认的client_body_buffer_size设置，也就是操作系统页面大小的两倍，8k或者16k，问题就出现了。 #无论使用firefox4.0还是IE8.0，提交一个比较大，200k左右的图片，都返回500 Internal Server Error错误 client_body_buffer_size 128k; #表示使nginx阻止HTTP应答代码为400或者更高的应答。 proxy_intercept_errors on; #后端服务器连接的超时时间_发起握手等候响应超时时间 #nginx跟后端服务器连接超时时间(代理连接超时) proxy_connect_timeout 90; #后端服务器数据回传时间(代理发送超时) #后端服务器数据回传时间_就是在规定时间之内后端服务器必须传完所有的数据 proxy_send_timeout 90; #连接成功后，后端服务器响应时间(代理接收超时) #连接成功后_等候后端服务器响应时间_其实已经进入后端的排队之中等候处理（也可以说是后端服务器处理请求的时间） proxy_read_timeout 90; #设置代理服务器（nginx）保存用户头信息的缓冲区大小 #设置从被代理服务器读取的第一部分应答的缓冲区大小，通常情况下这部分应答中包含一个小的应答头，默认情况下这个值的大小为指令proxy_buffers中指定的一个缓冲区的大小，不过可以将其设置为更小 proxy_buffer_size 4k; #proxy_buffers缓冲区，网页平均在32k以下的设置 #设置用于读取应答（来自被代理服务器）的缓冲区数目和大小，默认情况也为分页大小，根据操作系统的不同可能是4k或者8k proxy_buffers 4 32k; #高负荷下缓冲大小（proxy_buffers*2） proxy_busy_buffers_size 64k; #设置在写入proxy_temp_path时数据的大小，预防一个工作进程在传递文件时阻塞太长 #设定缓存文件夹大小，大于这个值，将从upstream服务器传 proxy_temp_file_write_size 64k; } #设定查看Nginx状态的地址 location /NginxStatus { stub_status on; access_log on; auth_basic \u0026#34;NginxStatus\u0026#34;; auth_basic_user_file confpasswd; #htpasswd文件的内容可以用apache提供的htpasswd工具来产生。 } #本地动静分离反向代理配置 #所有jsp的页面均交由tomcat或resin处理 location ~ .(jsp|jspx|do)?$ { proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://127.0.0.1:8080; } #所有静态文件由nginx直接读取不经过tomcat或resin location ~ .*.(htm|html|gif|jpg|jpeg|png|bmp|swf|ioc|rar|zip|txt|flv|mid|doc|ppt| pdf|xls|mp3|wma)$ { expires 15d; } location ~ .*.(js|css)?$ { expires 1h; } } } ######Nginx配置文件nginx.conf中文详解##### 四、反向代理 4.1 单端口反向代理 实现效果：打开浏览器，在浏览器地址栏输入 www.123.com ，跳转到 linux 系统的 tomcat 主页\n4.1.1 安装 tomcat 将Tomcat安装包复制到linux中\n进入 tomcat 的 bin 目录中，./startup.sh 启动 tomcat 服务器\n对外开放访问的端口号\nfirewall-cmd --add-port=8080/tcp --permanent firewall-cmd -reload 修改本机端口映射\n#1.修改本机 hosts 文件 增加：192.168.0.105 www.123.com 修改nginx配置文件 访问测试 4.2 多端口反向代理 4.2.1 实现效果： 访问 http://192.168.0.105:9001/edu/ 直接跳转到 127.0.0.1:8080 访问 http://192.168.0.105:9001/vod/ 直接跳转到 127.0.0.1:8081 4.2.2 安装两个 Tomcat 分别定义端口为 8001,8002 4.2.3 创建文件夹和测试页面 4.2.4 配置 nginx配置文件 4.2.5 开放端口号 firewall-cmd --add-port=8001/tcp --permanent firewall-cmd --add-port=8002/tcp --permanent firewall-cmd --add-port=9001/tcp --permanent firewall-cmd --reload 4.2.6 测试 1、访问：http://192.168.0.105:9001/edu/a.html\n2、访问：http:// 192.168.0.105:9001/vod/a.html\n4.3 负载均衡 1、准备两台 tomcat 服务器，一台8001，一台8002\n2、 在两台 tomcat 里面 webapps 目录中，创建名称是 edu 文件夹，在 edu 文件夹中创建页面a.html，用于测试\n3、配置nginx配置文件\n# 在http模块中配置 upstream myserver{ server 192.168.0.105:8001 weight=1; server 192.168.0.105:8002 weight=2; } # 在server模块配置 listen 80; server_name 192.168.0.105; location / { proxy_pass http://myserver; root html; index index.html index.htm; } 4、nginx提供了几种分配方式(策略)\n#1.轮询（默认） 每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器 down 掉，能自动剔除。 #2.weight weight 代表权重，默认是1，权重越高被分配的客户端越多。 指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况。例如： upstream server_pool{ server 192.168.5.21 weight=10; server 192.168.5.22 weight=10; } #3. ip_hash 每个请求按访问 ip 的 hash 结果分配，这样每个访客固定一个后端服务器，可以解决session的问题。例如： upstream server_pool{ ip_hash server 192.168.5.21:80; server 192.168.5.22:80; } #4.fair(第三方，需要安装第三方模块) 按后端服务器的响应时间来分配请求，响应时间短的优先分配。 upstream server_pool{ server 192.168.5.21:80; server 192.168.5.22:80; fair; } 5、测试\n五、动静分离 5.1 什么是动静分离 Nginx 动静分离简单来说就是把动态跟静态请求分开，不能理解成只是单纯的把动态页面和\n静态页面物理分离。\n5.2 在linux系统中准备静态资源，用于进行访问 5.3 nginx配置文件 5.4 测试 六、高可用 6.1 Keeplived+Nginx高可用集群（主从模式） 1、需要两台服务器 192.168.0.105 和 192.168.0.102\n2、在两台服务器上安装nginx和keeplived\n#1.安装keepalived yum install keepalived -y #2.keepalived.conf文件 安装之后，在etc里面生成目录 keepalived，有文件 keepalived.conf #3.修改配置文件\n105主服务器\n102副服务器\n#4. 启动keepalived\n#5.测试\n6.2 Keeplived+Nginx 高可用集群（双主模式） 修改配置\n（2） 配置 LB-02 节点\n","permalink":"http://hugo.itshare.work/posts/nginx/nginx/","summary":"Nginx 简介 一、Nginx概述 1.1 概述 Nginx（“engine x”）是一个高性能的 HTTP /反向代理的服务器及电子邮件（IMAP/POP3)代理服务器。官方测试nginx能够支撑5万并发，并且cpu，内存等资源消耗却非常低，运行非常稳定。最重要的是开源，免费，可商用的。 Nginx还支持热部署","title":"Nginx"},{"content":"选择服务器平台 zabbix版本 OS分布 OS版本 zabbix component 数据库 web server 5.0LTS CentOS 7 Server,Forontend,Agent MySQL Nginx 安装和配置zabbix 安装zabbix仓库 # rpm -Uvh https://repo.zabbix.com/zabbix/5.0/rhel/7/x86_64/zabbix-release-5.0-1.el7.noarch.rpm # yum clean all 安装zabbix # 安装Zabbix server，Web前端，agent # yum install zabbix-server-mysql zabbix-agent 安装Zabbix frontend 启用红帽软件集合\n# yum install centos-release-scl 编辑配置文件\n/etc/yum.repos.d/zabbix.repo and enable zabbix-frontend repository. # 修改enable的值为1 [zabbix-frontend] ... enabled=1 ... 安装Zabbix frontend 包\n# yum install zabbix-web-mysql-scl zabbix-nginx-conf-scl 创建初始数据库 注意:MySQL数据库的安装这里不做说明 在数据库主机上运行以下代码\n# mysql -uroot -p password mysql\u0026gt; create database zabbix character set utf8 collate utf8_bin; mysql\u0026gt; create user zabbix@localhost identified by \u0026#39;password\u0026#39;; mysql\u0026gt; grant all privileges on zabbix.* to zabbix@localhost; mysql\u0026gt; set global log_bin_trust_function_creators = 1; mysql\u0026gt; quit; 导入初始架构和数据\n# zcat /usr/share/doc/zabbix-server-mysql*/create.sql.gz | mysql -uzabbix -p zabbix 在导入数据库后禁用log_bin_trust_function_creators选项\n# mysql -uroot -p password mysql\u0026gt; set global log_bin_trust_function_creators = 0; mysql\u0026gt; quit; zabbix server配置数据库 编辑配置文件 /etc/zabbix/zabbix_server.conf\nDBPassword=password zabbix前端配置PHP 编辑配置文件 /etc/opt/rh/rh-nginx116/nginx/conf.d/zabbix.conf uncomment and set \u0026rsquo;listen\u0026rsquo; and \u0026lsquo;server_name\u0026rsquo; directives.\n# listen 80; # server_name example.com; 编辑配置文件 /etc/opt/rh/rh-php72/php-fpm.d/zabbix.conf add nginx to listen.acl_users directive.\nlisten.acl_users = apache,nginx 取消注释，设置正确的时区。\nphp_value[date.timezone] = Asia/Shanghai 启动Zabbix server和agent进程 # systemctl restart zabbix-server zabbix-agent rh-nginx116-nginx rh-php72-php-fpm # systemctl enable zabbix-server zabbix-agent rh-nginx116-nginx rh-php72-php-fpm 开始使用zabbix 浏览器中输入http://example.com/zabbix （即解析的域名或者IP）\n注意：使用过程这里不叙述 ","permalink":"http://hugo.itshare.work/posts/zabbix/zabbix5%E9%83%A8%E7%BD%B2%E5%AE%89%E8%A3%85/","summary":"选择服务器平台 zabbix版本 OS分布 OS版本 zabbix component 数据库 web server 5.0LTS CentOS 7 Server,Forontend,Agent MySQL Nginx 安装和配置zabbix 安装zabbix仓库 # rpm -Uvh https://repo.zabbix.com/zabbix/5.0/rhel/7/x86_64/zabbix-release-5.0-1.el7.noarch.rpm # yum clean all 安装zabbix # 安装Zabbix server，Web前端，agent # yum install zabbix-server-mysql zabbix-agent 安装Zabbix frontend 启用红帽软件集合 # yum install centos-release-scl 编辑配置文件 /etc/yum.repos.d/zabbix.repo and enable zabbix-frontend repository. # 修改","title":"zabbix5部署安装"},{"content":"Playbook playbook介绍 官方链接\nhttps://docs.ansible.com/ansible/latest/user_guide/playbooks_intro.html Playbook 组成 一个 playbook(剧本)文件是一个YAML语言编写的文本文件 通常一个playbook只包括一个play 一个 play的主要包括两部分: 主机和tasks. 即实现在指定一组主机上执行一个tasks定义好的任务列表。 一个tasks中可以有一个或多个task任务 每一个Task本质上就是调用ansible的一个module 在复杂场景中,一个playbook中也可以包括多个play，实现对多组不同的主机执行不同的任务 Playbook 与 Ad-Hoc 对比 Playbook是对多个 AD-Hoc 的一种编排组合的实现方式 Playbook能控制任务执行的先后顺序 Playbook可以持久保存到文件中从而方便多次调用运行，而Ad-Hoc只能临时运行。 Playbook适合复杂的重复性的任务，而Ad-Hoc适合做快速简单的一次性任务 YAML 语言 YAML 语言介绍 YAML：YAML Ain\u0026rsquo;t Markup Language，即YAML不是标记语言。不过，在开发的这种语言时，YAML的 意思其实是：\u0026ldquo;Yet Another Markup Language\u0026rdquo;（仍是一种标记语言） YAML是一个可读性高的用来表达资料序列的格式。 YAML参考了其他多种语言，包括：XML、C语言、Python、Perl以及电子邮件格式RFC2822等。 Clark Evans在2001年在首次发表了这种语言，另外Ingy döt Net与Oren Ben-Kiki也是这语言的共同设计者 目前很多最新的软件比较流行采用此格式的文件存放配置信息，如:ubuntu，anisble，docker，kubernetes等 YAML 官方网站：\nhttp://www.yaml.org ansible 官网:\nhttps://docs.ansible.com/ansible/latest/reference_appendices/YAMLSyntax.html ###　YAML 语言特性\nYAML的可读性好 YAML和脚本语言的交互性好 YAML使用实现语言的数据类型 YAML有一个一致的信息模型 YAML易于实现 YAML可以基于流来处理 YAML表达能力强，扩展性好 YAML语法简介 在单一文件第一行，用连续三个连字号\u0026quot;-\u0026quot; 开始，还有选择性的连续三个点号( \u0026hellip; )用来表示文件结尾 次行开始正常写Playbook的内容，一般建议写明该Playbook的功能 使用#号注释代码 缩进的级别也必须是一致的，同样的缩进代表同样的级别，程序判别配置的级别是通过缩进结行来实现的 缩进不支持tab,必须使用空格进行缩进 缩进的空格数不重要，只要相同层级的元素左对齐即可 YAML文件内容是区别大小写的，key/value的值均需大小写敏感 多个key/value可同行写也可换行写，同行使用，分隔 key后面冒号要加一个空格 比如: key: value value可是个字符串，也可是另一个列表 YAML文件扩展名通常为yml或yaml 支持的数据类型 YAML 支持以下常用几种数据类型：\n标量：单个的、不可再分的值 对象：键值对的集合，又称为: 字典（dictionary）/ 哈希（hashes） / 映射（mapping） 数组：一组按次序排列的值，又称为: 列表（list）/ 序列（sequence） scalar 标量 key对应value\nname: wang age: 18 使用缩进的方式\nname: wang age: 18 标量是最基本的，不可再分的值，包括：\n字符串 布尔值 整数 浮点数 Null 时间 日期 ####　Dictionary 字典 一个字典是由一个或多个key与value构成 key和value之间用冒号 ：分隔 冒号 : 后面有一个空格 所有 k/v 可以放在一行，,每个 k/v 之间用逗号分隔 所有每个 k/v 也可以分别放在不同行,一对k/v放在独立的一行 格式\naccount: { name: wang, age: 30 } 使用缩进方式\naccount: name: wang age: 18 范例：\n#不同行 # An employee record name: Example Developer job: Developer skill: Elite(社会精英) #同一行,也可以将key:value放置于{}中进行表示，用,分隔多个key:value # An employee record {name: \u0026#34;Example Developer\u0026#34;, job: \u0026#34;Developer\u0026#34;, skill: \u0026#34;Elite\u0026#34;} List 列表 列表由多个元素组成 每个元素放在不同行，每个元素一行,且元素前均使用中横线 - 开头，并且中横线 - 和元素之间有一个空格 也可以将所有元素用 [ ] 括起来放在同一行,每个元素之间用逗号分隔 格式\ncourse: [ linux , golang , python ] 也可以写成以 - 开头的多行\ncourse: - linux - golang - python course: - linux: manjaro - golang: gin - python: django 范例：\n#不同行,行以-开头,后面有一个空格 # A list of tasty fruits - Apple - Orange - Strawberry - Mango #同一行 [Apple,Orange,Strawberry,Mango] 范例：YAML 表示一个家庭\nname: John Smith age: 41 gender: Male spouse: { name: Jane Smith, age: 37, gender: Female } # 写在一行里 name: Jane Smith #也可以写成多行 age: 37 gender: Female children: [ {name: Jimmy Smith,age: 17, gender: Male}, {name: Jenny Smith, age:13, gender: Female}, {name: hao Smith, age: 20, gender: Male } ] #写在一行 - name: Jimmy Smith #写在多行,更为推荐的写法 age: 17 gender: Male - {name: Jenny Smith, age: 13, gender: Female} - {name: hao Smith, age: 20, gender: Male } 三种常见的数据格式 XML：Extensible Markup Language，可扩展标记语言，可用于数据交换和配置 JSON：JavaScript Object Notation, JavaScript 对象表记法，主要用来数据交换或配置，不支持注释 YAML：YAML Ain\u0026rsquo;t Markup Language YAML 不是一种标记语言， 主要用来配置，大小写敏感，不支持tab 可以用工具互相转换，参考网站： https://www.json2yaml.com/ http://www.bejson.com/json/json2yaml/\nPlaybook 核心组件 官方文档\nhttps://docs.ansible.com/ansible/latest/reference_appendices/playbooks_keywords.html#playbook-keywords 一个playbook 中由多个组件组成,其中所用到的常见组件类型如下:\nHosts 执行的远程主机列表 Tasks 任务集,由多个task的元素组成的列表实现,每个task是一个字典,一个完整的代码块功能需少元素需包括 name 和 task,一个name只能包括一个task Variables 内置变量或自定义变量在playbook中调用 Templates 模板，可替换模板文件中的变量并实现一些简单逻辑的文件 Handlers 和 notify 结合使用，由特定条件触发的操作，满足条件方才执行，否则不执行 tags 标签 指定某条任务执行，用于选择运行playbook中的部分代码。ansible具有幂等性，因此 会自动跳过没有变化的部分，即便如此，有些代码为测试其确实没有发生变化的时间依然会非常地长。此时，如果确信其没有变化，就可以通过tags跳过此些代码片断 hosts 组件 Hosts：playbook中的每一个play的目的都是为了让特定主机以某个指定的用户身份执行任务。hosts用于指定要执行指定任务的主机，须事先定义在主机清单中\none.example.com one.example.com:two.example.com 192.168.1.50 192.168.1.* Websrvs:dbsrvs #或者，两个组的并集 Websrvs:\u0026amp;dbsrvs #与，两个组的交集 webservers:!dbsrvs #在websrvs组，但不在dbsrvs组 案例：\n- hosts: websrvs:appsrvs remote_user 组件 remote_user: 可用于Host和task中。也可以通过指定其通过sudo的方式在远程主机上执行任务，其可用于play全局或某任务；此外，甚至可以在sudo时使用sudo_user指定sudo时切换的用户\n- hosts: websrvs remote_user: root tasks: - name: test connection ping: remote_user: magedu sudo: yes #默认sudo为root sudo_user:wang #sudo为wang task列表和action组件 play的主体部分是task list，task list中有一个或多个task,各个task 按次序逐个在hosts中指定的所有主机上执行，即在所有主机上完成第一个task后，再开始第二个task task的目的是使用指定的参数执行模块，而在模块参数中可以使用变量。模块执行是幂等的，这意味着多次执行是安全的，因为其结果均一致 每个task都应该有其name，用于playbook的执行结果输出，建议其内容能清晰地描述任务执行步骤。 如果未提供name，则action的结果将用于输出 task两种格式：\naction: module arguments #示例: action: shell wall hello module: arguments #建议使用 #示例: shell: wall hello 注意：shell和command模块后面跟命令，而非key=value 范例:\n[root@ansible ansible]#cat hello.yml --- #first yaml文件 # - hosts: websrvs remote_user: root gather_facts: no tasks: - name: task1 debug: msg=\u0026#34;task1 running\u0026#34; - name: task2 debug: msg=\u0026#34;task2 running\u0026#34; - hosts: appsrvs remote_user: root gather_facts: no tasks: - name: task3 debug: msg=\u0026#34;task3 running\u0026#34; - name: task4 debug: msg=\u0026#34;task4 running\u0026#34; 其它组件说明 某任务的状态在运行后为changed时，可通过\u0026quot;notify\u0026quot;通知给相应的handlers任务 还可以通过\u0026quot;tags\u0026quot;给task 打标签，可在ansible-playbook命令上使用-t指定进行调用\nShellScripts VS Playbook 案例 #SHELL脚本实现 #!/bin/bash # 安装Apache yum install --quiet -y httpd # 复制配置文件 cp /tmp/httpd.conf /etc/httpd/conf/httpd.conf cp/tmp/vhosts.conf /etc/httpd/conf.d/ # 启动Apache，并设置开机启动 systemctl enable --now httpd #Playbook实现 --- - hosts: websrvs remote_user: root gather_facts: no tasks: - name: \u0026#34;安装Apache\u0026#34; yum: name=httpd - name: \u0026#34;复制配置文件\u0026#34; copy: src=/tmp/httpd.conf dest=/etc/httpd/conf/ - name: \u0026#34;复制配置文件\u0026#34; copy: src=/tmp/vhosts.conf dest=/etc/httpd/conf.d/ - name: \u0026#34;启动Apache，并设置开机启动\u0026#34; service: name=httpd state=started enabled=yes playbook 命令 格式\nansible-playbook \u0026lt;filename.yml\u0026gt; ... [options] 选项\n--syntax,--syntax-check #语法检查,功能相当于bash -n -C --check #模拟执行dry run ,只检测可能会发生的改变，但不真正执行操作 --list-hosts #列出运行任务的主机 --list-tags #列出tag --list-tasks #列出task --limit 主机列表 #只针对主机列表中的特定主机执行 -i INVENTORY, --inventory INVENTORY #指定主机清单文件,通常一个项对应一个主机清单文件 --start-at-task START_AT_TASK #从指定task开始执行,而非从头开始,START_AT_TASK为任务的name -v -vv -vvv #显示过程 范例: 一个简单的 playbook\n[root@ansible ansible]#cat hello.yml --- - hosts: websrvs tasks: - name: hello command: echo \u0026#34;hello ansible\u0026#34; [root@ansible ansible]#ansible-playbook hello.yml [root@ansible ansible]#ansible-playbook -v hello.yml 范例: 检查和限制主机\nansible-playbook file.yml --check #只检测 ansible-playbook file.yml ansible-playbook file.yml --limit websrvs 范例: 一个playbook 多个play\ncat test_plays.yaml --- - hosts: localhost remote_user: root gather_facts: no tasks: - name: play1 command: echo \u0026#34;play1\u0026#34; - hosts: centos7 remote_user: root gather_facts: no tasks: - name: play2 command: echo \u0026#34;play2\u0026#34; 忽略错误 ignore_errors 如果一个task出错,默认将不会继续执行后续的其它task 利用 ignore_errors: yes 可以忽略此task的错误,继续向下执行playbook其它task\n[root@ansible ansible]#cat test_ignore.yml --- - hosts: centos7 tasks: - name: error command: /bin/false ignore_errors: yes - name: continue command: wall continue ansible-playbook案例 安装nginx --- - hosts: centos7 # yum install nginx remote_user: root gather_facts: no tasks: - name: install nginx yum: name=nginx state=present - name: service: name=nginx state=started enabled=yes 卸载httpd #remove_httpd.yml --- - hosts: webservers remote_user: root gather_facts: no tasks: - name: remove httpd package yum: name=httpd state=absent - name: remove apache user user: name=apache state=absent - name: remove config file file: name=/etc/httpd state=absent - name: remove web html file: name=/data/html/ state=absent Playbook中使用handlers和notify handlers和notify Handlers本质是task list ，类似于MySQL中的触发器触发的行为，其中的task与前述的task并没有本质上的不同，只有在关注的资源发生变化时，才会采取一定的操作。 Notify对应的action 在所有task都执行完才会最后被触发，这样可避免多个task多次改变发生时每次都触发执行指定的操作，Handlers仅在所有的变化发生完成后一次性地执行指定操作。 在notify中列出的操作称为handler，也即notify中调用handler中定义的操作 注意:\n如果多个task通知了相同的handlers， 此handlers仅会在所有task结束后运行一 次。 只有notify对应的task发生改变了才会通知handlers， 没有改变则不会触发handlers handlers 是在所有前面的tasks都成功执行才会执行,如果前面任何一个task失败,会导致handle跳过执行 案例:\n案例：\n案例：\n范例: 部署haproxy\nforce_handlers 如果不论前面的task成功与否,都希望handlers能执行, 可以使用force_handlers: yes 强制执行handler 范例: 强制调用handlers\nPlaybook中使用tags组件 官方文档:\nhttps://docs.ansible.com/ansible/latest/user_guide/playbooks_tags.html 默认情况下， Ansible 在执行一个 playbook 时，会执行 playbook 中所有的任务，在playbook文件中，可以利用tags组件，为特定 task 指定标签，当在执行playbook时，可以只执行特定tags的task,而非整个playbook文件 可以一个task对应多个tag,也可以多个task对应同一个tag 还有另外3个特殊关键字用于标签, tagged, untagged 和 all,它们分别是仅运行已标记，只有未标记和所有任务。 tags 主要用于调试环境 范例： tag 标签\nPlaybook中使用变量 Playbook中同样也支持变量 变量名：仅能由字母、数字和下划线组成，且只能以字母开头 变量定义：\nvariable=value variable: value 范例：\nhttp_port=80 http_port: 80 通过 {{ variable_name }} 调用变量，且变量名前后建议加空格，有时用\u0026quot;{{ variable_name }}\u0026ldquo;才生效 变量来源：\nansible 的 setup facts 远程主机的所有变量都可直接调用 通过命令行指定变量，优先级最高 ansible-playbook -e varname=value test.yml 3.在playbook文件中定义\nvars: var1: value1 var2: value2 4.在独立的变量YAML文件中定义\n- hosts: all vars_files: - vars.yml 在主机清单文件中定义 主机（普通）变量：主机组中主机单独定义，优先级高于公共变量 组（公共）变量：针对主机组中所有主机定义统一变量 在项目中针对主机和主机组定义 在项目目录中创建 host_vars和group_vars目录 在role中定义 变量的优先级从高到低如下\n-e 选项定义变量 --\u0026gt;playbook中vars_files --\u0026gt; playbook中vars变量定义 --\u0026gt;host_vars/主机名文件 --\u0026gt;主机清单中主机变量--\u0026gt; group_vars/主机组名文件--\u0026gt;group_vars/all文件--\u0026gt; 主机清单组变量 使用 setup 模块中变量 使用 facts 变量 本模块自动在playbook调用，生成的系统状态信息, 并将之存放在facts变量中 facts 包括的信息很多,如: 主机名,IP,CPU,内存,网卡等 facts 变量的实际使用场景案例\n通过facts变量获取被控端CPU的个数信息,从而生成不同的Nginx配置文件 通过facts变量获取被控端内存大小信息,从而生成不同的memcached的配置文件 通过facts变量获取被控端主机名称信息,从而生成不同的Zabbix配置文件 通过facts变量获取被控端网卡信息,从而生成不同的主机名 案例：使用setup变量\n[root@ansible ~]# ansible localhost -m setup -a \u0026#39;filter=\u0026#34;ansible_default_ipv4\u0026#34;\u0026#39; localhost | SUCCESS =\u0026gt; { \u0026#34;ansible_facts\u0026#34;: { \u0026#34;ansible_default_ipv4\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;192.168.32.133\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;ens160\u0026#34;, \u0026#34;broadcast\u0026#34;: \u0026#34;192.168.32.255\u0026#34;, \u0026#34;gateway\u0026#34;: \u0026#34;192.168.32.2\u0026#34;, \u0026#34;interface\u0026#34;: \u0026#34;ens160\u0026#34;, \u0026#34;macaddress\u0026#34;: \u0026#34;00:0c:29:7c:80:cd\u0026#34;, \u0026#34;mtu\u0026#34;: 1500, \u0026#34;netmask\u0026#34;: \u0026#34;255.255.255.0\u0026#34;, \u0026#34;network\u0026#34;: \u0026#34;192.168.32.0\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;24\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;ether\u0026#34; } }, \u0026#34;changed\u0026#34;: false } [root@ansible ~]# 范例：显示ens33的网卡的IP地址\n--- - hosts: centos7 tasks: - name: show ens33 ip debug: msg: IP address {{ ansible_ens33.ipv4.address }} #msg: IP address {{ ansible_facts[\u0026#34;ens33\u0026#34;][\u0026#34;ipv4\u0026#34;][\u0026#34;address\u0026#34;] }} #msg: IP address {{ ansible_facts.ens33.ipv4.address }} #msg: IP address {{ ansible_default_ipv4.address }} #msg: IP address {{ ansible_ens33.ipv4.address }} #msg: IP address {{ ansible_ens33.ipv4.address.split(\u0026#39;.\u0026#39;)[-1] }} #取IP中的最后一个数字 [root@ansible ansible]# ansible-playbook -v show_ip.yml Using /etc/ansible/ansible.cfg as config file PLAY [centos7] ************************************************************************************************************************* TASK [Gathering Facts] ***************************************************************************************************************** ok: [192.168.32.179] ok: [192.168.32.178] TASK [show ens33 ip] ******************************************************************************************************************* ok: [192.168.32.178] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;IP address 192.168.32.178\u0026#34; } ok: [192.168.32.179] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;IP address 192.168.32.179\u0026#34; } PLAY RECAP ***************************************************************************************************************************** 192.168.32.178 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 192.168.32.179 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 [root@ansible ansible]# 范例：修改主机名称为web-IP\n- hosts: centos7 tasks: - name: 打印facts变量 debug: msg={{ ansible_ens33.ipv4.address }} - name: 修改主机名 hostname: name=web-{{ ansible_ens33.ipv4.address }} #- name: 获取facts变量提取IP地址，以.结尾的最后一列,修改主机名为web-hostid #hostname: name=web-{{ ansible_ens33.ipv4.address.split(\u0026#39;.\u0026#39;)[-1] }} [root@ansible ansible]# ansible-playbook change_hostname.yml PLAY [centos7] ************************************************************************************************************************* TASK [Gathering Facts] ***************************************************************************************************************** ok: [192.168.32.178] ok: [192.168.32.179] TASK [打印facts变量] ******************************************************************************************************************* ok: [192.168.32.178] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;192.168.32.178\u0026#34; } ok: [192.168.32.179] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;192.168.32.179\u0026#34; } TASK [修改主机名] ********************************************************************************************************************** changed: [192.168.32.179] changed: [192.168.32.178] PLAY RECAP ***************************************************************************************************************************** 192.168.32.178 : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 192.168.32.179 : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 [root@ansible ansible]# ####　性能优化\n每次执行playbook,默认会收集每个主机的所有facts变量,将会导致速度很慢,可以采用下面方法加速 方法1 关闭facts采集加速执行,此方法将导致无法使用facts变量\n- hosts: all gather_facts: no 方法2 当使用 gather_facts: no 关闭 facts，确实能加速 Ansible 执行，但是有时候又需要使用 facts 中的内容，还希望执行的速度快，这时候可以设置facts 的缓存,将facts变量信息存在redis服务器中\n[root@ansible ~]# cat /etc/ansible/ansible.cfg [defaults] # smart 表示默认收集 facts，但 facts 已有的情况下不会收集，即使用缓存 facts # implicit 表示默认收集 facts，要禁止收集，必须使用 gather_facts: False # explicit 则表示默认不收集，要显式收集，必须使用gather_facts: True gathering = smart #在使用 facts 缓存时设置为smart fact_caching_timeout = 86400 #缓存时长 fact_caching = redis #缓存存在redis中 fact_caching_connection = 10.0.0.100:6379:0 #0表示redis的0号数据库 #若redis设置了密码 fact_caching_connection = 10.0.0.100:6379:0:password register 注册变量 在playbook中可以使用register将捕获命令的输出保存在临时变量中，方便后续调用此变量,比如可以使用debug模块进行显示输出 范例: 利用debug 模块输出变量\n--- - hosts: centos7 tasks: - name: get variable shell: hostname register: name - name: print variable debug: msg: \u0026#34;{{ name }}\u0026#34; #输出register注册的name变量的全部信息,注意变量要加\u0026#34; \u0026#34;引起来 #msg: \u0026#34;{{ name.cmd }}\u0026#34; #显示命令 #msg: \u0026#34;{{ name.rc }}\u0026#34; #显示命令成功与否 #msg: \u0026#34;{{ name.stdout }}\u0026#34; #显示命令的输出结果为字符串形式,所有结果都放在一行里显示,适合于结果是单行输出 #msg: \u0026#34;{{ name.stdout_lines }}\u0026#34; #显示命令的输出结果为列表形式,逐行标准输出,适用于多行显示 #msg: \u0026#34;{{ name[\u0026#39;stdout_lines\u0026#39;] }}\u0026#34; #显示命令的执行结果为列表形式,和效果上面相同 #msg: \u0026#34;{{ name.stdout_lines[0] }}\u0026#34; #显示命令的输出结果的列表中的第一个元素 #说明 第一个 task 中，使用了 register 注册变量名为 name ；当 shell 模块执行完毕后，会将数据放到该变量中。第二给 task 中，使用了 debug 模块，并从变量name中获取数据。 [root@ansible ansible]# ansible-playbook -C register.yml PLAY [centos7] ************************************************************************************************************************* TASK [Gathering Facts] ***************************************************************************************************************** ok: [192.168.32.179] ok: [192.168.32.178] TASK [get variable] ******************************************************************************************************************** skipping: [192.168.32.179] skipping: [192.168.32.178] TASK [print variable] ****************************************************************************************************************** ok: [192.168.32.178] =\u0026gt; { \u0026#34;msg\u0026#34;: { \u0026#34;changed\u0026#34;: false, \u0026#34;cmd\u0026#34;: \u0026#34;hostname\u0026#34;, \u0026#34;delta\u0026#34;: null, \u0026#34;end\u0026#34;: null, \u0026#34;failed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;Command would have run if not in check mode\u0026#34;, \u0026#34;rc\u0026#34;: 0, \u0026#34;skipped\u0026#34;: true, \u0026#34;start\u0026#34;: null, \u0026#34;stderr\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;stderr_lines\u0026#34;: [], \u0026#34;stdout\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;stdout_lines\u0026#34;: [] } } ok: [192.168.32.179] =\u0026gt; { \u0026#34;msg\u0026#34;: { \u0026#34;changed\u0026#34;: false, \u0026#34;cmd\u0026#34;: \u0026#34;hostname\u0026#34;, \u0026#34;delta\u0026#34;: null, \u0026#34;end\u0026#34;: null, \u0026#34;failed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;Command would have run if not in check mode\u0026#34;, \u0026#34;rc\u0026#34;: 0, \u0026#34;skipped\u0026#34;: true, \u0026#34;start\u0026#34;: null, \u0026#34;stderr\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;stderr_lines\u0026#34;: [], \u0026#34;stdout\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;stdout_lines\u0026#34;: [] } } PLAY RECAP ***************************************************************************************************************************** 192.168.32.178 : ok=2 changed=0 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 192.168.32.179 : ok=2 changed=0 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 [root@ansible ansible]# 范例: 安装启动服务并检查\n--- - hosts: centos7 vars: package_name: nginx service_name: nginx tasks: - name: install {{ package_name }} yum: name={{ package_name }} - name: start {{ service_name }} service: name={{ service_name }} state=started enabled=yes - name: check shell: ps axu|grep {{ service_name }} register: check_service - name: debug debug: msg: \u0026#34;{{ check_service.stdout_lines }}\u0026#34; 范例: 修改主机名形式为 web_\u0026lt;随机字符\u0026gt;\n- hosts: centos7 tasks: - name: genarate random shell: cmd: openssl rand -base64 12 |tr -dc \u0026#39;[:alnum:]\u0026#39; register: num - name: show random debug: msg: \u0026#34;{{ num }}\u0026#34; - name: change hostname hostname: name: web-{{ num.stdout }} 范例: 修改主机名形式为 web_随机数\n- hosts: centos7 tasks: - name: 定义一个随机数，设定为变量，然后后续调用 shell: echo $((RANDOM%255)) register: web_number - name: 使用debug输出变量结果 debug: msg={{ web_number }} - name: 使用hostname模块将主机名修改为web_随机数 hostname: name=web_{{ web_number.stdout }} 范例: 批量修改主机名为随机字符\n- hosts: centos7 vars: host: web domain: wang.org tasks: - name: get variable shell: echo $RANDOM | md5sum | cut -c 1-8 register: get_random - name: print variable debug: msg: \u0026#34;{{ get_random.stdout }}\u0026#34; - name: set hostname hostname: name={{ host }}-{{ get_random.stdout }}.{{ domain }} 范例: 批量修改主机名为IP最后1位数字\n- hosts: centos7 vars: host: web domain: wang.org tasks: - name: get variable shell: hostname -I | awk \u0026#39;{print $1}\u0026#39; register: get_ip - name: print variable debug: msg: \u0026#34;{{ get_ip.stdout.split(\u0026#39;.\u0026#39;)[3] }}\u0026#34; - name: set hostname hostname: name={{ host }}-{{ get_ip.stdout.split(\u0026#39;.\u0026#39;)[3] }}.{{ domain }} 在 Playbook 命令行中定义变量 范例：\n--- - hosts: centos7 remote_user: root tasks: - name: install nginx yum: name={{ pkname }} state=present [root@ansible ~]#ansible-playbook -e pkname=nginx var2.yml 范例：\n#也可以将多个变量放在一个文件中 [root@ansible ~]#cat vars pkname1: memcached pkname2: vsftpd [root@ansible ~]#vim var2.yml --- - hosts: centos7 remote_user: root tasks: - name: install package {{ pkname1 } yum: name={{ pkname1 }} state=present - name: install package {{ pkname2 } yum: name={{ pkname2 }} state=present [root@ansible ~]#ansible-playbook -e pkname1=memcached -e pkname2=httpd var2.yml [root@ansible ~]#ansible-playbook -e \u0026#39;@vars\u0026#39; var2.yml 在playbook文件中定义变量 此方式定义的是私有变量,即只能在当前playbook中使用,不能被其它Playbook共用 范例：\n- hosts: webservers remote_user: root vars: username: user1 groupname: group1 tasks: - name: create group {{ groupname }} group: name={{ groupname }} state=present - name: create user {{ username }} user: name={{ username }} group={{ groupname }} state=present [root@ansible ~]#ansible-playbook -e \u0026#34;username=user2 groupname=group2\u0026#34; var3.yml 范例：变量的相互调用\n--- - hosts: centos7 remote_user: root vars: collect_info: \u0026#34;/data/test/{{ansible_default_ipv4[\u0026#39;address\u0026#39;]}}/\u0026#34; tasks: - name: create IP directory file: name=\u0026#34;{{collect_info}}\u0026#34; state=directory 使用专用的公共的变量文件 可以在一个独立的playbook文件中定义公共变量，在其它的playbook文件中可以引用变量文件中的变量 此方式比playbook中定义的变量优化级高\nvim vars.yml --- # variables file package_name: mariadb-server service_name: mariadb vim var5.yml --- #install package and start service - hosts: dbsrvs remote_user: root vars_files: # 指定变量文件名 - vars.yml tasks: - name: install package yum: name={{ package_name }} tags: install - name: start service service: name={{ service_name }} state=started enabled=yes 在主机清单中定义主机和主机组的变量 所有项目的主机变量 在inventory 主机清单文件中为指定的主机定义变量以便于在playbook中使用 范例：\n[webservers] www1.wang.org http_port=80 maxRequestsPerChild=808 www2.wang.org http_port=8080 maxRequestsPerChild=909 所有项目的组（公共）变量 在inventory 主机清单文件中赋予给指定组内所有主机上的在playbook中可用的变量，如果和主机变是同名，优先级低于主机变量\n案例：\n[webservers:vars] http_port=80 ntp_server=ntp.wang.org nfs_server=nfs.wang.org [all:vars] # --------- Main Variables --------------- # Cluster container-runtime supported: docker, containerd CONTAINER_RUNTIME=\u0026#34;docker\u0026#34; # Network plugins supported: calico, flannel, kube-router, cilium, kube-ovn CLUSTER_NETWORK=\u0026#34;calico\u0026#34; # Service proxy mode of kube-proxy: \u0026#39;iptables\u0026#39; or \u0026#39;ipvs\u0026#39; PROXY_MODE=\u0026#34;ipvs\u0026#34; # K8S Service CIDR, not overlap with node(host) networking SERVICE_CIDR=\u0026#34;192.168.0.0/16\u0026#34; # Cluster CIDR (Pod CIDR), not overlap with node(host) networking CLUSTER_CIDR=\u0026#34;172.16.0.0/16\u0026#34; # NodePort Range NODE_PORT_RANGE=\u0026#34;20000-60000\u0026#34; # Cluster DNS Domain CLUSTER_DNS_DOMAIN=\u0026#34;magedu.local.\u0026#34; 范例：\n[root@ansible ~]#vim /etc/ansible/hosts [webservers] 10.0.0.8 hname=www1 domain=magedu.io 10.0.0.7 hname=www2 [webservers:vars] mark=\u0026#34;-\u0026#34; [all:vars] domain=wang.org [root@ansible ~]#ansible webservers -m hostname -a \u0026#39;name={{ hname }}{{ mark }} {{ domain }}\u0026#39; #命令行指定变量： [root@ansible ~]#ansible webservers -e domain=magedu.cn -m hostname -a \u0026#39;name= {{ hname }}{{ mark }}{{ domain }}\u0026#39; 针对当前项目的主机和主机组的变量 上面的方式是针对所有项目都有效,而官方更建议的方式是使用ansible特定项目的主机变量和组变量.生产建议在每个项目对应的目录中创建额外的两个变量目录,分别是host_vars和group_vars\nhost_vars下面的文件名和主机清单主机名一致,针对单个主机进行变量定义格式:host_vars/hostname group_vars下面的文件名和主机清单中组名一致, 针对单个组进行变量定义格式: group_vars/groupname group_vars/all文件内定义的变量对所有组都有效 范例: 特定项目的主机和组变量\n[root@ansible ansible]#pwd /data/ansible [root@ansible ansible]#mkdir host_vars [root@ansible ansible]#mkdir group_vars [root@ansible ansible]#cat host_vars/10.0.0.8 id: 2 [root@ansible ansible]#cat host_vars/10.0.0.7 id: 1 [root@ansible ansible]#cat group_vars/webservers name: web [root@ansible ansible]#cat group_vars/all domain: wang.org [root@ansible ansible]#tree host_vars/ group_vars/ host_vars/ ├── 10.0.0.7 └── 10.0.0.8 group_vars/ ├── all └── webservers 0 directories, 4 files [root@ansible ansible]#cat test.yml - hosts: webservers tasks: - name: get variable command: echo \u0026#34;{{name}}{{id}}.{{domain}}\u0026#34; register: result - name: print variable debug: msg: \u0026#34;{{result.stdout}}\u0026#34; [root@ansible ansible]#ansible-playbook test.yml PLAY [webservers] ******************************************************************************** *************************************** TASK [Gathering Facts] ******************************************************************************** ******************************* ok: [10.0.0.7] ok: [10.0.0.8] TASK [get variable] ******************************************************************************** ********************************** changed: [10.0.0.7] changed: [10.0.0.8] TASK [print variable] ******************************************************************************** ******************************** ok: [10.0.0.7] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;web1.wang.org\u0026#34; } ok: [10.0.0.8] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;web2.wang.org\u0026#34; } PLAY RECAP ******************************************************************************** ******************************************* 10.0.0.7 : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 10.0.0.8 : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Template 模板 模板是一个文本文件，可以用于根据每个主机的不同环境而为生成不同的文件 模板文件中支持嵌套jinja2语言的指令,来实现变量,条件判断,循环等功能 需要使用template模块实现文件的复制到远程主机,但和copy模块不同,复制过去的文件每个主机可以会有所不同\njinja2语言 Jinja2 是一个现代的，设计者友好的，仿照 Django 模板的 Python 模板语言。 它速度快，被广泛使用，并且提供了可选的沙箱模板执行环境保证安全: 特性:\n沙箱中执行 强大的 HTML 自动转义系统保护系统免受 XSS 模板继承 及时编译最优的 python 代码 可选提前编译模板的时间 易于调试。异常的行数直接指向模板中的对应行。 可配置的语法 官方网站：\nhttp://jinja.pocoo.org/ https://jinja.palletsprojects.com/en/2.11.x/ 官方中文文档\nhttp://docs.jinkan.org/docs/jinja2/ https://www.w3cschool.cn/yshfid/ jinja2 语言支持多种数据类型和操作: 字面量，如: 字符串：使用单引号或双引号,数字：整数，浮点数 列表：[item1, item2, \u0026hellip;] 元组：(item1, item2, \u0026hellip;) 字典：{key1:value1, key2:value2, \u0026hellip;} 布尔型：true/false 算术运算：+, -, *, /, //, %, ** 比较操作：==, !=, \u0026gt;, \u0026gt;=, \u0026lt;, \u0026lt;=\n逻辑运算：and，or，not 流表达式：For，If，When\n字面量： 表达式最简单的形式就是字面量。字面量表示诸如字符串和数值的 Python 对象。如\u0026quot;Hello World\u0026rdquo; 双引号或单引号中间的一切都是字符串。无论何时你需要在模板中使用一个字符串（比如函数调用、过滤器或只是包含或继承一个模板的参数），如42，42.23 数值可以为整数和浮点数。如果有小数点，则为浮点数，否则为整数。在 Python 里， 42 和 42.0 是不一样的\n算术运算： Jinja 允许用计算值。支持下面的运算符 +：把两个对象加到一起。通常对象是素质，但是如果两者是字符串或列表，你可以用这 种方式来衔接 它们。无论如何这不是首选的连接字符串的方式！连接字符串见 ~ 运算符。 {{ 1 + 1 }} 等于 2 -：用第一个数减去第二个数。 {{ 3 - 2 }} 等于 1 /：对两个数做除法。返回值会是一个浮点数。 {{ 1 / 2 }} 等于 0.5 //：对两个数做除法，返回整数商。 {{ 20 // 7 }} 等于 2 %：计算整数除法的余数。 {{ 11 % 7 }} 等于 4 *：用右边的数乘左边的操作数。 {{ 2 * 2 }} 会返回 4 。也可以用于重 复一个字符串多次。 {{ \u0026lsquo;=\u0026rsquo; * 80 }} 会打印 80 个等号的横条\n：取左操作数的右操作数次幂。 {{ 23 }} 会返回 8\n比较操作符\n== 比较两个对象是否相等 != 比较两个对象是否不等\n如果左边大于右边，返回 true = 如果左边大于等于右边，返回 true \u0026lt; 如果左边小于右边，返回 true \u0026lt;= 如果左边小于等于右边，返回 true 逻辑运算符\n对于 if 语句，在 for 过滤或 if 表达式中，它可以用于联合多个表达式 and 如果左操作数和右操作数同为真，返回 true or 如果左操作数和右操作数有一个为真，返回 true not 对一个表达式取反 (expr)表达式组 true / false true 永远是 true ，而 false 始终是 false\ntemplate template功能：可以根据和参考模块文件，动态生成相类似的配置文件 template文件存建议放于templates目录下，且命名为 .j2 结尾\nyaml/yml 文件和templates目录平级，此时playbook中指定模版文件时可不用指定路径, 目录结构如下 示例：\n./ ├── temnginx.yml └── templates └── nginx.conf.j2 范例：利用template 同步nginx配置文件\n#准备templates/nginx.conf.j2文件 [root@ansible ~]#vim temnginx.yml --- - hosts: centos7 remote_user: root tasks: - name: template config to remote hosts template: src=nginx.conf.j2 dest=/etc/nginx/nginx.conf [root@ansible ~]#ansible-playbook temnginx.yml template变更替换 范例：\n#修改文件nginx.conf.j2 [root@ansible ~]#mkdir templates [root@ansible ~]#vim templates/nginx.conf.j2 ...... worker_processes {{ ansible_processor_vcpus }}; ...... [root@ansible ~]#vim temnginx2.yml --- - hosts: centos7 remote_user: root tasks: - name: install nginx yum: name=nginx - name: template config to remote hosts template: src=nginx.conf.j2 dest=/etc/nginx/nginx.conf - name: start service service: name=nginx state=started enabled=yes [root@ansible ~]#ansible-playbook temnginx2.yml Roles 角色 角色是ansible自1.2版本引入的新特性，用于层次性、结构化地组织playbook。roles能够根据层次型结构自动装载变量文件、tasks以及handlers等。要使用roles只需要在playbook中使用include指令即可。简单来讲，roles就是通过分别将变量、文件、任务、模板及处理器放置于单独的目录中，并可以便捷地include它们的一种机制。角色一般用于基于主机构建服务的场景中，但也可以是用于构建守护进程等场景中 运维复杂的场景：建议使用 roles，代码复用度高 roles：多个角色的集合目录， 可以将多个的role，分别放至roles目录下的独立子目录中,如下示例\nroles/ mysql/ nginx/ tomcat/ redis/ 默认roles存放路径\n/root/.ansible/roles /usr/share/ansible/roles /etc/ansible/roles 官方文档:\nhttps://docs.ansible.com/ansible/latest/user_guide/playbooks_reuse_roles.html Ansible Roles目录编排 roles目录结构如下所示\n每个角色，以特定的层级目录结构进行组织 roles目录结构：\nplaybook1.yml playbook2.yml roles/ project1/ tasks/ files/ vars/ templates/ handlers/ default/ meta/ project2/ tasks/ files/ vars/ templates/ handlers/ default/ meta/ Roles各目录作用 roles/project/ :项目名称,有以下子目录\nfiles/ ：存放由copy或script模块等调用的文件 templates/：template模块查找所需要模板文件的目录 tasks/：定义task,role的基本元素，至少应该包含一个名为main.yml的文件；其它的文件需要在此文件中通过include进行包含 handlers/：至少应该包含一个名为main.yml的文件；此目录下的其它的文件需要在此文件中通过include进行包含 vars/：定义变量，至少应该包含一个名为main.yml的文件；此目录下的其它的变量文件需要在此文件中通过include进行包含,也可以通过项目目录中的group_vars/all定义变量,从而实现角色通用代码和项目数据的分离 meta/：定义当前角色的特殊设定及其依赖关系,至少应该包含一个名为main.yml的文件，其它文件需在此文件中通过include进行包含 default/：设定默认变量时使用此目录中的main.yml文件，比vars的优先级低 创建 role 创建role的步骤\n1 创建role的目录结构.在以roles命名的目录下分别创建以各角色名称命名的目录，如mysql等,在每个角色命名的目录中分别创建相关的目录和文件,比如tasks、files、handlers、templates和vars等目录；用不到的目录可以创建为空目录，也可以不创建 2 编写和准备指定role的功能文件,包括: tasks,templates,vars等相关文件 3 编写playbook文件调用上面定义的role,应用到指定的主机 针对大型项目使用Roles进行编排 范例: 利用 ansible-galaxy 创建角色目录的结构\n#创建初始化目录结构 [root@ansible roles]#ansible-galaxy role init test_role - Role test_role was created successfully [root@ansible roles]#tree test_role/ test_role/ ├── defaults │ └── main.yml ├── files ├── handlers │ └── main.yml ├── meta │ └── main.yml ├── README.md ├── tasks │ └── main.yml ├── templates ├── tests │ ├── inventory │ └── test.yml └── vars └── main.yml 8 directories, 8 files 范例：roles的目录结构\nnginx-role.yml roles/ └── nginx ├── files │ └── nginx.conf ├── tasks │ ├── groupadd.yml │ ├── install.yml │ ├── main.yml │ ├── restart.yml │ └── useradd.yml └── vars └── main.yml Playbook 调用角色 调用角色方法1：\n--- - hosts: webservers remote_user: root roles: - mysql - memcached - nginx 调用角色方法2： 键role用于指定角色名称，后续的k/v用于传递变量给角色\n--- - hosts: all remote_user: root roles: - role: mysql username: mysql - { role: nginx, username: nginx } 调用角色方法3： 还可基于条件测试实现角色调用\n--- - hosts: all remote_user: root roles: - { role: nginx, username: nginx, when: ansible_distribution_major_version == \u0026#39;7\u0026#39; } Roles 中 Tags 使用 [root@ansible ~]#vi app-role.yml --- #可以有多个play - hosts: lbserver roles: - role: haproxy - role: keepalived - hosts: appsrvs remote_user: root roles: - { role: nginx ,tags: [ \u0026#39;nginx\u0026#39;, \u0026#39;web\u0026#39; ] ,when: ansible_distribution_major_version == \u0026#34;6\u0026#34; } - { role: httpd ,tags: [ \u0026#39;httpd\u0026#39;, \u0026#39;web\u0026#39; ] } - { role: mysql ,tags: [ \u0026#39;mysql\u0026#39;, \u0026#39;db\u0026#39; ] } - role: mariadb tags: - mariadb - db tags: app #play的tag [root@ansible ~]#ansible-playbook --tags=\u0026#34;nginx,mysql\u0026#34; app-role.yml 实战案例 实现httpd角色 # 创建role目录 [root@ansible data]# ansible-galaxy role init httpd - Role htppd was created successfully [root@ansible data]# tree httpd/ httpd/ ├── defaults │ └── main.yml ├── files ├── handlers │ └── main.yml ├── meta │ └── main.yml ├── README.md ├── tasks │ └── main.yml ├── templates ├── tests │ ├── inventory │ └── test.yml └── vars └── main.yml 8 directories, 8 files [root@ansible data]# #main.yml 是task的入口文件 [root@ansible tasks]# cat main.yml --- # tasks file for httpd - include: group.yml - include: user.yml - include: install_httpd.yml - include: config.yml - inclusde: index.yml - include: service.yml [root@ansible tasks]# # 创建用户组 [root@ansible httpd]# cat tasks/group.yml - name: add group group: name={{ httpd_group}} system=yes gid={{ httpd_gid }} [root@ansible htppd]# # 创建用户 [root@ansible httpd]# cat tasks/user.yml - name: add httpd user user: name={{ httpd_user }} system=yes shel=/sbin/nologin home=/var/www uid={{ httpd_uid }} group={{ httpd_group }} [root@ansible htppd]# # yum install httpd [root@ansible httpd]# cat tasks/install_httpd.yml - name: install httpd yum: name=httpd [root@ansible httpd]# # 拷贝配置文件 #注意: 文件是放在files目录下,但src的路径无需写files目录名 [root@ansible htppd]# cat tasks/config.yml - name: httpd config copy: src=httpd.conf dest=/etc/httpd/conf backup=yes notify: restart httpd # 准备测试文件 [root@ansible htppd]# cat tasks/index.yml - name: copy index.html copy: src=index.html dest=/var/www/html [root@ansible htppd]# # start httpd [root@ansible htppd]# cat tasks/service.yml - name: start httpd service: name=httpd state=started enabled=yes [root@ansible htppd]# # 配置文件修改则重启httpd [root@ansible htppd]# cat handlers/main.yml --- # handlers file for httpd - name: restart httpd service: name=httpd state=restarted [root@ansible htppd]# #在files目录下准备两个文件 [root@ansible data]# ll httpd/files total 16 -rw-r--r-- 1 root root 11753 Mar 1 18:36 httpd.conf -rw-r--r-- 1 root root 23 Mar 1 21:10 index.html # 准备变量文件 [root@ansible data]# cat httpd/vars/main.yml --- # vars file for httpd httpd_group: apache httpd_gid: 88 httpd_user: apache httpd_uid: 88 [root@ansible data]# #在playbook中调用角色 [root@ansible data]# cat web_roles.yml --- - hosts: centos7 remote_user: root roles: - httpd #运行playbook [root@ansible data]# ansible-playbook /data/web_roles.yml 实现Nginx角色 # 创建roles目录 [root@ansible data]# ansible-galaxy init nginx - Role nginx was created successfully [root@ansible data]# ll total 12 -rw-r--r-- 1 root root 614 Mar 1 21:07 ansible.cfg -rw-r--r-- 1 root root 1382 Mar 1 21:07 hosts drwxr-xr-x 10 root root 154 Mar 1 18:07 httpd drwxr-xr-x 10 root root 154 Mar 1 21:52 nginx -rw-r--r-- 1 root root 63 Mar 1 21:14 web_roles.yml [root@ansible data]# # 创建tasks文件 [root@ansible data]# cat nginx/tasks/main.yml --- # tasks file for nginx - include: install_nginx.yml - import_playbook: config.yml - include: index.yml - import_playbook: service.yml [root@ansible data]# # 安装nginx [root@ansible data]# cat nginx/tasks/install_nginx.yml --- - name: install nginx yum: name: nginx state: present [root@ansible data]# # 配置文件 [root@ansible data]# cat nginx/tasks/config.yml --- - name: copy config template: src=nginx.conf.j2 dest=/etc/nginx/nginx.conf notify: restart nginx # 创建测试文件 [root@ansible data]# cat nginx/tasks/index.yml --- - name: copt index.html copy: src=index.html dest=/usr/share/nginx/html/ # 启动nginx [root@ansible data]# cat nginx/tasks/service.yml --- - name: start nginx service: name=nginx state=started enabled=yes #创建handler文件 [root@ansible data]# cat nginx/handlers/main.yml --- # handlers file for nginx - name: restart nginx service: naem=nginx state=restarted [root@ansible data]# ll #创建template文件 [root@ansible data]# ll nginx/templates/ total 4 -rw-r--r-- 1 root root 2336 Mar 1 22:12 nginx.conf.j2 [root@ansible data]# # 创建测试文件 [root@ansible data]# ll nginx/files/ total 4 -rw-r--r-- 1 root root 23 Mar 1 22:14 index.html [root@ansible data]# #在playbook中调用角色 [root@ansible data]# cat web_roles.yml --- - hosts: centos7 remote_user: root roles: # - httpd - nginx [root@ansible data]# #运行playbook [root@ansible data]# ansible-playbook web_roles.yml 实现MySql8角色 创建角色目录 [root@ansible data]# ansible-galaxy init mysql8 [root@ansible data]# ll total 12 -rw-r--r-- 1 root root 614 Mar 1 21:07 ansible.cfg -rw-r--r-- 1 root root 1382 Mar 1 21:07 hosts drwxr-xr-x 10 root root 154 Mar 1 18:07 httpd drwxr-xr-x 10 root root 154 Mar 1 22:55 mysql8 drwxr-xr-x 8 root root 125 Mar 1 22:44 nginx -rw-r--r-- 1 root root 75 Mar 1 22:38 web_roles.yml [root@ansible data]# 创建tasks yml文件 # 安装包 [root@ansible data]# cat mysql8/tasks/install_package.yml --- - name: install package yum: name={{ item }} state=latest loop: - libaio - numactl-libs # add group [root@ansible data]# cat mysql8/tasks/group.yml --- - name: add group group: name={{ group }} gid={{ group_gid }} [root@ansible data]# # add user [root@ansible data]# cat mysql8/tasks/user.yml --- - name: add user user: name={{ user }} uid={{ user_uid }} shell=/sbin/nologin group={{ group }} create_home=no system=yes home=/data/mysql [root@ansible data]# # 准备my.cnf文件 [root@ansible data]# cat mysql8/files/my.cnf [mysqld] server-id=1 log-bin datadir=/data/mysql socket=/data/mysql/mysql.sock log-error=/data/mysql/mysql.log pid-file=/data/mysql/mysql.pid [client] socket=/data/mysql/mysql.sock # 准备mysql二进制包 [root@ansible data]# ll mysql8/files/ total 1176056 -rw-r--r-- 1 root root 181 Mar 1 23:10 my.cnf -rw-r--r-- 1 root root 1204277208 Dec 18 2021 mysql-8.0.28-linux-glibc2.12-x86_64.tar.xz [root@ansible data]# # 将mysql二进制包解压到远程主机 [root@ansible data]# cat mysql8/tasks/unarchive.yml --- - name: copy mysql tar host # mysql_tar 为mysql二进制的压缩包名称 unarchive: src={{ mysql_tar }} dest=/usr/local/ owner=root group=root [root@ansible data]# # 将远程主机解压出的二进制包创建软连接 [root@ansible data]# cat mysql8/tasks/linkfile.yml --- - name: create link file: src=/usr/local/mysql-{{ mysql_version }}-linux-glibc2.12-x86_64 dest=/usr/local/mysql state=link [root@ansible data]# # 初始化数据库 [root@ansible data]# cat mysql8/tasks/init_mysql_data.yml --- - name: create datadir dir file: path=/data/mysql state=directory owner={{ user }} group={{ group } - name: init mysql data shell: /usr/local/mysql/bin/mysqld --initialize-insecure --user=mysql --datadir=/data/mysql [root@ansible data]# # copy config.con [root@ansible data]# cat mysql8/tasks/config.yml --- - name: copy my.cnf copy: src=my.cnf dest=/etc/my.cnf [root@ansible data]# [root@ansible data]# cat mysql8/tasks/script.yml --- - name: service script shell: /bin/cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysqld [root@ansible data]# [root@ansible data]# cat mysql8/tasks/path.yml --- - name: path copy: content=\u0026#39;PATH=/usr/local/mysql/bin:$PATH\u0026#39; dest=/etc/profile.d/mysql.sh [root@ansible data]# [root@ansible data]# cat mysql8/tasks/service.yml --- - name: service shell: chkconfig --add mysqld;/etc/init.d/mysqld start [root@ansible data]# [root@ansible data]# cat mysql8/tasks/main.yml --- # tasks file for mysql8 - include: install_package.yml - include: group.yml - include: user.yml - include: unarchive.yml - include: linkfile.yml - include: linkfile.yml - include: init_mysql_data.yml - include: config.yml - include: script.yml - include: path.yml - include: service.yml - include: secure.yml # 创建变量文件 [root@ansible data]# cat mysql8/vars/main.yml --- # vars file for mysql8 group: mysql group_gid: 306 user: mysql user_uid: 306 mysql_tar: mysql-8.0.28-linux-glibc2.12-x86_64.tar.xz mysql_version: 8.0.28 mysql_root_password: 123456 [root@ansible data]# # 调用角色 [root@ansible data]# cat web_roles.yml --- - hosts: centos7 remote_user: root roles: - mysql8 # 运行 [root@ansible data]# ansible-playbook web_roles.yml 实现Redis角色 # 创建角色目录 [root@ansible data]# ansible-galaxy init redis [root@ansible data]# tree redis/ redis/ ├── defaults │ └── main.yml ├── files ├── handlers │ └── main.yml ├── meta │ └── main.yml ├── README.md ├── tasks │ └── main.yml ├── templates ├── tests │ ├── inventory │ └── test.yml └── vars └── main.yml # 创建tasks文件 [root@ansible data]# cat redis/tasks/main.yml --- # tasks file for redis - name: Installed Redis Server yum: name: redis state: present - name: Configure Redis Server template: src: redis.conf.j2 dest: /etc/redis.conf owner: redis group: root mode: \u0026#39;0640\u0026#39; notify: Restart Redis Server - name: Start Redis Server systemd: name: redis state: started enabled: yes [root@ansible data]# # 创建handlers文件 [root@ansible data]# cat redis/handlers/main.yml --- # handlers file for redis - name: Restart Redis Server systemd: name: redis state: restarted [root@ansible data]# # 在/data/redis/templates目录下准备如下文件 [root@ansible data]# ll redis/templates/ total 48 -rw-r----- 1 root root 46729 Mar 2 02:51 redis.conf.j2 [root@ansible data]# # 调用角色 # 调用角色 [root@ansible data]# cat web_roles.yml --- - hosts: centos7 remote_user: root roles: - redis # 运行 [root@ansible data]# ansible-playbook web_roles.yml Ansible推荐学习资料 http://galaxy.ansible.com https://galaxy.ansible.com/explore#/ http://github.com/ http://ansible.com.cn/ https://github.com/ansible/ansible https://github.com/ansible/ansible-examples ","permalink":"http://hugo.itshare.work/posts/ansible/ansible2/","summary":"Playbook playbook介绍 官方链接 https://docs.ansible.com/ansible/latest/user_guide/playbooks_intro.html Playbook 组成 一个 playbook(剧本)文件是一个YAML语言编写的文本文件 通常一个playbook只包括一个play 一个 play的主要包括两部分: 主机和tasks. 即实现在指定一组主机上执行一个tasks定义好的任务列表。 一个tasks中可以有一个或多","title":"运维自动化工具Ansible(二)"},{"content":"Ansible介绍和架构 Ansible发展史 Ansible 的名称来自科幻小说《安德的游戏》中跨越时空的即时通信工具，使用它可以在相距数光年的距离，远程实时控制前线的舰队战斗 2012-03-09，发布0.0.1版，2015-10-17，Red Hat宣布1.5亿美元收购 官网：https://www.ansible.com/ 官方文档：https://docs.ansible.com/\nAnsible 功能 批量执行远程命令,可以对远程的多台主机同时进行命令的执行 批量安装和配置软件服务，可以对远程的多台主机进行自动化的方式配置和管理各种服务 编排高级的企业级复杂的IT架构任务, Ansible的Playbook和role可以轻松实现大型的IT复杂架构 提供自动化运维工具的开发API, 有很多运维工具,如jumpserver就是基于 ansible 实现自动化管工功能 Ansible 特点 优点\n功能丰富的模块：提供了多达数千个的各种功能的模块,完成特定任务只需调用特定模块即可，还 支持自定义模块，可使用任何编程语言写模块 使用和部署简单: 无需安装专用代理软件,基于python和SSH(默认已安装)实现 安全: 基于OpenSSH实现安全通讯无需专用协议 幂等性：一个任务执行1遍和执行n遍效果一样，不因重复执行带来意外情况,此特性和模块有关 支持playbook编排任务，YAML格式，编排任务，支持丰富的数据结构 较强大的多层解决方案 Role Python语言实现, 基于Paramiko（python对ssh的实现），PyYAML，Jinja2（模板语言）三个关键模块 属于红帽(IBM)公司产品,背景强大,未来发展前景光明 缺点\n如果管理的主机较多时,执行效率不如saltstack高 当前还不支持像MySQL数据库一样的事务回滚 Ansible 架构 Ansible 组成 组合INVENTORY、API、MODULES、PLUGINS的绿框，为ansible命令工具，其为核心执行工具\nINVENTORY：Ansible管理主机的清单文件,默认为 /etc/ansible/hosts MODULES：Ansible执行命令的功能模块，多数为内置核心模块，也可自定义 PLUGINS：模块功能的补充，如连接类型插件、循环插件、变量插件、过滤插件等，该功能不常用 API：供第三方程序调用的应用程序编程接口 Ansible 命令执行来源 USER 普通用户，即SYSTEM ADMINISTRATOR PLAYBOOKS：任务剧本（任务集），编排定义Ansible任务集的配置文件，由Ansible顺序依次执行，通常是JSON格式的YML文件 CMDB（配置管理数据库） API 调用 PUBLIC/PRIVATE CLOUD API调用 USER-\u0026gt; Ansible Playbook -\u0026gt; Ansibile 注意事项 执行ansible的主机一般称为管理端, 主控端，中控，master或堡垒机 主控端Python版本需要2.6或以上 被控端Python版本小于2.4，需要安装python-simplejson 被控端如开启SELinux需要安装libselinux-python windows 不能做为主控端,只能做为被控制端 Ansible 安装和常见模块 Ansible 安装 ansible的安装方法有多种 官方文档\nhttps://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html https://docs.ansible.com/ansible/latest/installation_guide/index.html 下载\nhttps://releases.ansible.com/ansible/ pip 下载\nhttps://pypi.org/project/ansible/ 包安装方式 #CentOS 的EPEL源的rpm包安装 [root@centos ~]#yum install ansible #ubuntu 安装 [root@ubuntu ~]#apt -y install ansible pip安装 pip 是安装Python包的管理器，类似 yum 范例: 在rocky8上通过pip3安装ansible\n[root@rocky8 ~]#yum -y install python39 rust [root@rocky8 ~]#pip3 install ansible [root@rocky8 ~]#ansible --version ansible [core 2.12.6] config file = None configured module search path = [\u0026#39;/root/.ansible/plugins/modules\u0026#39;, \u0026#39;/usr/share/ansible/plugins/modules\u0026#39;] ansible python module location = /usr/lib/python3.9/site-packages/ansible ansible collection location = /root/.ansible/collections:/usr/share/ansible/collections executable location = /usr/bin/ansible python version = 3.9.6 (default, Nov 9 2021, 13:31:27) [GCC 8.5.0 20210514 (Red Hat 8.5.0-3)] jinja version = 3.1.2 libyaml = True [root@rocky8 ~]#ansible-doc -l 2\u0026gt; /dev/null|wc -l 6763 范例: 安装python3.8 支持ansible2.12以上版本\n[root@rocky8 ~]#yum -y install python38 python38-pip [root@rocky8 ~]#pip3 install --upgrade pip -i https://pypi.douban.com/simple [root@rocky8 ~]#pip3 install ansible -i https://pypi.douban.com/simple/ [root@rocky8 ~]#ansible --version ansible [core 2.12.6] config file = None configured module search path = [\u0026#39;/root/.ansible/plugins/modules\u0026#39;, \u0026#39;/usr/share/ansible/plugins/modules\u0026#39;] ansible python module location = /usr/local/lib/python3.8/site- packages/ansible ansible collection location = /root/.ansible/collections:/usr/share/ansible/collections executable location = /usr/local/bin/ansible python version = 3.8.8 (default, Nov 9 2021, 13:31:34) [GCC 8.5.0 20210514 (Red Hat 8.5.0-3)] jinja version = 3.1.2 libyaml = True 范例: 安装默认的python3.6版本会有警报提示\n[root@rocky8 ~]#yum -y install python3 [root@rocky8 ~]#pip3 install --upgrade pip -i https://pypi.douban.com/simple [root@rocky8 ~]#pip3 install ansible -i https://pypi.douban.com/simple/ [root@rocky8 ~]#ansible --version [DEPRECATION WARNING]: Ansible will require Python 3.8 or newer on the controller starting with Ansible 2.12. Current version: 3.6.8 (default, Nov 9 2021, 14:44:26) [GCC 8.5.0 20210514 (Red Hat 8.5.0-3)]. This feature will be removed from ansible-core in version 2.12. Deprecation warnings can be disabled by setting deprecation_warnings=False in ansible.cfg. /usr/local/lib/python3.6/site-packages/ansible/parsing/vault/__init__.py:44: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release. from cryptography.exceptions import InvalidSignature ansible [core 2.11.12] config file = None configured module search path = [\u0026#39;/root/.ansible/plugins/modules\u0026#39;, \u0026#39;/usr/share/ansible/plugins/modules\u0026#39;] ansible python module location = /usr/local/lib/python3.6/site- packages/ansible ansible collection location = /root/.ansible/collections:/usr/share/ansible/collections executable location = /usr/local/bin/ansible python version = 3.6.8 (default, Nov 9 2021, 14:44:26) [GCC 8.5.0 20210514 (Red Hat 8.5.0-3)] jinja version = 3.0.3 libyaml = True [root@rocky8 ~]#ansible-doc -l 2\u0026gt; /dev/null|wc -l 6141 范例\n[root@centos7 ~]#yum -y install python-pip [root@centos7 ~]#pip install --upgrade pip [root@centos7 ~]#pip install ansible --upgrade [root@centos7 ~]#ansible --version /usr/lib64/python2.7/site-packages/cryptography/__init__.py:39: CryptographyDeprecationWarning: Python 2 is no longer supported by the Python core team. Support for it is now deprecated in cryptography, and will be removed in a future release. CryptographyDeprecationWarning, ansible 2.9.12 config file = None configured module search path = [u\u0026#39;/root/.ansible/plugins/modules\u0026#39;, u\u0026#39;/usr/share/ansible/plugins/modules\u0026#39;] ansible python module location = /usr/lib/python2.7/site-packages/ansible executable location = /usr/bin/ansible python version = 2.7.5 (default, Apr 2 2020, 13:16:51) [GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] [root@centos7 ~]#ll /opt/etc/ansible/ansible.cfg -rw-r--r-- 1 wang bin 19980 Aug 11 21:34 /opt/etc/ansible/ansible.cfg 确认安装 [root@ansible ~]#ansible --version ansible 2.9.5 config file = /etc/ansible/ansible.cfg configured module search path = [\u0026#39;/root/.ansible/plugins/modules\u0026#39;, \u0026#39;/usr/share/ansible/plugins/modules\u0026#39;] ansible python module location = /usr/lib/python3.6/site-packages/ansible executable location = /usr/bin/ansible python version = 3.6.8 (default, Nov 21 2019, 19:31:34) [GCC 8.3.1 20190507 (Red Hat 8.3.1-4)] Ansible 相关文件 Ansible 配置文件列表 /etc/ansible/ansible.cfg 主配置文件，配置ansible工作特性,也可以在项目的目录中创建此文件,当前目录下如果也有ansible.cfg,则此文件优先生效,建议每个项目目录下,创建独有的ansible.cfg文 件 /etc/ansible/hosts 主机清单 /etc/ansible/roles/ 存放角色的目录 Ansible 主配置文件 Ansible 的配置文件可以放在多个不同地方,优先级从高到低顺序如下\nANSIBLE_CONFIG #环境变量,目录下的文件必须存在才能生效 ./ansible.cfg #当前目录下的ansible.cfg,一般一个项目对应一个专用配置文件,推荐使用 ~/.ansible.cfg #当前用户家目录下的.ansible.cfg /etc/ansible/ansible.cfg #系统默认配置文件 Ansible 的默认配置文件 /etc/ansible/ansible.cfg ,其中大部分的配置内容无需进行修改\n[defaults] #inventory = /etc/ansible/hosts #主机列表配置文件 #library = /usr/share/my_modules/ #库文件存放目录 #remote_tmp = $HOME/.ansible/tmp #临时py命令文件存放在远程主机目录 #local_tmp = $HOME/.ansible/tmp #本机的临时命令执行目录 #forks = 5 #默认并发数 #sudo_user = root #默认sudo 用户 #ask_sudo_pass = True #每次执行ansible命令是否询问ssh密码 #ask_pass = True #remote_port = 22 #host_key_checking = False #检查对应服务器的host_key，建议取消此行注释,实现第一次连 接自动信任目标主机 #log_path=/var/log/ansible.log #日志文件，建议启用 #module_name = command #默认模块，可以修改为shell模块 [privilege_escalation] #普通用户提权配置 #become=True #become_method=sudo #become_user=root #become_ask_pass=False 范例: 通过环境变量ANSIBLE_CONFIG指定ansible配置文件路径\n[root@rocky8 ~]#cd /data/ansible/ [root@rocky8 ansible]#cat ansbile.cfg [defaults] inventory = ./hosts [root@rocky8 ansible]#cat hosts [ubuntu] 10.0.0.100 [centos] 10.0.0.7 10.0.0.8 #定义变量 [root@rocky8 ansible]#export ANSIBLE_CONFIG=./ansbile.cfg [root@rocky8 ansible]#ansible --version ansible [core 2.12.6] config file = /data/ansible/ansbile.cfg configured module search path = [\u0026#39;/root/.ansible/plugins/modules\u0026#39;, \u0026#39;/usr/share/ansible/plugins/modules\u0026#39;] ansible python module location = /usr/lib/python3.9/site-packages/ansible ansible collection location = /root/.ansible/collections:/usr/share/ansible/collections executable location = /usr/bin/ansible python version = 3.9.6 (default, Nov 9 2021, 13:31:27) [GCC 8.5.0 20210514 (Red Hat 8.5.0-3)] jinja version = 3.1.2 libyaml = True [root@rocky8 ansible]#ansible --list-hosts all hosts (3): 10.0.0.100 10.0.0.7 10.0.0.8 范例: 创建ansible 指定项目专用的配置文件\n[root@ubuntu2004 ~]#ansible --version ansible 2.9.6 config file = /etc/ansible/ansible.cfg configured module search path = [\u0026#39;/root/.ansible/plugins/modules\u0026#39;, \u0026#39;/usr/share/ansible/plugins/modules\u0026#39;] ansible python module location = /usr/lib/python3/dist-packages/ansible executable location = /usr/bin/ansible python version = 3.8.10 (default, Mar 15 2022, 12:22:08) [GCC 9.4.0] [root@ubuntu2004 ~]#mkdir /data/ansible -p [root@ubuntu2004 ~]#cd /data/ansible/ [root@ubuntu2004 ansible]#touch ansible.cfg [root@ubuntu2004 ansible]#ansible --version ansible 2.9.6 config file = /data/ansible/ansible.cfg configured module search path = [\u0026#39;/root/.ansible/plugins/modules\u0026#39;, \u0026#39;/usr/share/ansible/plugins/modules\u0026#39;] ansible python module location = /usr/lib/python3/dist-packages/ansible executable location = /usr/bin/ansible python version = 3.8.10 (default, Mar 15 2022, 12:22:08) [GCC 9.4.0] [root@ubuntu2004 ansible]#cd [root@ubuntu2004 ~]#ansible --version ansible 2.9.6 config file = /etc/ansible/ansible.cfg configured module search path = [\u0026#39;/root/.ansible/plugins/modules\u0026#39;, \u0026#39;/usr/share/ansible/plugins/modules\u0026#39;] ansible python module location = /usr/lib/python3/dist-packages/ansible executable location = /usr/bin/ansible python version = 3.8.10 (default, Mar 15 2022, 12:22:08) [GCC 9.4.0] 范例: 当前目录下的ansible的配置文件优先生效\n[root@ansible ~]#ansible --version ansible 2.9.17 config file = /etc/ansible/ansible.cfg configured module search path = [\u0026#39;/root/.ansible/plugins/modules\u0026#39;, \u0026#39;/usr/share/ansible/plugins/modules\u0026#39;] ansible python module location = /usr/lib/python3.6/site-packages/ansible executable location = /usr/bin/ansible python version = 3.6.8 (default, Apr 16 2020, 01:36:27) [GCC 8.3.1 20191121 (Red Hat 8.3.1-5)] [root@ansible ~]#cp /etc/ansible/ansible.cfg . [root@ansible ~]#ansible --version ansible 2.9.17 config file = /root/ansible.cfg #注意配置文件路径 configured module search path = [\u0026#39;/root/.ansible/plugins/modules\u0026#39;, \u0026#39;/usr/share/ansible/plugins/modules\u0026#39;] ansible python module location = /usr/lib/python3.6/site-packages/ansible executable location = /usr/bin/ansible python version = 3.6.8 (default, Apr 16 2020, 01:36:27) [GCC 8.3.1 20191121 (Red Hat 8.3.1-5)] [root@ansible ~]# Inventory 主机清单文件 ansible的主要功用在于批量主机操作，为了便捷地使用其中的部分主机，可以在inventory 主机清单文件中将其分组组织 默认的inventory file为 /etc/ansible/hosts inventory file可以有多个，且也可以通过Dynamic Inventory来动态生成 注意:\n生产建议在每个项目目录下创建项目独立的hosts文件 通过项目目录下的ansible.cfg文件中的 inventory = ./hosts实现 官方文档:\nhttps://docs.ansible.com/ansible/latest/user_guide/intro_inventory.html 主机清单文件格式 inventory文件遵循INI文件风格，中括号中的字符为组名。可以将同一个主机同时归并到多个不同的组中,此外，当如若目标主机使用了非默认的SSH端口，还可以在主机名称之后使用冒号加端口号来标明,如果主机名称遵循相似的命名模式，还可以使用列表的方式标识各主机 Inventory 参数说明\nansible_ssh_host #将要连接的远程主机名.与你想要设定的主机的别名不同的话,可通过此变量设置. ansible_ssh_port #ssh端口号.如果不是默认的端口号,通过此变量设置.这种可以使用 ip:端口 192.168.1.100:2222 ansible_ssh_user #默认的 ssh 用户名 ansible_ssh_pass #ssh 密码(这种方式并不安全,我们强烈建议使用 --ask-pass 或 SSH 密钥) ansible_sudo_pass #sudo 密码(这种方式并不安全,我们强烈建议使用 --ask-sudo-pass) ansible_sudo_exe (new in version 1.8) #sudo 命令路径(适用于1.8及以上版本) ansible_connection #与主机的连接类型.比如:local, ssh 或者 paramiko. Ansible 1.2 以前默认使用 paramiko.1.2 以后默认使用 \u0026#39;smart\u0026#39;,\u0026#39;smart\u0026#39; 方式会根据是否支持 ControlPersist,来判断\u0026#39;ssh\u0026#39; 方式是否可行. ansible_ssh_private_key_file #ssh 使用的私钥文件.适用于有多个密钥,而你不想使用 SSH 代理的情况. ansible_shell_type #目标系统的shell类型.默认情况下,命令的执行使用 \u0026#39;sh\u0026#39; 语法,可设置为\u0026#39;csh\u0026#39; 或 \u0026#39;fish\u0026#39;. ansible_python_interpreter #目标主机的 python 路径.适用于的情况: 系统中有多个 Python,或者命令路径不是\u0026#34;/usr/bin/python\u0026#34;,比如 \\*BSD, 或者 /usr/bin/python 不是 2.X 版本的Python.之所以不使用 \u0026#34;/usr/bin/env\u0026#34; 机制,因为这要求远程用户的路径设置正确,且要求 \u0026#34;python\u0026#34;可执行程序名不可为 python以外的名字(实际有可能名为python26).与ansible_python_interpreter 的工作方式相同,可设定如 ruby 或 perl 的路径.... 范例：\nntp.wang.org [webservers] www1.wang.org:2222 www2.wang.org [dbservers] db1.wang.org db2.wang.org db3.wang.org #或者 db[1:3].wang.org 范例: 组嵌套\n[webservers] www[1:100].example.com [dbservers] db-[a:f].example.com [appservers] 10.0.0.[1:100] #定义testsrvs组中包括两个其它分组,实现组嵌套 [testsrvs:children] webservers dbservers 范例: 基于用户名和密码的ssh连接主机清单\n[test] 10.0.0.8 ansible_connection=local #指定本地连接,无需ssh配置 #每个主机分别指定用户和密码,ansible_connection=ssh 需要StrictHostKeyChecking no 或者host_key_checking = False 10.0.0.7 ansible_connection=ssh ansible_ssh_port=2222 ansible_ssh_user=wangansible_ssh_password=123456 10.0.0.6 ansible_ssh_user=root ansible_ssh_password=123456 #对每个分组的所有主机统一定义用户和密码,执行ansible命令时显示别名,如web01 [websrvs] web01 ansible_ssh_host=10.0.0.101 web02 ansible_ssh_host=10.0.0.102 [websrvs:vars] ansible_ssh_password=magedu some_host ansible_ssh_port=2222 ansible_ssh_user=manager aws_host ansible_ssh_private_key_file=/home/example/.ssh/aws.pem freebsd_host ansible_python_interpreter=/usr/local/bin/python ruby_module_host ansible_ruby_interpreter=/usr/bin/ruby.1.9.3 Ansible相关工具 /usr/bin/ansible 主程序，临时命令执行工具\n/usr/bin/ansible-doc 查看配置文档，模块功能查看工具,相当于man\n/usr/bin/ansible-playbook 定制自动化任务，编排剧本工具,相当于脚本\n/usr/bin/ansible-pull 远程执行命令的工具\n/usr/bin/ansible-vault 文件加密工具\n/usr/bin/ansible-console 基于Console界面与用户交互的执行工具\n/usr/bin/ansible-galaxy 下载/上传优秀代码或Roles模块的官网平台\n利用ansible实现管理的主要方式：\nAnsible Ad-Hoc 即利用ansible命令，主要用于临时命令使用场景\nAnsible playbook 主要用于长期规划好的，大型项目的场景，需要有前期的规划过程\nansible 使用前准备 ansible 相关工具大多数是通过ssh协议，实现对远程主机的配置管理、应用部署、任务执行等功能 建议：使用此工具前，先配置ansible主控端能基于密钥认证的方式联系各个被管理节点 范例：利用sshpass批量实现基于key验证脚本1\n[root@centos8 ~]#vim /etc/ssh/ssh_config #修改下面一行 StrictHostKeyChecking no [root@centos8 ~]#cat hosts.list 192.168.32.178 192.168.32.179 [root@centos8 ~]#vim push_ssh_key.sh #!/bin/bash rpm -ql shpass \u0026amp;\u0026gt; /dev/null || yum -y install sshpass [ -f /root/.ssh/id_rsa ] || ssh-keygen -f /root/.ssh/id_rsa -P \u0026#39;\u0026#39; export SSHPASS=123456 while read IP;do sshpass -e ssh-copy-id -o StrictHostKeyChecking=no $IP done \u0026lt; hosts.list 范例: 实现基于key验证的脚本2\n[root@centos8 ~]#cat ssh_key.sh #!/bin/bash PLIST=\u0026#34; 192.168.32.178 192.168.32.179\u0026#34; rpm -q sshpass \u0026amp;\u0026gt; /dev/null || yum -y install sshpass [ -f /root/.ssh/id_rsa ] || ssh-keygen -f /root/.ssh/id_rsa -P \u0026#39;\u0026#39; export SSHPASS=123456 for IP in $IPLIST;do { sshpass -e ssh-copy-id -o StrictHostKeyChecking=no $IP; } \u0026amp; done wait ansible-doc 此工具用来显示模块帮助,相当于man 格式\nansible-doc [options] [module...] -l, --list #列出可用模块 -s, --snippet #显示指定模块的playbook片段 范例: 查看帮助\n[root@rocky ~]# ansible-doc --help usage: ansible-doc [-h] [--version] [-v] [-M MODULE_PATH] [--playbook-dir BASEDIR] [-t {become,cache,callback,cliconf,connection,httpapi,inventory,lookup,netconf,shell,vars,module,strategy,role,keyword}] [-j] [-r ROLES_PATH] [-e ENTRY_POINT | -s | -F | -l | --metadata-dump] [--no-fail-on-errors] [plugin ...] plugin documentation tool positional arguments: plugin Plugin 范例：\n#列出所有模块 ansible-doc -l #查看指定模块帮助用法 ansible-doc ping #查看指定模块帮助用法 ansible-doc -s ping 范例: 查看指定的插件\n[root@rocky ~]# ansible-doc -t connection -l local execute on controller paramiko_ssh Run tasks via python ssh (paramiko) psrp Run tasks over Microsoft PowerShell Remoting Protocol ssh connect via SSH client binary winrm Run tasks over Microsoft\u0026#39;s WinRM [root@rocky ~]# [root@rocky ~]# ansible-doc -t lookup -l config Lookup current Ansible configuration values csvfile read data from a TSV or CSV file dict returns key/value pair items from dictionaries env Read the value of environment variables file read file contents fileglob list files matching a pattern first_found return first file found from list indexed_items rewrites lists to return \u0026#39;indexed items\u0026#39; ini read data from an ini file inventory_hostnames list of inventory hosts matching a host pattern items list of items lines read lines from command list simply returns what it is given nested composes a list with nested elements of other lists password retrieve or generate a random password, stored in a file pipe read output from a command random_choice return random element from list sequence generate a list based on a number sequence subelements traverse nested key from a list of dictionaries template retrieve contents of file after templating with Jinja2 together merges lists into synchronized list unvault read vaulted file(s) contents url return contents from URL varnames Lookup matching variable names vars Lookup templated value of variables [root@rocky ~]# ansible Ansible Ad-Hoc 介绍 Ansible Ad-Hoc 的执行方式的主要工具就是 ansible 特点: 一次性的执行,不会保存执行命令信息,只适合临时性或测试性的任务\nansible 命令用法 格式：\nansible \u0026lt;host-pattern\u0026gt; [-m module_name] [-a args] 选项说明：\n--version #显示版本 -m module #指定模块，默认为command -v #详细过程 -vv -vvv更详细 --list-hosts #显示主机列表，可简写 --list -C, --check #检查，并不执行 -T, --timeout=TIMEOUT #执行命令的超时时间，默认10s -k, --ask-pass #提示输入ssh连接密码，默认Key验证 -u, --user=REMOTE_USER #执行远程执行的用户,默认root -b, --become #代替旧版的sudo实现通过sudo机制实现提升权限 --become-user=USERNAME #指定sudo的runas用户，默认为root -K, --ask-become-pass #提示输入sudo时的口令 -f FORKS, --forks FORKS #指定并发同时执行ansible任务的主机数 -i INVENTORY, --inventory INVENTORY #指定主机清单文件 范例:\n#以wang用户执行ping存活检测 ansible all -m ping -u wang -k #以wang sudo至root执行ping存活检测 ansible all -m ping -u wang -k -b #以wang sudo至mage用户执行ping存活检测 ansible all -m ping -u wang -k -b --become-user=mage #以wang sudo至root用户执行ls ansible all -m command -u wang -a \u0026#39;ls /root\u0026#39; -b --become-user=root -k -K 范例: 并发执行控制\n#分别执行下面两条命令观察结果 [root@ansible ~]#ansible all -a \u0026#39;sleep 5\u0026#39; -f1 [root@ansible ~]#ansible all -a \u0026#39;sleep 5\u0026#39; -f10 范例: 使用普能用户进行远程管理\n#在所有控制端和被控制端创建用户和密码 [root@rocky8 ~]#useradd wang [root@rocky8 ~]#echo wang:123456 | chpasswd #在所有被控制端对用户sudo授权 [root@rocky8 ~]#visudo wang ALL=(ALL) NOPASSWD: ALL [root@rocky8 ~]#visudo -c /etc/sudoers: parsed OK #实现从控制端到被控制端的基于key验证 [root@ansible ~]#su - wang wang@ansible:~$ssh-keygen -f ~/.ssh/id_rsa -P \u0026#39;\u0026#39; wang@ansible:~$$ssh-copy-id wang@\u0026#39;10.0.0.8\u0026#39; #使用普通用户测试连接,默认连接权限不足失败 wang@ansible:~$ ansible 10.0.0.8 -m shell -a \u0026#39;ls /root\u0026#39; 10.0.0.8 | FAILED | rc=2 \u0026gt;\u0026gt; ls: cannot open directory \u0026#39;/root\u0026#39;: Permission deniednon-zero return code #使用普通用户通过-b选项连接实现sudo提权后连接成功 wang@ansible:~$ ansible 10.0.0.8 -m shell -a \u0026#39;ls /root\u0026#39; -b --become-user root 10.0.0.8 | CHANGED | rc=0 \u0026gt;\u0026gt; anaconda-ks.cfg #修改配置文件指定sudo机制 [root@ansible ~]#vim /etc/ansible/ansible.cfg #取消下面行前面的注释 [privilege_escalation] become=True become_method=sudo become_user=root become_ask_pass=False #再次测试 [root@ansible ~]#su - wang wang@ansible:~$ ansible 10.0.0.8 -m shell -a \u0026#39;ls /root\u0026#39; 10.0.0.8 | CHANGED | rc=0 \u0026gt;\u0026gt; anaconda-ks.cfg 范例: 使用普通用户连接远程主机执行代替另一个用户身份执行操作\n[root@centos8 ~]#useradd wang [root@centos8 ~]#echo wang:123456 | chpasswd #先在被控制端能过sudo对普通用户授权 [root@centos8 ~]#grep wang /etc/sudoers wang ALL=(ALL) NOPASSWD: ALL #以wang的用户连接用户,并利用sudo代表mage执行whoami命令 [root@ansible ~]#ansible 10.0.0.8 -m shell -a \u0026#39;whoami\u0026#39; -u wang -k -b --become- user=mage SSH password: #输入远程主机wang用户ssh连接密码 10.0.0.8 | CHANGED | rc=0 \u0026gt;\u0026gt; mage ansible的Host-pattern 用于匹配被控制的主机的列表 All ：表示所有Inventory中的所有主机 范例\nansible all -m ping *:通配符\nansible \u0026#34;*\u0026#34; -m ping ansible 192.168.1.* -m ping ansible \u0026#34;srvs\u0026#34; -m ping ansible \u0026#34;10.0.0.6 10.0.0.7\u0026#34; -m ping 或关系\nansible \u0026#34;websrvs:appsrvs\u0026#34; -m ping ansible \u0026#34;192.168.1.10:192.168.1.20\u0026#34; -m ping 逻辑与\n#在websrvs组并且在dbsrvs组中的主机 ansible \u0026#34;websrvs:\u0026amp;dbsrvs\u0026#34; -m ping 逻辑非\n#在所有主机,但不在websrvs组和dbsrvs组中的主机 #注意：此处为单引号 ansible \u0026#39;all:!dbsrvs:!websrvs\u0026#39; -m ping 综合逻辑\nansible \u0026#39;websrvs:dbsrvs:\u0026amp;appsrvs:!ftpsrvs\u0026#39; -m ping 正则表达式\nansible \u0026#34;websrvs:dbsrvs\u0026#34; -m ping ansible \u0026#34;~(web|db).*\\.magedu\\.com\u0026#34; -m ping ansible 命令的执行过程 加载自己的配置文件,默认/etc/ansible/ansible.cfg 查找主机清单中对应的主机或主机组 加载自己对应的模块文件，如：command 通过ansible将模块或命令生成对应的临时py文件，并将该文件传输至远程服务器的对应执行用户 $HOME/.ansible/tmp/ansible-tmp-数字/XXX.PY文件 给文件+x执行 执行并返回结果 删除临时py文件，退出 ansible 命令的执行状态 [root@centos8 ~]#grep -A 14 \u0026#39;\\[colors\\]\u0026#39; /etc/ansible/ansible.cfg [colors] #highlight = white #verbose = blue #warn = bright purple #error = red #debug = dark gray #deprecate = purple #skip = cyan #unreachable = red #ok = green #changed = yellow #diff_add = green #diff_remove = red #diff_lines = cyan 绿色：执行成功并且对目标主机不需要做改变的操作 黄色：执行成功并且对目标主机做变更 红色：执行失败 ansible-console 此工具可交互执行命令，支持tab，ansible 2.0+新增 提示符格式：\n执行用户@当前操作的主机组 (当前组的主机数量)[f:并发数]$ 常用子命令：\n设置并发数： forks n 例如： forks 10 切换组： cd 主机组 例如： cd web 列出当前组主机列表： list 列出所有的内置命令： ?或help 范例\n[root@ansible ~]#ansible-console Welcome to the ansible console. Type help or ? to list commands. root@all (3)[f:5]$ ping 10.0.0.7 | SUCCESS =\u0026gt; { \u0026#34;ansible_facts\u0026#34;: { \u0026#34;discovered_interpreter_python\u0026#34;: \u0026#34;/usr/bin/python\u0026#34; }, \u0026#34;changed\u0026#34;: false, \u0026#34;ping\u0026#34;: \u0026#34;pong\u0026#34; } 10.0.0.6 | SUCCESS =\u0026gt; { \u0026#34;ansible_facts\u0026#34;: { \u0026#34;discovered_interpreter_python\u0026#34;: \u0026#34;/usr/bin/python\u0026#34; }, \u0026#34;changed\u0026#34;: false, \u0026#34;ping\u0026#34;: \u0026#34;pong\u0026#34; } 10.0.0.8 | SUCCESS =\u0026gt; { \u0026#34;ansible_facts\u0026#34;: { \u0026#34;discovered_interpreter_python\u0026#34;: \u0026#34;/usr/libexec/platform-python\u0026#34; }, \u0026#34;changed\u0026#34;: false, \u0026#34;ping\u0026#34;: \u0026#34;pong\u0026#34; } root@all (3)[f:5]$ list 10.0.0.8 10.0.0.7 10.0.0.6 root@all (3)[f:5]$ cd websrvs root@websrvs (2)[f:5]$ list 10.0.0.7 10.0.0.8 root@websrvs (2)[f:5]$ forks 10 root@websrvs (2)[f:10]$ cd appsrvs root@appsrvs (2)[f:5]$ yum name=httpd state=present root@appsrvs (2)[f:5]$ service name=httpd state=started ansible-playbook 此工具用于执行编写好的 playbook 任务 范例：\nansible-playbook hello.yml cat hello.yml --- #hello world yml file - hosts: websrvs remote_user: root gather_facts: no tasks: - name: hello world command: /usr/bin/wall hello world ansible-vault 此工具可以用于加密解密yml文件 格式：\nansible-vault [create|decrypt|edit|encrypt|rekey|view] 范例\nansible-vault encrypt hello.yml #加密 ansible-vault decrypt hello.yml #解密 ansible-vault view hello.yml #查看 ansible-vault edit hello.yml #编辑加密文件 ansible-vault rekey hello.yml #修改口令 ansible-vault create new.yml #创建新文件 #执行加密的playbook,交互式输入密码 chmod 600 hello.yml ansible-playbook --ask-vault-pass hello.yml #从pass.txt文件中读取密码 ansible-playbook --vault-password-file pass.txt hello.yml #从配置文件中取得密码 #vi /etc/ansible/ansible.cfg [defaults] ault-password-file=pass.txt #可以直接执行加密文件 ansible-playbook hello.yml ansible-galaxy Galaxy 是一个免费网站, 类似于github网站, 网站上发布了很多的共享的roles角色。 Ansible 提供了ansible-galaxy命令行工具连接 https://galaxy.ansible.com 网站下载相应的roles, 进行init(初始化、search( 查拘、install(安装、 remove(移除)等操作。\n范例：\n#搜索项目 [root@ansible ~]#ansible-galaxy search lamp #列出所有已安装的galaxy ansible-galaxy list #安装galaxy,默认下载到~/.ansible/roles下 ansible-galaxy install geerlingguy.mysql ansible-galaxy install geerlingguy.redis #删除galaxy ansible-galaxy remove geerlingguy.redis Ansible常用模块 2015年12月只270多个模块 2016年12年26日ansible 1.9.2 有540个模块 2018年01月12日ansible 2.3.8 有1378个模块 2018年05月28日ansible 2.5.3 有1562个模块 2018年07月15日ansible 2.6.3 有1852个模块 2018年11月19日ansible 2.7.2 有2080个模块 2020年03月02日ansible 2.9.5 有3387个模块 2021年12月22日ansible 2.11.8 有6141个模块 2022年06月04日ansible 2.12.6 有6763个模块 虽然模块众多，但最常用的模块也就2，30个而已，针对特定业务只需要熟悉10几个模块即可 常用模块帮助文档参考：\nhttps://docs.ansible.com/ansible/2.9/modules/modules_by_category.html https://docs.ansible.com/ansible/2.9/modules/list_of_all_modules.html https://docs.ansible.com/ansible/latest/modules/list_of_all_modules.html https://docs.ansible.com/ansible/latest/modules/modules_by_category.html Command 模块 功能：在远程主机执行命令，此为默认模块，可忽略 -m 选项 注意：此命令不支持 $VARNAME \u0026lt; \u0026gt; | ; \u0026amp; 等，可用shell模块实现 注意：此模块不具有幂等性 常见选项\nchdir=dir #执行命令前,先切换至目录dir creates=file #当file不存在时才会执行 removes=file #当file存在时才会执行 范例：\n[root@ansible ~]#ansible websrvs -m command -a \u0026#39;chdir=/etc cat centos-release\u0026#39; 10.0.0.7 | CHANGED | rc=0 \u0026gt;\u0026gt; CentOS Linux release 7.7.1908 (Core) 10.0.0.8 | CHANGED | rc=0 \u0026gt;\u0026gt; CentOS Linux release 8.1.1911 (Core) [root@ansible ~]#ansible websrvs -m command -a \u0026#39;chdir=/etc creates=/data/f1.txt cat centos-release\u0026#39; 10.0.0.7 | CHANGED | rc=0 \u0026gt;\u0026gt; CentOS Linux release 7.7.1908 (Core) 10.0.0.8 | SUCCESS | rc=0 \u0026gt;\u0026gt; skipped, since /data/f1.txt exists [root@ansible ~]#ansible websrvs -m command -a \u0026#39;chdir=/etc removes=/data/f1.txt cat centos-release\u0026#39; 10.0.0.7 | SUCCESS | rc=0 \u0026gt;\u0026gt; skipped, since /data/f1.txt does not exist 10.0.0.8 | CHANGED | rc=0 \u0026gt;\u0026gt; CentOS Linux release 8.1.1911 (Core) ansible websrvs -m command -a \u0026#39;service vsftpd start\u0026#39; ansible websrvs -m command -a \u0026#39;echo magedu |passwd --stdin wang\u0026#39; ansible websrvs -m command -a \u0026#39;rm -rf /data/\u0026#39; ansible websrvs -m command -a \u0026#39;echo hello \u0026gt; /data/hello.log\u0026#39; ansible websrvs -m command -a \u0026#34;echo $HOSTNAME\u0026#34; Shell 模块 功能：和command相似，用shell执行命令,支持各种符号,比如:*,$, \u0026gt; , 相当于增强版的command模块 注意：此模块不具有幂等性,建议能不能就用此模块,最好使用专用模块 常见选项\nchdir=dir #执行命令前,先切换至目录dir creates=file #当file不存在时才会执行 removes=file #当file存在时才会执行 范例：\n[root@ansible ~]#ansible websrvs -m shell -a \u0026#34;echo $HOSTNAME\u0026#34; 10.0.0.7 | CHANGED | rc=0 \u0026gt;\u0026gt; ansible 10.0.0.8 | CHANGED | rc=0 \u0026gt;\u0026gt; ansible [root@ansible ~]#ansible websrvs -m shell -a \u0026#39;echo $HOSTNAME\u0026#39; 10.0.0.7 | CHANGED | rc=0 \u0026gt;\u0026gt; centos7.wangxiaochun.com 10.0.0.8 | CHANGED | rc=0 \u0026gt;\u0026gt; centos8.localdomain [root@ansible ~]#ansible websrvs -m shell -a \u0026#39;echo centos | passwd --stdin wang\u0026#39; 10.0.0.7 | CHANGED | rc=0 \u0026gt;\u0026gt; Changing password for user wang. passwd: all authentication tokens updated successfully. 10.0.0.8 | CHANGED | rc=0 \u0026gt;\u0026gt; Changing password for user wang. passwd: all authentication tokens updated successfully. [root@ansible ~]#ansible websrvs -m shell -a \u0026#39;ls -l /etc/shadow\u0026#39; 10.0.0.7 | CHANGED | rc=0 \u0026gt;\u0026gt; ---------- 1 root root 889 Mar 2 14:34 /etc/shadow 10.0.0.8 | CHANGED | rc=0 \u0026gt;\u0026gt; ---------- 1 root root 944 Mar 2 14:34 /etc/shadow [root@ansible ~]#ansible websrvs -m shell -a \u0026#39;echo hello \u0026gt; /data/hello.log\u0026#39; 10.0.0.7 | CHANGED | rc=0 \u0026gt;\u0026gt; 10.0.0.8 | CHANGED | rc=0 \u0026gt;\u0026gt; [root@ansible ~]#ansible websrvs -m shell -a \u0026#39;cat /data/hello.log\u0026#39; 10.0.0.7 | CHANGED | rc=0 \u0026gt;\u0026gt; hello 10.0.0.8 | CHANGED | rc=0 \u0026gt;\u0026gt; hello 注意：调用bash执行命令 类似 cat /tmp/test.md | awk -F\u0026rsquo;|\u0026rsquo; \u0026lsquo;{print $1,$2}\u0026rsquo; \u0026amp;\u0026gt; /tmp/example.txt 这些复杂命令，即使使用shell也可能会失败，解决办法：写到脚本时，copy到远程，执行，再把需要的结果拉回执行命令的机器 范例：将shell模块代替command，设为模块\n[root@ansible ~]#vim /etc/ansible/ansible.cfg #修改下面一行 module_name = shell Script 模块 功能：在远程主机上运行ansible服务器上的脚本(无需执行权限) 注意：此模块不具有幂等性 常见选项\nchdir=dir #执行命令前,先切换至目录dir cmd #指定ansible主机的命令 creates=file #当file不存在时才会执行 removes=file #当file存在时才会执行 范例：\nansible websrvs -m script -a /data/test.sh Copy 模块 功能：复制ansible服务器主控端或远程的本机的文件到远程主机 注意: src=file 如果是没指明路径,则为当前目录或当前目录下的files目录下的file文件 常见选项\nsrc #控制端的源文件路径 dest #被控端的文件路径 owner #属主 group #属组 mode #权限 backup #是否备份 validate #验证成功才会执行copy remote_src #no是默认值,表示src文件在ansible主机,yes表示src文件在远程主机 范例:\n#如目标存在，默认覆盖，此处指定先备 ansible websrvs -m copy -a \u0026#34;src=/root/test1.sh dest=/tmp/test2.sh owner=wang mode=600 backup=yes\u0026#34; #指定内容，直接生成目标文件 ansible websrvs -m copy -a \u0026#34;content=\u0026#39;wang 123456\\nxiao 654321\\n\u0026#39; dest=/etc/rsync.pas owner=root group=root mode=0600\u0026#34; #复制/etc目录自身,注意/etc/后面没有/ ansible websrvs -m copy -a \u0026#34;src=/etc dest=/backup\u0026#34; #复制/etc/下的文件，不包括/etc/目录自身,注意/etc/后面有/ ansible websrvs -m copy -a \u0026#34;src=/etc/ dest=/backup\u0026#34; #复制/etc/suders,并校验语法 ansible websrvs -m copy -a \u0026#34;src=/etc/suders dest=/etc/sudoers.edit remote_src=yes validate=/usr/sbin/visudo -csf %s\u0026#34; Get_url 模块 功能: 用于将文件从http、https或ftp下载到被管理机节点上 常用参数如下：\nurl #下载文件的URL,支持HTTP，HTTPS或FTP协议 dest #下载到目标路径（绝对路径），如果目标是一个目录，就用原文件名，如果目标设置了名称就用目标 设置的名称 owner #指定属主 group #指定属组 mode #指定权限 force #如果yes，dest不是目录，将每次下载文件，如果内容改变替换文件。如果no，则只有在目标不存 在时才会下载 checksum #对目标文件在下载后计算摘要，以确保其完整性 #示例: checksum=\u0026#34;sha256:D98291AC[...]B6DC7B97\u0026#34;, checksum=\u0026#34;sha256:http://example.com/path/sha256sum.txt\u0026#34; url_username #用于HTTP基本认证的用户名。 对于允许空密码的站点，此参数可以不使用`url_password\u0026#39; url_password #用于HTTP基本认证的密码。 如果未指定`url_username\u0026#39;参数，则不会使用`url_password\u0026#39;参数 validate_certs #如果“no”，SSL证书将不会被验证。 适用于自签名证书在私有网站上使用 timeout #URL请求的超时时间,秒为单位 范例: 下载并MD5验证\n[root@ansible ~]#ansible websrvs -m get_url -a \u0026#39;url=http://nginx.org/download/nginx-1.18.0.tar.gz dest=/usr/local/src/nginx.tar.gz checksum=\u0026#34;md5:b2d33d24d89b8b1f87ff5d251aa27eb8\u0026#34;\u0026#39; Fetch 模块 功能：从远程主机提取文件至ansible的主控端，copy相反，目前不支持目录 常见选项\nsrc #被控制端的源文件路径,只支持文件 dest #ansible控制端的目录路径 范例：\nansible websrvs -m fetch -a \u0026#39;src=/root/test.sh dest=/data/scripts\u0026#39; 范例：\n[root@ansible ~]#ansible all -m fetch -a \u0026#39;src=/etc/redhat-release dest=/data/os\u0026#39; [root@ansible ~]#tree /data/os/ /data/os/ ├── 10.0.0.6 │ └── etc │ └── redhat-release ├── 10.0.0.7 │ └── etc │ └── redhat-release └── 10.0.0.8 └── etc └── redhat-release 6 directories, 3 files File 模块 功能：设置文件属性,创建文件,目录和软链接等 常见选项\npath #在被控端创建的路径 owner #属主 group #属组 mode #权限 state #状态 =touch #创建文件 =directory #创建目录 =link #软链接 =hard #硬链接 recurse #yes表示递归授权 范例：\n#创建空文件 ansible all -m file -a \u0026#39;path=/data/test.txt state=touch\u0026#39; ansible all -m file -a \u0026#39;path=/data/test.txt state=absent\u0026#39; ansible all -m file -a \u0026#34;path=/root/test.sh owner=wang mode=755\u0026#34; #创建目录 ansible all -m file -a \u0026#34;path=/data/mysql state=directory owner=mysql group=mysql\u0026#34; #创建软链接 ansible all -m file -a \u0026#39;src=/data/testfile path|dest|name=/data/testfile-link state=link\u0026#39; #创建目录 ansible all -m file -a \u0026#39;path=/data/testdir state=directory\u0026#39; #递归修改目录属性,但不递归至子目录 ansible all -m file -a \u0026#34;path=/data/mysql state=directory owner=mysql group=mysql\u0026#34; #递归修改目录及子目录的属性 ansible all -m file -a \u0026#34;path=/data/mysql state=directory owner=mysql group=mysql recurse=yes\u0026#34; stat 模块 功能：检查文件或文件系统的状态 注意：对于Windows目标，请改用win_stat模块\n常见选项\npath #文件/对象的完整路径（必须） 常用的返回值判断：\nexists： 判断是否存在 isuid： 调用用户的ID与所有者ID是否匹配 范例:\n[root@ansible ~]#ansible 127.0.0.1 -m stat -a \u0026#39;path=/etc/passwd\u0026#39; 127.0.0.1 | SUCCESS =\u0026gt; { \u0026#34;changed\u0026#34;: false, \u0026#34;stat\u0026#34;: { \u0026#34;atime\u0026#34;: 1614601466.7493012, \u0026#34;attr_flags\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;attributes\u0026#34;: [], \u0026#34;block_size\u0026#34;: 4096, \u0026#34;blocks\u0026#34;: 8, \u0026#34;charset\u0026#34;: \u0026#34;us-ascii\u0026#34;, \u0026#34;checksum\u0026#34;: \u0026#34;8f7a9a996d24de98bf1eab4a047f8e89e9c708cf\u0026#34;, \u0026#34;ctime\u0026#34;: 1614334259.4498665, \u0026#34;dev\u0026#34;: 2050, \u0026#34;device_type\u0026#34;: 0, \u0026#34;executable\u0026#34;: false, \u0026#34;exists\u0026#34;: true, \u0026#34;gid\u0026#34;: 0, \u0026#34;gr_name\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;inode\u0026#34;: 134691833, \u0026#34;isblk\u0026#34;: false, \u0026#34;ischr\u0026#34;: false, \u0026#34;isdir\u0026#34;: false, \u0026#34;isfifo\u0026#34;: false, \u0026#34;isgid\u0026#34;: false, \u0026#34;islnk\u0026#34;: false, \u0026#34;isreg\u0026#34;: true, \u0026#34;issock\u0026#34;: false, \u0026#34;isuid\u0026#34;: false, \u0026#34;mimetype\u0026#34;: \u0026#34;text/plain\u0026#34;, \u0026#34;mode\u0026#34;: \u0026#34;0000\u0026#34;, \u0026#34;mtime\u0026#34;: 1614334259.4498665, \u0026#34;nlink\u0026#34;: 1, \u0026#34;path\u0026#34;: \u0026#34;/etc/passwd\u0026#34;, \u0026#34;pw_name\u0026#34;: \u0026#34;root\u0026#34;, \u0026#34;readable\u0026#34;: true, \u0026#34;rgrp\u0026#34;: false, \u0026#34;roth\u0026#34;: false, \u0026#34;rusr\u0026#34;: false, \u0026#34;size\u0026#34;: 1030, \u0026#34;uid\u0026#34;: 0, \u0026#34;version\u0026#34;: \u0026#34;671641160\u0026#34;, \u0026#34;wgrp\u0026#34;: false, \u0026#34;woth\u0026#34;: false, \u0026#34;writeable\u0026#34;: true, \u0026#34;wusr\u0026#34;: false, \u0026#34;xgrp\u0026#34;: false, \u0026#34;xoth\u0026#34;: false, \u0026#34;xusr\u0026#34;: false } } 案例：\n- name: install | Check if file is already configured. stat: path={{ nginx_file_path }} connection: local register: nginx_file_result - name: install | Download nginx file get_url: url={{ nginx_file_url }} dest={{ software_files_path }} validate_certs=no connection: local when:，not. nginx_file_result.stat.exists 范例:\n[root@ansible ansible]#cat stat.yml --- - hosts: websrvs tasks: - name: check file stat: path=/data/mysql register: st - name: debug debug: msg: \u0026#34;/data/mysql is not exist\u0026#34; when: not st.stat.exists [root@ansible ansible]#ansible-playbook stat.yml PLAY [websrvs] ******************************************************************************** *************************************** TASK [Gathering Facts] ******************************************************************************** ******************************* ok: [10.0.0.7] ok: [10.0.0.8] TASK [check file] ******************************************************************************** ************************************ ok: [10.0.0.7] ok: [10.0.0.8] TASK [debug] ******************************************************************************** ***************************************** ok: [10.0.0.7] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;/data/mysql is not exist\u0026#34; } ok: [10.0.0.8] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;/data/mysql is not exist\u0026#34; } PLAY RECAP ******************************************************************************** ******************************************* 10.0.0.7 : ok=3 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 10.0.0.8 : ok=3 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 unarchive 模块 功能：解包解压缩 实现有两种用法：\n将ansible主机上的压缩包传到远程主机后解压缩至特定目录，设置remote_src=no,此为默认值,可省略 将远程本主机上或非ansible的其它主机的某个压缩包解压缩到远程主机本机的指定路径下，需要设置remote_src=yes 常见参数：\nremote_src #和copy功能一样且选项互斥，yes表示源文件在远程被控主机或其它非ansible的其它主机上，no表示文件在ansible主机上,默认值为no, 此选项代替copy选项 copy #默认为yes，当copy=yes，拷贝的文件是从ansible主机复制到远程主机上，如果设置为copy=no，会在远程主机上寻找src源文件,此选项已废弃 src #源路径，可以是ansible主机上的路径，也可以是远程主机(被管理端或者第三方主机)上的路径，如果是远程主机上的路径，则需要设置remote_src=yes dest #远程主机上的目标路径 mode #设置解压缩后的文件权限 creates=/path/file #当绝对路径/path/file不存在时才会执行 范例：\nansible all -m unarchive -a \u0026#39;src=/data/foo.tgz dest=/var/lib/foo owner=wang group=bin\u0026#39; ansible all -m unarchive -a \u0026#39;src=/tmp/foo.zip dest=/data mode=0777\u0026#39; ansible all -m unarchive -a \u0026#39;src=https://example.com/example.zip dest=/data \u0026#39; ansible websrvs -m unarchive -a \u0026#39;src=https://releases.ansible.com/ansible/ansible-2.1.6.0-0.1.rc1.tar.gz dest=/data/ owner=root remote_src=yes\u0026#39; ansible websrvs -m unarchive -a \u0026#39;src=http://nginx.org/download/nginx- 1.18.0.tar.gz dest=/usr/local/src/ remote_src=yes\u0026#39;\u0026#39; Archive 模块 功能：打包压缩保存在被管理节点\n常见选项\npath #压缩的文件或目录 dest #压缩后的文件 format #压缩格式,支持gz,bz2,xz,tar,zip 范例：\nansible websrvs -m archive -a \u0026#39;path=/var/log/ dest=/data/log.tar.bz2 format=bz2 owner=wang mode=0600\u0026#39; Hostname 模块 功能：管理主机名 常见选项\nname #修改后的主机名称 范例：\nansible node1 -m hostname -a \u0026#34;name=websrv\u0026#34; ansible 10.0.0.18 -m hostname -a \u0026#39;name=node18.wang.org\u0026#39; Cron 模块 功能：计划任务 支持时间：minute，hour，day，month，weekday 常见选项\nname #描述脚本的作用 minute #分钟 hour #小时 weekday #周 user #任务由哪个用户运行；默认root job #任务 范例：\n#备份数据库脚本 [root@centos8 ~]#cat /root/mysql_backup.sh #!/bin/bash mysqldump -A -F --single-transaction --master-data=2 -q -uroot |gzip \u0026gt; /data/mysql_`date +%F_%T`.sql.gz #创建任务 ansible 10.0.0.8 -m cron -a \u0026#39;hour=2 minute=30 weekday=1-5 name=\u0026#34;backup mysql\u0026#34; job=/root/mysql_backup.sh\u0026#39; ansible websrvs -m cron -a \u0026#34;minute=*/5 job=\u0026#39;/usr/sbin/ntpdate ntp.aliyun.com \u0026amp;\u0026gt;/dev/null\u0026#39; name=Synctime\u0026#34; #禁用计划任务 ansible websrvs -m cron -a \u0026#34;minute=*/5 job=\u0026#39;/usr/sbin/ntpdate 172.20.0.1 \u0026amp;\u0026gt;/dev/null\u0026#39; name=Synctime disabled=yes\u0026#34; #启用计划任务 ansible websrvs -m cron -a \u0026#34;minute=*/5 job=\u0026#39;/usr/sbin/ntpdate 172.20.0.1 \u0026amp;\u0026gt; /dev/null\u0026#39; name=Synctime disabled=no\u0026#34; #删除任务 ansible websrvs -m cron -a \u0026#34;name=\u0026#39;backup mysql\u0026#39; state=absent\u0026#34; ansible websrvs -m cron -a \u0026#39;state=absent name=Synctime\u0026#39; Yum 和 Apt 模块 功能：管理软件包 yum 管理软件包，只支持RHEL，CentOS，fedora，不支持Ubuntu其它版本 apt 模块管理 Debian 相关版本的软件包 yum常见选项\nname #软件包名称 state #状态 =present #安装,此为默认值 =absent #删除 =latest #最新版 list #列出指定包 enablerepo #启用哪个仓库安装 disablerepo #不使用哪些仓库的包 exclude #排除指定的包 validate #是否检验,默认为yes 范例：\n[root@ansible ~]#ansible websrvs -m yum -a \u0026#39;name=httpd state=present\u0026#39; #安装zabbix agent rpm包 [root@ansible ~]#ansible websrvs -m yum -a \u0026#39;name=https://mirrors.tuna.tsinghua.edu.cn/zabbix/zabbix/5.0/rhel/8/x86_64/zabbix-agent2-5.0.24-1.el8.x86_64.rpm state=present validate_certs=no\u0026#39; #启用epel源进行安装 [root@ansible ~]#ansible websrvs -m yum -a \u0026#39;name=nginx state=present enablerepo=epel\u0026#39; #升级除kernel和foo开头以外的所有包 [root@ansible ~]#ansible websrvs -m yum -a \u0026#39;name=* state=lastest exclude=kernel*,foo*\u0026#39; #删除 [root@ansible ~]#ansible websrvs -m yum -a \u0026#39;name=httpd state=absent\u0026#39; [root@ansible ~]#ansible websrvs -m yum -a \u0026#39;name=sl,cowsay\u0026#39; yum_repository 模块 功能: 此模块实现yum的仓库配置管理 常见选项\nname #仓库id description #仓库描述名称,对应配置文件中的name= baseurl #仓库的地址 gpgcheck #验证开启 gpgkey #仓库公钥路径 state=absen #删除 范例：\nansible websrvs -m yum_repository -a \u0026#39;name=ansible_nginx description=\u0026#34;nginx repo\u0026#34; baseurl=\u0026#34;http://nginx.org/packages/centos/$releasever/$basearch/\u0026#34; gpgcheck=yes gpgkey=\u0026#34;https://nginx.org/keys/nginx_signing.key\u0026#34;\u0026#39; [root@rocky8 ~]#cat /etc/yum.repos.d/ansible_nginx.repo [ansible_nginx] baseurl = http://nginx.org/packages/centos/$releasever/$basearch/ gpgcheck = 1 gpgkey = https://nginx.org/keys/nginx_signing.key name = nginx repo Service 模块 此模块和sytemd功能相似,选项很多相同 功能：管理服务 常见选项\nname #服务名称 state #服务状态 =started #启动 =stopped #停止 =restarted #重启 =reloaded #重载 enabled #开启自启动 daemon_reload #加载新的配置文件,适用于systemd模块 范例：\nansible all -m service -a \u0026#39;name=httpd state=started enabled=yes\u0026#39; ansible all -m service -a \u0026#39;name=httpd state=stopped\u0026#39; ansible all -m service -a \u0026#39;name=httpd state=reloaded\u0026#39; ansible all -m shell -a \u0026#34;sed -i \u0026#39;s/^Listen 80/Listen 8080/\u0026#39; /etc/httpd/conf/httpd.conf\u0026#34; ansible all -m service -a \u0026#39;name=httpd state=restarted\u0026#39; #重启动指定网卡服务 ansible all -m service -a \u0026#39;name=network state=absent args=eth0\u0026#39; User 模块 功能：管理用户 常见选项\nname #创建的名称 uid #指定uid group #指定基本组 shell #登录shell类型默认/bin/bash create_home #是否创建家目录 password #设定对应的密码，必须是加密后的字符串才行，否则不生效 system #yes表示系统用户 groups #附加组 append #追加附加组使用,yes表示增加新的附加组 state #absen删除 remove #yes表示删除用户时将家目录一起删除 generate_ssh_key #创建私钥 ssh_keyu_bits #私钥位数 ssh_key_file #私钥文件路径 范例：\n#创建用户 ansible all -m user -a \u0026#39;name=user1 comment=\u0026#34;test user\u0026#34; uid=2048 home=/app/user1group=root\u0026#39; ansible all -m user -a \u0026#39;name=nginx comment=nginx uid=88 group=nginxgroups=\u0026#34;root,daemon\u0026#34; shell=/sbin/nologin system=yes create_home=nohome=/data/nginx non_unique=yes\u0026#39; #remove=yes表示删除用户及家目录等数据,默认remove=no ansible all -m user -a \u0026#39;name=nginx state=absent remove=yes\u0026#39; #生成123456加密的密码 ansible localhost -m debug -a \u0026#34;msg={{ \u0026#39;123456\u0026#39;| password_hash(\u0026#39;sha512\u0026#39;,\u0026#39;salt\u0026#39;)}}\u0026#34; localhost | SUCCESS =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;$6$salt$MktMKPZJ6t59GfxcJU20DwcwQzfMvOlHFVZiOVD71w.\u0026#34; } #用上面创建的密码创建用户 ansible websrvs -m user -a \u0026#39;name=www group=www system=yes shell=/sbin/nlogin password=\u0026#34;$6$salt$MktMKPZJ6t59GfxcJU20DwcwQzfMvOlHFVZiOVD71w.\u0026#34;\u0026#39; #创建用户test,并生成4096bit的私钥 ansible websrvs -m user -a \u0026#39;name=test generate_ssh_key=yes ssh_key_bits=4096 ssh_key_file=.ssh/id_rsa\u0026#39; Group 模块 功能：管理组 常见选项\nname #指定组名称 gid #指定gid state =present #创建,默认 =absent #删除 范例：\n#创建组 ansible websrvs -m group -a \u0026#39;name=nginx gid=88 system=yes\u0026#39; #删除组 ansible websrvs -m group -a \u0026#39;name=nginx state=absent\u0026#39; Lineinfile 模块 ansible在使用sed进行替换时，经常会遇到需要转义的问题，而且ansible在遇到特殊符号进行替换时， 会存在问题，无法正常进行替换 。\nansible自身提供了两个模块：lineinfile模块和replace模块，可以方便的进行替换一般在ansible当中去修改某个文件的单行进行替换的时候需要使用lineinfile模块 功能：相当于sed，主要用于修改一行的文件内容 常见选项\npath #被控端文件的路径 regexp #正则匹配语法格式,表示被替换的内容 line #替换为的内容 state #absent表示删除 insertafter #插入到替换内容前面,如和regexp同时存在,只在没找到与regexp匹配时才使用 insertafter insertbefore #插入到替换内容后面,如和regexp同时存在,只在没找到与regexp匹配时才使用 insertafter backrefs #支持后面引用,yes和no backup #修改前先备份 create #如果文件不存在,则创建,默认不存在会出错 mode #指定权限 owner #指定用户 group #指定组 #注意 regexp参数 ：使用正则表达式匹配对应的行，当替换文本时，如果有多行文本都能被匹配，则只有最后面被 匹配到的那行文本才会被替换，当删除文本时，如果有多行文本都能被匹配，这么这些行都会被删除。 注意: 如果想进行多行匹配进行替换需要使用replace模块 范例：\n#修改监听端口 ansible websrvs -m lineinfile -a \u0026#34;path=/etc/httpd/conf/httpd.conf regexp=\u0026#39;^Listen\u0026#39; line=\u0026#39;Listen 8080\u0026#39;\u0026#34; #修改SELinux ansible all -m lineinfile -a \u0026#34;path=/etc/selinux/config regexp=\u0026#39;^SELINUX=\u0026#39;line=\u0026#39;SELINUX=disabled\u0026#39;\u0026#34; #添加网关 ansible webservers -m lineinfile -a \u0026#39;path=/etc/sysconfig/network-scripts/ifcfg-eth0 line=\u0026#34;GATEWAY=10.0.0.254\u0026#34;\u0026#39; #给主机增加一个网关，但需要增加到NAME=下面 ansible webservers -m lineinfile -a \u0026#39;path=/etc/sysconfig/network-scripts/ifcfg-eth0 insertafter=\u0026#34;^NAME=\u0026#34; line=\u0026#34;GATEWAY=10.0.0.254\u0026#34;\u0026#39; #效果如下 cat /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=eth0 NAME=eth0 GATEWAY=10.0.0.254 #给主机增加一个网关，但需要增加到NAME=上面 ansible webservers -m lineinfile -a \u0026#39;path=/etc/sysconfig/network-scripts/ifcfg- eth0 insertbefore=\u0026#34;^NAME=\u0026#34; line=\u0026#34;GATEWAY=10.0.0.254\u0026#34;\u0026#39; #效果如下 cat /etc/sysconfig/network-scripts/ifcfg-eth0 DEVICE=eth0 GATEWAY=10.0.0.254 NAME=eth0 #删除网关 ansible webservers -m lineinfile -a \u0026#39;path=/etc/sysconfig/network-scripts/ifcfg-eth0 regexp=\u0026#34;^GATEWAY\u0026#34; state=absent\u0026#39; #删除#开头的行 ansible all -m lineinfile -a \u0026#39;dest=/etc/fstab state=absent regexp=\u0026#34;^#\u0026#34;\u0026#39; Replace 模块 该模块有点类似于sed命令，主要也是基于正则进行匹配和替换，建议使用 功能: 多行修改替换 常见选项\npath #被控端文件的路径 regexp #正则匹配语法格式,表示被替换的内容 replace #替换为的内容 after #插入到替换内容前面, before #插入到替换内容后面 backup #修改前先备份 mode #指定权限 owner #指定用户 group #指定组 范例\nansible all -m replace -a \u0026#34;path=/etc/fstab regexp=\u0026#39;^(UUID.*)\u0026#39; replace=\u0026#39;#\\1\u0026#39;\u0026#34; ansible all -m replace -a \u0026#34;path=/etc/fstab regexp=\u0026#39;^#(UUID.*)\u0026#39; replace=\u0026#39;\\1\u0026#39;\u0026#34; SELinux 模块 功能: 该模块管理 SELInux 策略 常见选项\npolicy #指定SELINUXTYPE=targeted state #指定SELINUX=disabled 范例\n[root@rocky ansible-apps]# ansible 192.168.32.132 -m selinux -a \u0026#39;state=disabled\u0026#39; 192.168.32.132 | FAILED! =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;The module selinux was redirected to ansible.posix.selinux, which could not be loaded.\u0026#34; } # ansible版本2.13.3出现如下错误 \u0026#34;msg\u0026#34;: \u0026#34;The module selinux was redirected to ansible.posix.selinux, which could not be loaded.\u0026#34; # 解决方法 [root@rocky ansible-apps]# ansible-galaxy collection install ansible.posix # 再次执行，显示成功 [root@rocky ansible-apps]# ansible 192.168.32.132 -m selinux -a \u0026#39;state=disabled\u0026#39; [WARNING]: SELinux state temporarily changed from \u0026#39;enforcing\u0026#39; to \u0026#39;permissive\u0026#39;. State change will take effect next reboot. 192.168.32.132 | CHANGED =\u0026gt; { \u0026#34;ansible_facts\u0026#34;: { \u0026#34;discovered_interpreter_python\u0026#34;: \u0026#34;/usr/libexec/platform-python\u0026#34; }, \u0026#34;changed\u0026#34;: true, \u0026#34;configfile\u0026#34;: \u0026#34;/etc/selinux/config\u0026#34;, \u0026#34;msg\u0026#34;: \u0026#34;Config SELinux state changed from \u0026#39;enforcing\u0026#39; to \u0026#39;disabled\u0026#39;\u0026#34;, \u0026#34;policy\u0026#34;: \u0026#34;targeted\u0026#34;, \u0026#34;reboot_required\u0026#34;: true, \u0026#34;state\u0026#34;: \u0026#34;disabled\u0026#34; } reboot 模块 功能: 重启 常见选项\nmsg #重启提示 pre_reboot_delay #重启前延迟时间的秒数 post_reboot_delay #重启后延迟时间的秒数后,再验证系统正常启动 reboot_timeout #重启后延迟时间再执行测试成功与否的命令 test_command #执行测试成功与否的命令 范例:\n[root@ansible ~]#ansible websrvs -m reboot -a \u0026#39;msg=\u0026#34;host will be reboot\u0026#34;\u0026#39; mount 模块 功能: 挂载和卸载文件系统 常见选项\nsrc #源设备路径，或网络地址 path #挂载至本地哪个路径下 fstype #设备类型； nfs opts #挂载的选项 state #挂载还是卸载 =present #永久挂载，但没有立即生效 =absent #卸载临时挂载,并删除永久挂载 =mounted #临时挂载 =unmounted #临时卸载 范例:\n#修改fstab文件永久挂载,但不立即生效 mount websrvs -m mount -a \u0026#39;src=\u0026#34;UUID=b3e48f45-f933-4c8e-a700-22a159ec9077\u0026#34; path=/home fstype=xfs opts=noatime state=present\u0026#39; #临时取消挂载 mount websrvs -m mount -a \u0026#39;path=/home fstype=xfs opts=noatime state=unmounted\u0026#39; #永久挂载,并立即生效 ansible websrvs -m mount -a \u0026#39;src=10.0.0.8:/data/wordpress path=/var/www/html/wp- content/uploads opts=\u0026#34;_netdev\u0026#34; state=mounted\u0026#39; #永久卸载,并立即生效 ansible websrvs -m mount -a \u0026#39;src=10.0.0.8:/data/wordpress path=/var/www/html/wp- content/uploads state=absent\u0026#39; Setup 模块 功能： setup 模块来收集主机的系统信息，这些 facts 信息可以直接以变量的形式使用，但是如果主机 较多，会影响执行速度 可以使用 gather_facts: no 来禁止 Ansible 收集 facts 信息 常见选项\nfilter #指定过滤条件 范例:\nansible all -m setup ansible all -m setup -a \u0026#34;filter=ansible_nodename\u0026#34; ansible all -m setup -a \u0026#34;filter=ansible_hostname\u0026#34; # 主机名称 ansible all -m setup -a \u0026#34;filter=ansible_domain\u0026#34; ansible all -m setup -a \u0026#34;filter=ansible_memtotal_mb\u0026#34; ansible all -m setup -a \u0026#34;filter=ansible_memory_mb\u0026#34; ansible all -m setup -a \u0026#34;filter=ansible_memfree_mb\u0026#34; ansible all -m setup -a \u0026#34;filter=ansible_os_family\u0026#34; ansible all -m setup -a \u0026#34;filter=ansible_distribution\u0026#34; ansible all -m setup -a \u0026#34;filter=ansible_distribution_major_version\u0026#34; ansible all -m setup -a \u0026#34;filter=ansible_distribution_version\u0026#34; ansible all -m setup -a \u0026#34;filter=ansible_processor_vcpus\u0026#34; ansible all -m setup -a \u0026#34;filter=ansible_all_ipv4_addresses\u0026#34; ansible all -m setup -a \u0026#34;filter=ansible_architecture\u0026#34; ansible all -m setup -a \u0026#34;filter=ansible_uptime_seconds\u0026#34; ansible all -m setup -a \u0026#34;filter=ansible_processor*\u0026#34; ansible all -m setup -a \u0026#39;filter=ansible_env\u0026#39; 范例：\n[root@ansible ~]#ansible all -m setup -a \u0026#39;filter=ansible_python_version\u0026#39; 10.0.0.7 | SUCCESS =\u0026gt; { \u0026#34;ansible_facts\u0026#34;: { \u0026#34;ansible_python_version\u0026#34;: \u0026#34;2.7.5\u0026#34;, \u0026#34;discovered_interpreter_python\u0026#34;: \u0026#34;/usr/bin/python\u0026#34; }, \u0026#34;changed\u0026#34;: false } 10.0.0.6 | SUCCESS =\u0026gt; { \u0026#34;ansible_facts\u0026#34;: { \u0026#34;ansible_python_version\u0026#34;: \u0026#34;2.6.6\u0026#34;, \u0026#34;discovered_interpreter_python\u0026#34;: \u0026#34;/usr/bin/python\u0026#34; }, \u0026#34;changed\u0026#34;: false } 10.0.0.8 | SUCCESS =\u0026gt; { \u0026#34;ansible_facts\u0026#34;: { \u0026#34;ansible_python_version\u0026#34;: \u0026#34;3.6.8\u0026#34;, \u0026#34;discovered_interpreter_python\u0026#34;: \u0026#34;/usr/libexec/platform-python\u0026#34; }, \u0026#34;changed\u0026#34;: false } [root@ansible ~]# 范例：取IP地址\n#取所有IP ansible 10.0.0.101 -m setup -a \u0026#39;filter=ansible_all_ipv4_addresses\u0026#39; 10.0.0.101 | SUCCESS =\u0026gt; { \u0026#34;ansible_facts\u0026#34;: { \u0026#34;ansible_all_ipv4_addresses\u0026#34;: [ \u0026#34;192.168.0.1\u0026#34;, \u0026#34;192.168.0.2\u0026#34;, \u0026#34;192.168.64.238\u0026#34;, \u0026#34;192.168.13.36\u0026#34;, \u0026#34;10.0.0.101\u0026#34;, \u0026#34;172.16.1.0\u0026#34;, \u0026#34;172.17.0.1\u0026#34; ] }, \u0026#34;changed\u0026#34;: false } #取默认IP ansible all -m setup -a \u0026#39;filter=\u0026#34;ansible_default_ipv4\u0026#34;\u0026#39; 10.0.0.101 | SUCCESS =\u0026gt; { \u0026#34;ansible_facts\u0026#34;: { \u0026#34;ansible_default_ipv4\u0026#34;: { \u0026#34;address\u0026#34;: \u0026#34;10.0.0.101\u0026#34;, \u0026#34;alias\u0026#34;: \u0026#34;eth0\u0026#34;, \u0026#34;broadcast\u0026#34;: \u0026#34;10.0.0.255\u0026#34;, \u0026#34;gateway\u0026#34;: \u0026#34;10.0.0.2\u0026#34;, \u0026#34;interface\u0026#34;: \u0026#34;eth0\u0026#34;, \u0026#34;macaddress\u0026#34;: \u0026#34;00:0c:29:e8:c7:9b\u0026#34;, \u0026#34;mtu\u0026#34;: 1500, \u0026#34;netmask\u0026#34;: \u0026#34;255.255.255.0\u0026#34;, \u0026#34;network\u0026#34;: \u0026#34;10.0.0.0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;ether\u0026#34; } }, \u0026#34;changed\u0026#34;: false } debug 模块 功能: 此模块可以用于输出信息,并且通过 msg 定制输出的信息内容,功能类似于echo命令 注意: msg后面的变量有时需要加 \u0026quot; \u0026quot; 引起来 常见选项\nmsg #指定命令输出的信息 var #指定变量名,和msg互斥 verbosity #详细度 范例: debug 模块默认输出Hello world\n[root@ansible ~]#ansible 10.0.0.18 -m debug 10.0.0.18 | SUCCESS =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;Hello world!\u0026#34; } [root@ansible ansible]#cat debug.yml --- - hosts: websrvs tasks: - name: output Hello world debug: #默认没有指定msg,默认输出\u0026#34;Hello world!\u0026#34; [root@ansible ansible]#ansible-playbook debug.yml ..... TASK [output variables] ******************************************************************************** ****************************** ok: [10.0.0.7] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;Hello world!\u0026#34; } ok: [10.0.0.8] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;Hello world!\u0026#34; } PLAY RECAP ******************************************************************************** ******************************************* 10.0.0.7 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 10.0.0.8 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 范例: 利用debug 模块输出变量\n[root@centos8 ~]#cat debug.yaml --- - hosts: websrvs tasks: - name: output variables debug: msg: Host \u0026#34;{{ ansible_nodename }}\u0026#34; Ip \u0026#34;{{ ansible_default_ipv4.address }}\u0026#34; [root@centos8 ~]#ansible-playbook debug.yaml PLAY [websrvs] ******************************************************************************** *************************************** TASK [Gathering Facts] ******************************************************************************** ******************************* ok: [10.0.0.7] ok: [10.0.0.8] TASK [output variables] ******************************************************************************** ****************************** ok: [10.0.0.7] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;Host \\\u0026#34;centos7.wangxiaochun.com\\\u0026#34; Ip \\\u0026#34;10.0.0.7\\\u0026#34;\u0026#34; } ok: [10.0.0.8] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;Host \\\u0026#34;centos8.wangxiaochun.com\\\u0026#34; Ip \\\u0026#34;10.0.0.8\\\u0026#34;\u0026#34; } PLAY RECAP ******************************************************************************** ******************************************* 10.0.0.7 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 10.0.0.8 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 范例: 显示字符串特定字符\n# cat debug.yml - hosts: all gather_facts: no vars: a: \u0026#34;12345\u0026#34; tasks: - debug: msg: - \u0026#34;{{a[0]}}\u0026#34; - \u0026#34;{{a[1]}}\u0026#34; - \u0026#34;{{a[2]}}\u0026#34; #定义了一个字符串变量a，如果想要获取a字符串的第3个字符，则可以使用”a[2]”获取，索引从0开始，执行上例playbook，debug的输出信息如下： TASK [debug] ************************* ok: [test1] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;1\u0026#34; \u0026#34;msg\u0026#34;: \u0026#34;2\u0026#34; \u0026#34;msg\u0026#34;: \u0026#34;3\u0026#34; } sysctl 模块 功能: 修改内核参数 常见选项\nname #内核参数 value #指定值 state #是否保存在sysctl.conf文件中,默认present sysctl_set #使用sysctl -w 验证值生效 范例:\nansible websrvs -m sysctl -a \u0026#39;name=net.ipv4.ip_forward value=1 state=present\u0026#39; 范例: 内核参数优化\n- name: Change Port Range sysctl: name: net.ipv4.ip_local_port_range value: \u0026#39;1024 65000\u0026#39; sysctl_set: yes - name: Enabled Forward sysctl: name: net.ipv4.ip_forward value: \u0026#39;1\u0026#39; sysctl_set: yes - name: Enabled tcp_reuse sysctl: name: net.ipv4.tcp_tw_reuse value: \u0026#39;1\u0026#39; sysctl_set: yes - name: Chanage tcp tw_buckets sysctl: name: net.ipv4.tcp_max_tw_buckets value: \u0026#39;5000\u0026#39; sysctl_set: yes - name: Chanage tcp_syncookies sysctl: name: net.ipv4.tcp_syncookies value: \u0026#39;1\u0026#39; sysctl_set: yes - name: Chanage tcp max_syn_backlog sysctl: name: net.ipv4.tcp_max_syn_backlog value: \u0026#39;8192\u0026#39; sysctl_set: yes - name: Chanage tcp Established Maxconn sysctl: name: net.core.somaxconn value: \u0026#39;32768\u0026#39; sysctl_set: yes state: present - name: Chanage tcp_syn_retries sysctl: name: net.ipv4.tcp_syn_retries value: \u0026#39;2\u0026#39; sysctl_set: yes state: present - name: Chanage net.ipv4.tcp_synack_retries sysctl: name: net.ipv4.tcp_synack_retries value: \u0026#39;2\u0026#39; sysctl_set: yes state: presen pam_limits 功能: 管理资源限制 范例\n- name: Change Limit /etc/security/limit.conf pam_limits: domain: \u0026#34;*\u0026#34; limit_type: \u0026#34;{{ item.limit_type }}\u0026#34; limit_item: \u0026#34;{{ item.limit_item }}\u0026#34; value: \u0026#34;{{ item.value }}\u0026#34; loop: - { limit_type: \u0026#39;soft\u0026#39;, limit_item: \u0026#39;nofile\u0026#39;,value: \u0026#39;100000\u0026#39; } - { limit_type: \u0026#39;hard\u0026#39;, limit_item: \u0026#39;nofile\u0026#39;,value: \u0026#39;10000\u0026#39; } apt_repository 模块 功能: 此模块实现apt的仓库配置管理 常见选项\nrepo #仓库信息 state #添加或删除 update_cache #是否apt update,默认yes filename #仓库文件,默认放在/etc/apt/sources.list.d/file.list 范例:\nansible ubuntu-servers -m apt_repository -a \u0026#39;repo=\u0026#34;deb http://archive.canonical.com/ubuntu focal partner\u0026#34; filename=google-chrome\u0026#39; [root@ubuntu2004 ~]#cat /etc/apt/sources.list.d/google-chrome.list deb http://archive.canonical.com/ubuntu focal partner apt_key 模块 功能: 添加和删除apt key 常见选项\nurl #key路径 state #添加或删除 范例: 生成ceph仓库配置\n#先导入key,注意先后顺序 ansible ubuntu-servers -m apt_key -a \u0026#39;url=https://download.ceph.com/keys/release.asc state=present\u0026#39; #再生成apt配置,如果不导入key此步会出错 ansible ubuntu-servers -m apt_repository -a \u0026#39;repo=\u0026#34;deb http://mirror.tuna.tsinghua.edu.cn/ceph/debian-pacific focal main\u0026#34; filename=ansible_ceph\u0026#39; #验证结果 [root@ubuntu2004 ~]#cat /etc/apt/sources.list.d/ansible_ceph.list deb http://mirror.tuna.tsinghua.edu.cn/ceph/debian-pacific focal main 其它模块 ansible 还提供了很多针对各种应用的模块,比如\nnginx_status_info nginx_status_facts mysql_db #需要安装MySQL-python包 mysql_user #需要安装MySQL-python包 redis mongodb* postgresql* haproxy git ","permalink":"http://hugo.itshare.work/posts/ansible/ansible/","summary":"Ansible介绍和架构 Ansible发展史 Ansible 的名称来自科幻小说《安德的游戏》中跨越时空的即时通信工具，使用它可以在相距数光年的距离，远程实时控制前线的舰队战斗 2012-03-09，发布0.0.1版，2015-10-17，Red Hat宣布1.5亿美元收购 官网：https://ww","title":"运维自动化工具Ansible(一)"},{"content":" 系统 Centos7.9\n步骤\n1.备份\n[root@centos7-master ~]# mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak 2.创建/etc/yum.repos.d/CentOS-Base.repo文件并复制如下内容\n[base] name=CentOS-$releasever - Base baseurl=http://mirrors.163.com/centos/$releasever/os/$basearch/ http://mirrors.aliyun.com/centos/$releasever/os/$basearch/ http://mirrors.cloud.tencent.com/centos/$releasever/os/$basearch/ http://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/os/$basearch/ http://mirrors.huaweicloud.com/centos/$releasever/os/$basearch/ http://mirror.centos.org/centos/$releasever/os/$basearch/\tgpgcheck=0 #released updates [updates] name=CentOS-$releasever - Updates baseurl=http://mirrors.163.com/centos/$releasever/updates/$basearch/ http://mirrors.aliyun.com/centos/$releasever/updates/$basearch/ http://mirrors.cloud.tencent.com/centos/$releasever/updates/$basearch/ http://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/updates/$basearch/ http://mirrors.huaweicloud.com/centos/$releasever/updates/$basearch/ http://mirror.centos.org/centos/$releasever/updates/$basearch/\tgpgcheck=0 #additional packages that may be useful [extras] name=CentOS-$releasever - Extras baseurl=http://mirrors.163.com/centos/$releasever/extras/$basearch/ http://mirrors.aliyun.com/centos/$releasever/extras/$basearch/ http://mirrors.cloud.tencent.com/centos/$releasever/extras/$basearch/ http://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/extras/$basearch/ http://mirrors.huaweicloud.com/centos/$releasever/extras/$basearch/ http://mirror.centos.org/centos/$releasever/extras/$basearch/ gpgcheck=0 #additional packages that extend functionality of existing packages [centosplus] name=CentOS-$releasever - Plus baseurl=http://mirrors.163.com/centos/$releasever/centosplus/$basearch/ http://mirrors.aliyun.com/centos/$releasever/centosplus/$basearch/ http://mirrors.cloud.tencent.com/centos/$releasever/centosplus/$basearch/ http://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/centosplus/$basearch/ http://mirrors.huaweicloud.com/centos/$releasever/centosplus/$basearch/ http://mirror.centos.org/centos/$releasever/centosplus/$basearch/ gpgcheck=0 enabled=1 3.清除缓存\nyum clean all 4.重新生成缓存\nyum makecache ","permalink":"http://hugo.itshare.work/posts/linux/centos%E7%B3%BB%E7%BB%9Fyum%E9%85%8D%E7%BD%AE/","summary":"系统 Centos7.9 步骤 1.备份 [root@centos7-master ~]# mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak 2.创建/etc/yum.repos.d/CentOS-Base.repo文件并复制如下内容 [base] name=CentOS-$releasever - Base baseurl=http://mirrors.163.com/centos/$releasever/os/$basearch/ http://mirrors.aliyun.com/centos/$releasever/os/$basearch/ http://mirrors.cloud.tencent.com/centos/$releasever/os/$basearch/ http://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/os/$basearch/ http://mirrors.huaweicloud.com/centos/$releasever/os/$basearch/ http://mirror.centos.org/centos/$releasever/os/$basearch/ gpgcheck=0 #released updates [updates] name=CentOS-$releasever - Updates baseurl=http://mirrors.163.com/centos/$releasever/updates/$basearch/ http://mirrors.aliyun.com/centos/$releasever/updates/$basearch/ http://mirrors.cloud.tencent.com/centos/$releasever/updates/$basearch/ http://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/updates/$basearch/ http://mirrors.huaweicloud.com/centos/$releasever/updates/$basearch/ http://mirror.centos.org/centos/$releasever/updates/$basearch/ gpgcheck=0 #additional packages that may be useful [extras] name=CentOS-$releasever - Extras baseurl=http://mirrors.163.com/centos/$releasever/extras/$basearch/ http://mirrors.aliyun.com/centos/$releasever/extras/$basearch/ http://mirrors.cloud.tencent.com/centos/$releasever/extras/$basearch/ http://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/extras/$basearch/ http://mirrors.huaweicloud.com/centos/$releasever/extras/$basearch/ http://mirror.centos.org/centos/$releasever/extras/$basearch/ gpgcheck=0 #additional packages that extend functionality of existing packages [centosplus] name=CentOS-$releasever - Plus baseurl=http://mirrors.163.com/centos/$releasever/centosplus/$basearch/ http://mirrors.aliyun.com/centos/$releasever/centosplus/$basearch/ http://mirrors.cloud.tencent.com/centos/$releasever/centosplus/$basearch/ http://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/centosplus/$basearch/ http://mirrors.huaweicloud.com/centos/$releasever/centosplus/$basearch/ http://mirror.centos.org/centos/$releasever/centosplus/$basearch/ gpgcheck=0 enabled=1 3.清除缓存 yum clean all 4.重新生成缓存 yum makecache","title":"Centos系统yum源配置"},{"content":"名字解析介绍和DNS DNS服务工作原理 DNS查询类型 递归查询： 是指DNS服务器在收到用户发起的请求时，必须向用户返回一个准确的查询结果。如果DNS服务器 本地没有存储与之对应的信息，则该服务器需要询问其他服务器，并将返回的查询结构提交给用 户。 一般客户机和本地DNS服务器之间属于递归查询，即当客户机向DNS服务器发出请求后,若DNS服 务器本身不能解析，则会向另外的DNS服务器发出查询请求，得到最终的肯定或否定的结果后转交 给客户机。此查询的源和目标保持不变,为了查询结果只需要发起一次查询 递归算法:客户端向LocalDNS发起域名查询\u0026ndash;\u0026gt;localDNS不知道域名对应的IP\u0026ndash;\u0026gt;但它知道谁知道-\u0026gt;他 代为帮客户端去查找\u0026ndash;\u0026gt;最后再返回最终结果 迭代查询： 是指DNS服务器在收到用户发起的请求时，并不直接回复查询结果，而是告诉另一台DNS服务器的 地址，用户再向这台DNS服务器提交请求，这样依次反复，直到返回查询结果。 一般情况下(有例外)本地的DNS服务器向其它DNS服务器的查询属于迭代查询,如：若对方不能返回 权威的结果，则它会向下一个DNS服务器(参考前一个DNS服务器返回的结果)再次发起进行查询， 直到返回查询的结果为止。此查询的源不变,但查询的目标不断变化,为查询结果一般需要发起多次 查询 迭代算法︰\n客户端向LocalDNS发起域名查询\u0026ndash;\u0026gt;localDNS不知道域名对应的IP\u0026ndash;\u0026gt;但它知道谁知道并 推荐客户端应该找谁\u0026ndash;\u0026gt;客户端自己去找它 DNS缓存: DNS缓存是将解析数据存储在靠近发起请求的客户端的位置，也可以说DNS数据是可以缓存在任意 位置，最终目的是以此减少递归查询过程，可以更快的让用户获得请求结果。 解析类型 FQDN \u0026ndash;\u0026gt; IP 正向解析 IP \u0026ndash;\u0026gt; FQDN 反向解析\n注意：正反向解析是两个不同的名称空间，是两棵不同的解析树 完整查询流程 Client --\u0026gt;hosts文件 --\u0026gt; Client DNS Service Local Cache --\u0026gt; DNS Server (recursion递 归) --\u0026gt; DNS Server Cache --\u0026gt;DNS iteration(迭代) --\u0026gt; 根--\u0026gt; 顶级域名DNS--\u0026gt;二级域名DNS… DNS服务相关概念和技术 各种资源记录 区域解析库：由众多资源记录RR(Resource Record)组成 记录类型：A, AAAA, PTR, SOA, NS, CNAME, MX\nSOA：Start Of Authority，起始授权记录；一个区域解析库有且仅能有一个SOA记录，必须位于解析库的第一条记录 A：internet Address，作用，FQDN \u0026ndash;\u0026gt; IP AAAA：FQDN \u0026ndash;\u0026gt; IPv6 PTR：PoinTeR，IP \u0026ndash;\u0026gt; FQDN NS：Name Server，专用于标明当前区域的DNS服务器 CNAME ： Canonical Name，别名记录 MX：Mail eXchanger，邮件交换器 TXT：对域名进行标识和说明的一种方式，一般做验证记录时会使用此项，如：SPF（反垃圾邮 件）记录，https验证等，如下示例： _dnsauth TXT 2012011200000051qgs69bwoh4h6nht4n1h0lr038x SOA记录 name: 当前区域的名字，例如\u0026quot;magedu.org.\u0026quot; value: 有多部分组成 注意：\n当前区域的主DNS服务器的FQDN，也可以使用当前区域的名字，只是注释功能，可以不需要配置 对应的NS记录和A记录 当前区域管理员的邮箱地址；但地址中不能使用@符号，一般用.替换，例如：admin.magedu.org 主从服务区域传输相关定义以及否定的答案的统一的TTL 范例\nmagedu.org. 86400 IN SOA ns.magedu.org. nsadmin.magedu.org. ( 2015042201 ;序列号 2H ;刷新时间 10M ;重试时间 1W ;过期时间 1D ;否定答案的TTL值 ) NS记录 name: 当前区域的名字 value: 当前区域的某DNS服务器的名字，例如: ns.magedu.org. 注意：\n相邻的两个资源记录的name相同时，后续的可省略 对NS记录而言，任何一个ns记录后面的服务器名字，都应该在后续有一个A记录 一个区域可以有多个NS记录 范例： magedu.org. IN NS ns1.magedu.org. magedu.org. IN NS ns2.magedu.org. MX记录 name: 当前区域的名字\nvalue: 当前区域的某邮件服务器(smtp服务器)的主机名\n注意：\n一个区域内，MX记录可有多个；但每个记录的value之前应该有一个数字(0-99)，表示此服务器的优先级；数字越小优先级越高 对MX记录而言，任何一个MX记录后面的服务器名字，都应该在后续有一个A记录\n范例： magedu.org. IN MX 10 mx1.magedu.org. IN MX 20 mx2.magedu.org. mx1 A 10.0.0.100 mx2 A 10.0.0.200 A记录 name: 某主机的FQDN，例如：www.magedu.org.\nvalue: 主机名对应主机的IP地址\n避免用户写错名称时给错误答案，可通过泛域名解析进行解析至某特定地址\n范例：\nwww.magedu.org. IN A 1.1.1.1 www.magedu.org. IN A 2.2.2.2 mx1.magedu.org. IN A 3.3.3.3 mx2.magedu.org. IN A 4.4.4.4 $GENERATE 1-254 HOST$ IN A 1.2.3.$ *.magedu.org. IN A 5.5.5.5 magedu.org. IN A 6.6.6.6 #注意：如果有和DNS的IP相同的多个同名的A记录，优先返回DNS的本机IP AAAA记录 name: FQDN value: IPv6 PTR记录 name: IP，有特定格式，把IP地址反过来写，1.2.3.4，要写作4.3.2.1；而有特定后缀：in- addr.arpa.，所以完整写法为：4.3.2.1.in-addr.arpa. value: FQDN 注意：网络地址及后缀可省略；主机地址依然需要反着写 例如：\n4.3.2.1.in-addr.arpa. IN PTR www.magedu.org. #如1.2.3为网络地址，可简写成： 4 IN PTR www.magedu.org. CNAME别名记录 name: 别名的FQDN value: 真正名字的FQDN 例如\nwww.magedu.org. IN CNAME websrv.magedu.org. DNS软件bind DNS服务器软件：bind，powerdns，dnsmasq，unbound，coredns\nbind相关程序包 yum list all bind*\nbind：服务器 bind-utils: 客户端 bind-libs：相关库,依赖关系自动安装 bind-chroot: 安全包，将dns相关文件放至 /var/named/chroot/\n范例：安装bind软件\n[root@centos8 ~]#dnf -y install bind bind-utils [root@ubuntu2004 ~]#apt -y install bind9 bind9-utils bind包相关文件 BIND主程序：/usr/sbin/named 服务脚本和Unit名称：/etc/rc.d/init.d/named，/usr/lib/systemd/system/named.service 主配置文件：/etc/named.conf, /etc/named.rfc1912.zones, /etc/rndc.key 管理工具：/usr/sbin/rndc：remote name domain controller，默认与bind安装在同一主机，且 只能通过127.0.0.1连接named进程，提供辅助性的管理功能；953/tcp 解析库文件：/var/named/ZONE_NAME.ZONE 注意： (1) 一台物理服务器可同时为多个区域提供解析 (2) 必须要有根区域文件；named.ca (3) 应该有两个（如果包括ipv6的，应该更多）实现localhost和本地回环地址的解析库\n主配置文件 全局配置：options {}; 日志子系统配置：logging {}; 区域定义：本机能够为哪些zone进行解析，就要定义哪些zone zone \u0026ldquo;ZONE_NAME\u0026rdquo; IN {}; 注意： 任何服务程序如果期望其能够通过网络被其它主机访问，至少应该监听在一个能与外部主机通信的 IP地址上 缓存名称服务器的配置：监听外部地址即可 dnssec: 建议关闭dnssec，设为no ","permalink":"http://hugo.itshare.work/posts/linux/dns/","summary":"名字解析介绍和DNS DNS服务工作原理 DNS查询类型 递归查询： 是指DNS服务器在收到用户发起的请求时，必须向用户返回一个准确的查询结果。如果DNS服务器 本地没有存储与之对应的信息，则该服务器需要询问其他服务器，并将返回的查询结构提交给用 户。 一般客户机和本地DNS服务器之间属于递归","title":"DNS服务"},{"content":"磁盘管理与文件系统 前言 磁盘是计算机主要的存储介质，可以存储大量的二进制数据，并且断电后也能保持数据不丢失，使用磁盘存储数据的时候我们可以将磁盘划分成我们所需要的格式来进行使用\n1. 磁盘结构 1、硬盘的物理结构 盘片：硬盘有多个盘片，每个盘片有2面 磁头：每面有一个磁头\n2.硬盘数据结构 扇区：磁盘上的每个磁道被等分为若干个弧段，这些弧段便是硬盘的扇区。硬盘的第一个扇区，叫做引导扇区 ，盘片被分为多个扇形区域，每个扇区存放512字节的数据，是硬盘最小的存储单元 磁道：当磁盘旋转时，磁头若保持在一个位置上，则每个磁头都会在磁盘表面划出一个圆形轨迹，这些圆形轨迹就叫做磁道 柱面：在有多个盘片构成的盘组中，由不同盘片的面，但处于同一半径圆的多个磁道组成的一个圆柱面\n3、磁盘结构 硬盘存储容量 = 磁头数 x 磁道（柱面）数 x 每道扇区数 x 每扇区字节数（512字节） 可以用柱面/磁头/扇区来唯一定位磁盘上的每一个区域 磁盘的接口类型：IDE、SATA、SCSI、SAS、光纤通道 用 fdisk -l 查看分区信息\n2. 管理存储 2.1 磁盘分区 2.1.1 为什么分区 优化I/O性能 实现磁盘空间配额限制 提高修复速度 隔离系统和程序 安装多个OS 采用不同文件系统 2.1.2 分区方式 两种分区方式：MBR，GPT\nMBR分区\nMBR：Master Boot Record，1982年，使用32位表示扇区数，分区不超过2T 划分分区的单位： CentOS 5 之前按整柱面划分 CentOS 6 版本后可以按Sector划分 0磁道0扇区：512bytes 446bytes: boot loader 启动相关 64bytes：分区表，其中每16bytes标识一个分区 2bytes: 55AA，标识位 MBR分区中一块硬盘最多有4个主分区，也可以3主分区+1扩展(N个逻辑分区) MBR分区：主和扩展分区对应的1\u0026ndash;4，/dev/sda3，逻辑分区从5开始，/dev/sda5\n问题：利用分区策略相同的另一台主机的分区表来还原和恢复当前主机破环的分区表？\nGPT分区 GPT：GUID（Globals Unique Identifiers） partition table 支持128个分区，使用64位，支持8Z（ 512Byte/block ）64Z （ 4096Byte/block） 使用128位UUID(Universally Unique Identifier) 表示磁盘和分区 GPT分区表自动备份在头和尾两份， 并有CRC校验位 UEFI (Unified Extensible Firmware Interface 统一可扩展固件接口)硬件支持GPT，使得操作系统可以 启动\nGPT分区结构分为4个区域：\nGPT头 分区表 GPT分区 备份区域 2.2 管理分区 列出块设备\nlsblk 创建分区命令\nfdisk 管理MBR分区 gdisk 管理GPT分区 parted 高级分区操作，可以是交互或非交互方式 重新设置内存中的内核分区表版本，适合于除了CentOS 6 以外的其它版本 5，7，8\npartprobe 2.2.1 添加并检测新硬盘 1、添加新硬盘使用lsblk命令显示出块设备\nroot@ubuntu200404:~# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7:0 0 61.9M 1 loop /snap/core20/1328 loop1 7:1 0 67.2M 1 loop /snap/lxd/21835 loop2 7:2 0 62M 1 loop /snap/core20/1587 loop3 7:3 0 43.6M 1 loop /snap/snapd/14978 loop4 7:4 0 47M 1 loop /snap/snapd/16292 loop5 7:5 0 67.8M 1 loop /snap/lxd/22753 sda 8:0 0 20G 0 disk ├─sda1 8:1 0 1M 0 part ├─sda2 8:2 0 1.5G 0 part /boot └─sda3 8:3 0 18.5G 0 part └─ubuntu--vg-ubuntu--lv 253:0 0 10G 0 lvm / sr0 11:0 1 1.2G 0 rom 发现并没有检测出来新添加的硬盘\n2、检测新硬盘\n方法1：可以重启电脑\n方法2： 重新扫描存储设备的scsi总线\n# host后面的数字不是固定的，以实际为准 root@ubuntu200404:~# echo \u0026#39;- - -\u0026#39; \u0026gt; /sys/class/scsi_host/host32/scan 再次使用lsblk命令查看发现已经多了sda的硬盘，说明成功了\nroot@ubuntu200404:~# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT loop0 7:0 0 61.9M 1 loop /snap/core20/1328 loop1 7:1 0 67.2M 1 loop /snap/lxd/21835 loop2 7:2 0 62M 1 loop /snap/core20/1587 loop3 7:3 0 43.6M 1 loop /snap/snapd/14978 loop4 7:4 0 47M 1 loop /snap/snapd/16292 loop5 7:5 0 67.8M 1 loop /snap/lxd/22753 sda 8:0 0 20G 0 disk ├─sda1 8:1 0 1M 0 part ├─sda2 8:2 0 1.5G 0 part /boot └─sda3 8:3 0 18.5G 0 part └─ubuntu--vg-ubuntu--lv 253:0 0 10G 0 lvm / sdb 8:16 0 20G 0 disk\t# 新添加的硬盘 sr0 11:0 1 1.2G 0 rom 2.2.2 partend命令 注意：parted的操作都是实时生效的，小心使用\n格式:\nparted [选项]... [设备 [命令 [参数]...]...] 范例：\nparted /dev/sdb mklabel gpt|msdos parted /dev/sdb print parted /dev/sdb mkpart primary 1 200 （默认M） parted /dev/sdb rm 1 parted -l 列出所有硬盘分区信息 2.2.3 分区工具fdisk和gdisk fdisk -l [-u] [device...] 查看分区 fdisk [device...] 管理MBR分区 gdisk [device...] 类fdisk 的GPT分区工具 # 范例： fdisk /dev/sdb 子命令：\np 分区列表 t 更改分区类型 n 创建新分区 d 删除分区 v 校验分区 u 转换单位 w 保存并退出 q 不保存并退出 查看内核是否已经识别新的分区\ncat /proc/partitions CentOS 7,8 同步分区表:\npartprobe 2.3 文件系统 2.3.1 文件系统概念 文件系统是操作系统用于明确存储设备或分区上的文件的方法和数据结构；即在存储设备上组织文件的 方法。操作系统中负责管理和存储文件信息的软件结构称为文件管理系统，简称文件系统 从系统角度来看，文件系统是对文件存储设备的空间进行组织和分配，负责文件存储并对存入的文件进 行保护和检索的系统。具体地说，它负责为用户建立文件，存入、读出、修改、转储文件，控制文件的 存取，安全控制，日志，压缩，加密等 支持的文件系统：\n/lib/modules/`uname -r`/kernel/fs (各种文件系统)[https://en.wikipedia.org/wiki/Comparison_of_file_systems]\n**帮助：**man 5 fs\n2.3.2 文件系统类型 Linux常用文件系统\next2：Extended file system 适用于那些分区容量不是太大，更新也不频繁的情况，例如 /boot 分 区 ext3：是 ext2 的改进版本，其支持日志功能，能够帮助系统从非正常关机导致的异常中恢复 ext4：是 ext 文件系统的最新版。提供了很多新的特性，包括纳秒级时间戳、创建和使用巨型文件 (16TB)、最大1EB的文件系统，以及速度的提升 xfs：SGI，支持最大8EB的文件系统 swap iso9660 光盘 btrfs（Oracle） reiserfs Windows 常用文件系统\nFAT32 NTFS exFAT Unix： FFS（fast） UFS（unix） JFS2 网络文件系统：\nNFS CIFS 集群文件系统：\nGFS2 OCFS2（oracle） 分布式文件系统：\nfastdfs ceph moosefs mogilefs glusterfs Lustre RAW：\n裸文件系统,未经处理或者未经格式化产生的文件系统 常用的文件系统特性：\nFAT32\n最多只能支持16TB的文件系统和4GB的文件 NTFS\n最多只能支持16EB的文件系统和16EB的文件 EXT3\n最多只能支持32TB的文件系统和2TB的文件，实际只能容纳2TB的文件系统和16GB的文件 Ext3目前只支持32000个子目录 Ext3文件系统使用32位空间记录块数量和 inode数量 当数据写入到Ext3文件系统中时，Ext3的数据块分配器每次只能分配一个4KB的块 EXT4：\nEXT4是Linux系统下的日志文件系统，是EXT3文件系统的后继版本 Ext4的文件系统容量达到1EB，而支持单个文件则达到16TB 理论上支持无限数量的子目录 Ext4文件系统使用64位空间记录块数量和 inode数量 Ext4的多块分配器支持一次调用分配多个数据块 修复速度更快 XFS\n根据所记录的日志在很短的时间内迅速恢复磁盘文件内容 用优化算法，日志记录对整体文件操作影响非常小 是一个全64-bit的文件系统，最大可以支持8EB的文件系统，而支持单个文件则达到8EB 能以接近裸设备I/O的性能存储数据 查前支持的文件系统：\ncat /proc/filesystems 2.3.3 文件系统的组成部分 内核中的模块：ext4, xfs, vfat Linux的虚拟文件系统：VFS 用户空间的管理工具：mkfs.ext4, mkfs.xfs,mkfs.vfat\n2.3.4 文件系统选择管理 2.3.4.1 创建文件系统 创建文件管理工具\nmkfs命令： (1) mkfs.FS_TYPE /dev/DEVICE ext4 xfs btrfs vfat (2) mkfs -t FS_TYPE /dev/DEVICE -L \u0026#39;LABEL\u0026#39; 设定卷标 mke2fs：ext系列文件系统专用管理工具 常用选项：\n-t {ext2|ext3|ext4|xfs} 指定文件系统类型 -b {1024|2048|4096} 指定块 block 大小 -L ‘LABEL’ 设置卷标 -j 相当于 -t ext3， mkfs.ext3 = mkfs -t ext3 = mke2fs -j = mke2fs -t ext3 -i # 为数据空间中每多少个字节创建一个inode；不应该小于block大 小 -N # 指定分区中创建多少个inode -I 一个inode记录占用的磁盘空间大小，128---4096 -m # 默认5%,为管理人员预留空间占总空间的百分比 -O FEATURE[,...] 启用指定特性 -O ^FEATURE 案例：mkfs.ext4 /dev/sdb1\nroot@ubuntu200404:~# mkfs.ext4 /dev/sdb1 mke2fs 1.45.5 (07-Jan-2020) Creating filesystem with 2621440 4k blocks and 655360 inodes Filesystem UUID: a7ef4142-26e5-43dd-b9d0-24c4d09155a1 Superblock backups stored on blocks: 32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632 Allocating group tables: done Writing inode tables: done Creating journal (16384 blocks): done Writing superblocks and filesystem accounting information: done root@ubuntu200404:~# 2.3.4.2 查看和管理分区信息 blkid 可以查看块设备属性信息 格式：\nblkid [OPTION]... [DEVICE] 常用选项：\n-U UUID 根据指定的UUID来查找对应的设备 -L LABEL 根据指定的LABEL来查找对应的设备 e2label：管理ext系列文件系统的LABEL e2label DEVICE [LABEL] 范例\nroot@ubuntu200404:~# blkid /dev/sdb1 /dev/sdb1: UUID=\u0026#34;a7ef4142-26e5-43dd-b9d0-24c4d09155a1\u0026#34; TYPE=\u0026#34;ext4\u0026#34; PARTUUID=\u0026#34;db60ac71-01\u0026#34; root@ubuntu200404:~# 查找分区\nfindfs [options] LABEL=\u0026lt;label\u0026gt; findfs [options] UUID=\u0026lt;uuid\u0026gt; tune2fs：重新设定ext系列文件系统可调整参数的值\n-l 查看指定文件系统超级块信息；super block -L \u0026#39;LABEL’ 修改卷标 -m # 修预留给管理员的空间百分比 -j 将ext2升级为ext3 -O 文件系统属性启用或禁用, -O ^has_journal -o 调整文件系统的默认挂载选项，-o ^acl -U UUID 修改UUID号 dumpe2fs：显示ext文件系统信息，将磁盘块分组管理 -h：查看超级块信息，不显示分组信息\n范例：查看ext文件系统的元数据和块组信息\nroot@ubuntu200404:~# dumpe2fs /dev/sdb1 dumpe2fs 1.45.5 (07-Jan-2020) Filesystem volume name: \u0026lt;none\u0026gt; Last mounted on: \u0026lt;not available\u0026gt; Filesystem UUID: a7ef4142-26e5-43dd-b9d0-24c4d09155a1 Filesystem magic number: 0xEF53 Filesystem revision #: 1 (dynamic) Filesystem features: has_journal ext_attr resize_inode dir_index filetype extent 64bit flex_bg sparse_super large_file huge_file dir_nlink extra_isize metadata_csum Filesystem flags: signed_directory_hash Default mount options: user_xattr acl Filesystem state: clean Errors behavior: Continue Filesystem OS type: Linux Inode count: 655360 Block count: 2621440 Reserved block count: 131072 Free blocks: 2554687 Free inodes: 655349 First block: 0 ...... ...... ...... xfs_info：显示示挂载或已挂载的 xfs 文件系统信息\nxfs_info mountpoint|devname 范例\nxfs_info /dev/sda1 2.3.4.3 文件系统检测和修复 文件系统夹故障常发生于死机或者非正常关机之后，挂载为文件系统标记为“no clean” 注意：一定不要在挂载状态下执行下面命令修复\nfsck: File System Check\nfsck.FS_TYPE fsck -t FS_TYPE 注意：FS_TYPE 一定要与分区上已经文件类型相同\n常用选项\n-a 自动修复 -r 交互式修复错误 e2fsck：ext系列文件专用的检测修复工具\n-y 自动回答为yes -f 强制修复 -p 自动进行安全的修复文件系统问题 用法：\ne2fsck /dev/sdb2 xfs_repair：xfs文件系统专用检测修复工具 常用选项：\n-f 修复文件，而设备 -n 只检查 -d 允许修复只读的挂载设备，在单用户下修复 / 时使用，然后立即reboot 用法：\nxfs_repair /dev/sda1 2.4 挂载 挂载:将额外文件系统与根文件系统某现存的目录建立起关联关系，进而使得此目录做为其它文件访问入 口的行为 卸载:为解除此关联关系的过程 把设备关联挂载点：mount Point 挂载点下原有文件在挂载完成后会被临时隐藏，因此，挂载点目录一般为空 进程正在使用中的设备无法被卸载\n2.4.1 挂载文件系统 mount 格式\nmount [-fnrsvw] [-t vfstype] [-o options] device mountpoint device：指明要挂载的设备\n设备文件：例如:/dev/sda5 卷标：-L \u0026lsquo;LABEL\u0026rsquo;, 例如 -L \u0026lsquo;MYDATA\u0026rsquo; UUID： -U \u0026lsquo;UUID\u0026rsquo;：例如 -U \u0026lsquo;0c50523c-43f1-45e7-85c0-a126711d406e\u0026rsquo; 伪文件系统名称：proc, sysfs, devtmpfs, configfs mountpoint：挂载点目录必须事先存在，建议使用空目录 mount 常用命令选项\n-t fstype 指定要挂载的设备上的文件系统类型,如:ext4,xfs -r readonly，只读挂载 -w read and write, 读写挂载,此为默认设置,可省略 -n 不更新/etc/mtab，mount不可见 -a 自动挂载所有支持自动挂载的设备(定义在了/etc/fstab文件中，且挂载选项中有 auto功能) -L \u0026#39;LABEL\u0026#39; 以卷标指定挂载设备 -U \u0026#39;UUID\u0026#39; 以UUID指定要挂载的设备 -B, --bind 绑定目录到另一个目录上 -o options：(挂载文件系统的选项)，多个选项使用逗号分隔 async 异步模式,内存更改时,写入缓存区buffer,过一段时间再写到磁盘中，效率高，但不安全 sync 同步模式,内存更改时，同时写磁盘，安全，但效率低下 atime/noatime 包含目录和文件 diratime/nodiratime 目录的访问时间戳 auto/noauto 是否支持开机自动挂载，是否支持-a选项 exec/noexec 是否支持将文件系统上运行应用程序 dev/nodev 是否支持在此文件系统上使用设备文件 suid/nosuid 是否支持suid和sgid权限 remount 重新挂载 ro/rw 只读、读写 user/nouser 是否允许普通用户挂载此设备，/etc/fstab使用 acl/noacl 启用此文件系统上的acl功能 loop 使用loop设备 _netdev 当网络可用时才对网络资源进行挂载，如：NFS文件系统 defaults 相当于rw, suid, dev, exec, auto, nouser, async 挂载规则:\n一个挂载点同一时间只能挂载一个设备 一个挂载点同一时间挂载了多个设备，只能看到最后一个设备的数据，其它设备上的数据将被隐藏 一个设备可以同时挂载到多个挂载点 通常挂载点一般是已存在空的目录 范例:挂载案例\nroot@ubuntu200404:/data# mount /dev/sdb1 /data/mysql_mount/ root@ubuntu200404:/data# df ###　2.4.2 卸载文件系统 umount 卸载时：可使用设备，也可以使用挂载点\numount 设备名|挂载点 2.4.3 查看挂载情况 查看挂载\n#通过查看/etc/mtab文件显示当前已挂载的所有设备 mount #查看内核追踪到的已挂载的所有设备 cat /proc/mounts 查看挂载点情况\nfindmnt MOUNT_POINT|device 查看正在访问指定文件系统的进程\nlsof MOUNT_POINT fuser -v MOUNT_POINT 终止所有在正访问指定的文件系统的进程\nfuser -km MOUNT_POINT 2.4.4 持久挂载 将挂载保存到 /etc/fstab 中可以下次开机时，自动启用挂载 /etc/fstab格式帮助：\nman 5 fstab 每行定义一个要挂载的文件系统,，其中包括共 6 项\n要挂载的设备或伪文件系统设备文件 LABEL：LABEL=\u0026quot;\u0026quot; UUID：UUID=\u0026quot;\u0026quot; 伪文件系统名称：proc, sysfs 挂载点：必须是事先存在的目录 文件系统类型：ext4，xfs，iso9660，nfs，none 挂载选项：defaults ，acl，bind 转储频率：0：不做备份 1：每天转储 2：每隔一天转储 fsck检查的文件系统的顺序：允许的数字是0 1 2 0：不自检 ，1：首先自检；一般只有rootfs才用 2：非rootfs使用 添加新的挂载项，需要执行下面命令生效\nmount -a 范例：centos7, 8 /etc/fstab 的分区UUID错误，无法启动*\n自动进入emergency mode,输入root 密码 #cat /proc/mounts 可以查看到/ 以rw方式挂载 #vim /etc/fstab #reboot 范例：centos 6 /etc/fstab 的分区UUID错误，无法启动\n如果/etc/fstab 的挂载设备出错，比如文件系统故障，并且文件系统检测项（即第6项为非0），将导致无 法启动 自动进入emergency mode,输入root 密码 #cat /proc/mounts 可以查看到/ 以ro方式挂载，无法直接修改配置文件 #mount -o remount,rw / #vim /etc/fstab 将故障行的最后1项，即第6项修改为0，开机不检测此项挂载设备的健康性，从而忽略错误，能实现启动 范例：/etc/fstab格式\nroot@ubuntu200404:/data# cat /etc/fstab # /etc/fstab: static file system information. # # Use \u0026#39;blkid\u0026#39; to print the universally unique identifier for a # device; this may be used with UUID= as a more robust way to name devices # that works even if disks are added and removed. See fstab(5). # # \u0026lt;file system\u0026gt; \u0026lt;mount point\u0026gt; \u0026lt;type\u0026gt; \u0026lt;options\u0026gt; \u0026lt;dump\u0026gt; \u0026lt;pass\u0026gt; # / was on /dev/ubuntu-vg/ubuntu-lv during curtin installation /dev/disk/by-id/dm-uuid-LVM-3aQ0WgB04ZXwNPYVAYy9ssb3Wd06E34ggUUxCcYQaVwAb0L03K40wpOxbnqqqa3f / ext4 defaults 0 1 # /boot was on /dev/sda2 during curtin installation /dev/disk/by-uuid/5e8f9763-2db8-48d0-85e2-a26d76521e2f /boot ext4 defaults 0 1 /swap.img\tnone\tswap\tsw\t0\t0 root@ubuntu200404:/data# 范例：添加新的挂载点后修改/etc/fstab文件\n# /etc/fstab: static file system information. # # Use \u0026#39;blkid\u0026#39; to print the universally unique identifier for a # device; this may be used with UUID= as a more robust way to name devices # that works even if disks are added and removed. See fstab(5). # # \u0026lt;file system\u0026gt; \u0026lt;mount point\u0026gt; \u0026lt;type\u0026gt; \u0026lt;options\u0026gt; \u0026lt;dump\u0026gt; \u0026lt;pass\u0026gt; # / was on /dev/ubuntu-vg/ubuntu-lv during curtin installation /dev/disk/by-id/dm-uuid-LVM-3aQ0WgB04ZXwNPYVAYy9ssb3Wd06E34ggUUxCcYQaVwAb0L03K40wpOxbnqqqa3f / ext4 defaults 0 1 # /boot was on /dev/sda2 during curtin installation /dev/disk/by-uuid/5e8f9763-2db8-48d0-85e2-a26d76521e2f /boot ext4 defaults 0 1 /swap.img none swap sw 0 0 # 添加该行后、重启系统 UUID=0e850a4a-028d-48b2-aa18-dd8b16090aa6 /data/mysql_mount ext4 defaults 0 0 2.5 处理交换文件和分区 2.5.1 swap分区 swap交换分区是系统RAM的补充，swap 分区支持虚拟内存。当没有足够的 RAM 保存系统处理的数据 时会将数据写入 swap 分区，当系统缺乏 swap 空间时，内核会因 RAM 内存耗尽而终止进程。配置过 多 swap 空间会造成存储设备处于分配状态但闲置，造成浪费，过多 swap 空间还会掩盖内存泄露 注意：为优化性能，可以将swap 分布存放，或高性能磁盘存放\n2.5.2 交换分区实现过程 创建交换分区或者文件 使用mkswap写入特殊签名 在/etc/fstab文件中添加适当的条目 使用swapon -a 激活交换空间 启用swap分区\nswapon [OPTION]... [DEVICE] 常用选项\n-a #激活所有的交换分区 -p PRIORITY #指定优先级(-1到32767之间)，值越大,优先级越高.也可在/etc/fstab文件中的第4列指 定：pri=value 范例:创建swap分区\n[root@centos8 ~]#mkswap /dev/sdc1 禁用swap分区\nswapoff [OPTION]... [DEVICE] 范例:禁用swap分区\n[root@centos8 ~]#sed -i.bak \u0026#39;/swap/d\u0026#39; /etc/fstab [root@centos8 ~]#swapoff -a 2.5.3 swap的使用策略 /proc/sys/vm/swappiness 的值决定了当内存占用达到一定的百分比时，会启用swap分区的空间 使用规则\n当内存使用率达到100-swappiness时,会启用交换分区 简单地说这个参数定义了系统对swap的使用倾向，此值越大表示越倾向于使用swap。 可以设为0，这样做并不会禁止对swap的使用，只是最大限度地降低了使用swap的可能性 范例\n#说明：CentOS7和8默认值为30，内存在使用到100-30=70%的时候，就开始出现有交换分区的使用。 [root@centos8 ~]# cat /proc/sys/vm/swappiness 2.6 磁盘常见工具 2.6.1 df 文件系统空间实际真正占用等信息的查看工具 df\ndf [OPTION]... [FILE]... 常用选项\n-H 以10为单位 -T 文件系统类型 -h human-readable -i inodes instead of blocks -P 以Posix兼容的格式输出 2.6.3 du 查看某目录总体空间实际占用状态 du\n显示指定目录下面各个子目录的大小,单位为KB\n常用选项\n-a --all 显示所有文件和目录的大小,默认只显示目录大小 -h human-readable -s summary --max-depth=# 指定最大目录层级 -x, --one-file-system #忽略不在同一个文件系统的目录 面试题\n1.df 和 du 区别?什么时候df \u0026gt;du（空分区的时候) df 查看是文件系统的空间使用，包括元数据和数据，删除文件后，如果此文件正在使用，不会立即释放空间;du 查看是文件数据空间使用，不包括元数据，删除文件后空间立即释放。\n2.什么时候df \u0026lt; du? 目录内挂载有其它分区时的情况\n3.当删除文件但不释放空间时,有什么不同? du 查看文件空间释放,df不释放\n3. RAID ","permalink":"http://hugo.itshare.work/posts/linux/disk/","summary":"磁盘管理与文件系统 前言 磁盘是计算机主要的存储介质，可以存储大量的二进制数据，并且断电后也能保持数据不丢失，使用磁盘存储数据的时候我们可以将磁盘划分成我们所需要的格式来进行使用 1. 磁盘结构 1、硬盘的物理结构 盘片：硬盘有多个盘片，每个盘片有2面 磁头：每面有一个磁头 2.硬盘数据结构 扇区：","title":"磁盘存储和文件系统管理"},{"content":"Docker 详细教程 一、Docker简介 1.1 docker是什么 【问题】：问什么会有docker出现\n​\tDocker的出现 使得Docker得以打破过去「程序即应用」的观念。透过镜像(images)将作业系统核心除外，运作应用程式所需要的系统环境，由下而上打包，达到应用程式跨平台间的无缝接轨运作。\n【docker理念】：解决了运行环境和配置问题的软件容器，方便持续继承并有助于整体发布的容器虚拟化技术。\n1.2 容器与虚拟机比较 1.2.1 容器发展简史 ￼￼￼\r1.2.2 传统虚拟机技术 虚拟机（virtual machine）就是带环境安装的一种解决方案。\n它可以在一种操作系统里面运行另一种操作系统，比如在Windows10系统里面运行Linux系统CentOS7。应用程序对此毫无感知，因为虚拟机看上去跟真实系统一模一样，而对于底层系统来说，虚拟机就是一个普通文件，不需要了就删掉，对其他部分毫无影响。这类虚拟机完美的运行了另一套系统，能够使应用程序，操作系统和硬件三者之间的逻辑不变。\nWin10 VMWare Centos7 各种cpu、内存网络额配置+各种软件 虚拟机实例 虚拟机的缺点：\n1 资源占用多 2 冗余步骤多 3 启动慢\n1.2.3 容器虚拟化技术 由于前面虚拟机存在某些缺点，Linux发展出了另一种虚拟化技术：\nLinux容器(Linux Containers，缩写为 LXC)\nLinux容器是与系统其他部分隔离开的一系列进程，从另一个镜像运行，并由该镜像提供支持进程所需的全部文件。容器提供的镜像包含了应用的所有依赖项，因而在从开发到测试再到生产的整个过程中，它都具有可移植性和一致性。\nLinux 容器不是模拟一个完整的操作系统 而是对进程进行隔离。有了容器，就可以将软件运行所需的所有资源打包到一个隔离的容器中。 容器与虚拟机不同，不需要捆绑一整套操作系统 ，只需要软件工作所需的库资源和设置。系统因此而变得高效轻量并保证部署在任何环境中的软件都能始终如一地运行。\n1.2.4 对比 比较了 Docker 和传统虚拟化方式的不同之处：\n传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程； 容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核 且也没有进行硬件虚拟 。因此容器要比传统虚拟机更为轻便。 每个容器之间互相隔离，每个容器有自己的文件系统 ，容器之间进程不会相互影响，能区分计算资源。\n1.3 能干什么 1.3.1 技术职级变化 coder -\u0026gt; programmer -\u0026gt; software engineer -\u0026gt; DevOps engineer\n1.3.2 开发/运维（Devops)新一代开发工程师 一次构建、随处运行 更快速的应用交付和部署 更便捷的升级和扩缩容 更简单的系统运维 更高效的计算资源利用 1.3.3 Docker应用场景 Docker 借鉴了标砖集装箱的概念。标准集装箱将货物运往世界各地，Docker将这个模型运用到自己的设计中，唯一不同的是：集装箱运输货物，而Docker运输软件。\n1.4 那些企业在使用 新浪\n美团\n蘑菇街 1.5 下载地址 官网：http://www.docker.com\nDocker Hub 官网：https://hub.docker.com\n二、Docker安装 2.1 前提说明 2.1.1 CentOS Docker 安装 2.1.2 前提条件 目前，CentOS仅发行版本中的内核支持Docker。Docker运行在CentOS 7（64-bit）上，要求系统为64位，Linux系统内核版本为3.8以上，这里选用Centos7.x\n2.1.3 查看自己的内核 uname 命令用于打印当前系统相关信息（内核版本号，硬件架构，主机名称和操作系统类型等）。\n2.2 Docker的基本组成 2.2.1 镜像（image） Docker 镜像（Image）就是一个 只读 的模板。镜像可以用来创建 Docker 容器， 一个镜像可以创建很多容器 。\n它也相当于是一个root文件系统。比如官方镜像 centos:7 就包含了完整的一套 centos:7 最小系统的 root 文件系统。\n相当于容器的“源代码”， docker镜像文件类似于Java的类模板，而docker容器实例类似于java中new出来的实例对象。\n2.2.2 容器（container） 从面向对象角度 Docker 利用容器（Container）独立运行的一个或一组应用，应用程序或服务运行在容器里面，容器就类似于一个虚拟化的运行环境， 容器是用镜像创建的运行实例 。就像是Java中的类和实例对象一样，镜像是静态的定义，容器是镜像运行时的实体。容器为镜像提供了一个标准的和隔离的运行环境 ，它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台\n从镜像容器角度 可以把容器看做是一个简易版的 *Linux* 环境 （包括root用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。\n2.2.3 仓库（repository） 仓库（Repository）是 集中存放镜像 文件的场所。\n类似于\nMaven仓库，存放各种jar包的地方；\ngithub仓库，存放各种git项目的地方；\nDocker公司提供的官方registry被称为Docker Hub，存放各种镜像模板的地方。\n仓库分为公开仓库（Public）和私有仓库（Private）两种形式。\n最大的公开仓库是 Docker Hub(https://hub.docker.com/) ，\n存放了数量庞大的镜像供用户下载。国内的公开仓库包括阿里云 、网易云等\n2.2.4 小总结 需要正确的理解仓库/镜像/容器这几个概念: Docker 本身是一个容器运行载体或称之为管理引擎。我们把应用程序和配置依赖打包好形成一个可交付的运行环境，这个打包好的运行环境就是image镜像文件。只有通过这个镜像文件才能生成Docker容器实例(类似Java中new出来一个对象)。\nimage文件可以看作是容器的模板。Docker 根据 image 文件生成容器的实例。同一个 image 文件，可以生成多个同时运行的容器实例。\n镜像文件 image 文件生成的容器实例，本身也是一个文件，称为镜像文件。\n容器实例 一个容器运行一种服务，当我们需要的时候，就可以通过docker客户端创建一个对应的运行实例，也就是我们的容器 。\n仓库 就是放一堆镜像的地方，我们可以把镜像发布到仓库中，需要的时候再从仓库中拉下来就可以了。\n2.3 Docker平台架构图解（入门版） 2.3.1 Docker工作原理 Docker是一个Client-Server结构的系统，Docker守护进程运行在主机上， 然后通过Socket连接从客户端访问，守护进程从客户端接受命令并管理运行在主机上的容器 。 容器，是一个运行时环境，就是我们前面说到的集装箱。可以对比mysql演示对比讲解\n2.3.2 整体架构及底层通信原理简述 Docker是一个C/S模式的架构，后端是一个松耦合架构，众多模块各司其职\n2.3.3 Docker运行的基本流程为： 用户是使用Docker Client 与Docker Daemon 建立通信，并发送请求给后者。 Docker Daemon 作为Docker架构中的主体部分，首先提供Docker Server 的功能时期可以接受 Docker Client的请求。 Docker Engine 执行Docker内部的一些列工作，每一项工作都是以一个Job的形式的存在。 Job的运行过程中，当需要容器镜像是，则从Docker Register中下载镜像，并通过镜像管理驱动Graph driver 将下载镜像以Graph的形式存储。 当需要为Docker创建网络环境时，通过网络驱动Network driver创建并配置Docker容器网络环境。 当需要限制Docker容器运行资源或执行用户指令等操作时，则通过Exec driver来完成。 Libcontainer是一项独立的容器管理包，Network driver以及Exec driver都是通过Libcontainer来实现具体容器进行的操作。 2.4、安装步骤 2.4.1 CentOS7安装Docker 确定你是CentOS7以上版本 # 查看CentOS版本命令 cat /etc/redhat-release 卸载旧版本\n# 卸载旧版本docker命令 $ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine\tyum安装gcc相关命令 # yum安装gcc相关命令 yum -y install gcc yum -y install gcc-c++ 安装需要的软件包\n使用存储库安装\n在新主机上首次安装Docker Engine之前，您需要设置Docker存储库。之后，您可以从存储库安装和更新Docker 设置存储库 安装 yum-utils 包（提供yum-config-manager 实用程序）并设置稳定的存储库 # 官网要求 yum install -y yum-utils 设置stable镜像仓库\n# 推荐使用 使用阿里的 docker 镜像仓库，国外的镜像仓库是比较慢的 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 更新yum软件包索引 # 更新yum软件包索引 yum makecache fast 安装DOCKER CE 引擎 # 命令 yum -y install docker-ce docker-ce-cli containerd.io 启动docker # 启动命令 systemctl start docker 测试 # 测试 docker version docker run hello-world 卸载 # 卸载命令 systemctl stop docker yum remove docker-ce docker-ce-cli containerd.io rm -rf /var/lib/docker rm -rf /var/lib/containerd 2.5、阿里云镜像加速 2.5.1 是什么 地址：https://promotion.aliyun.com/ntms/act/kubernetes.html\n注册一个属于自己的阿里云账户\n获得加速器地址连接：\n登陆阿里云开发者平台 点击控制台 选择容器镜像服务 获取加速器地址 粘贴脚本直接执行\nmkdir -p /etc/docker tee /etc/docker/daemon.json \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://aa25jngu.mirror.aliyuncs.com\u0026#34;] } EOF # 或者分开步骤执行 mkdir -p /etc/docker vim /etc/docker/daemon.json 重启服务器 # 重启服务器 systemctl daemon-reload systemctl restart docker 2.5.2 永远的HelloWorld 启动Docker后台容器（测试运行 hello-world）\n# 命令 docker run hello-world 2.5.3 底层原理 为什么Docker会比VM虚拟机快:\n(1)docker有着比虚拟机更少的抽象层 由于docker不需要Hypervisor(虚拟机)实现硬件资源虚拟化,运行在docker容器上的程序直接使用的都是实际物理机的硬件资源。因此在CPU、内存利用率上docker将会在效率上有明显优势。 (2)docker利用的是宿主机的内核,而不需要加载操作系统OS内核 当新建一个容器时,docker不需要和虚拟机一样重新加载一个操作系统内核。进而避免引寻、加载操作系统内核返回等比较费时费资源的过程,当新建一个虚拟机时,虚拟机软件需要加载OS,返回新建过程是分钟级别的。而docker由于直接利用宿主机的操作系统,则省略了返回过程,因此新建一个docker容器只需要几秒钟。 三、Docker常用命令 3.1 帮助启动类命令 # 启动命令 systemctl start docker # 停止命令 systemctl stop docker # 重启命令 systemctl restart docker # 查看docker状态 systemctl status docker # 开机启动 systemctl enable docker # 查看 docker 概要信息 docker info # 查看docker 总体帮助文档 docker --help # 查看docker命令帮助文档： docker 具体命令 --help 3.2 镜像命令 3.2.1 docker images # 列出本地主机上的镜像 docker images 各个选项说明:\nREPOSITORY：表示镜像的仓库源\nTAG：镜像的标签版本号\nIMAGE ID：镜像ID\nCREATED：镜像创建时间\nSIZE：镜像大小\n同一仓库源可以有多个 TAG版本，代表这个仓库源的不同个版本，我们使用 REPOSITORY:TAG 来定义不同的镜像。\n如果你不指定一个镜像的版本标签，例如你只使用 ubuntu，docker 将默认使用 ubuntu:latest 镜像\n3.2.2 OPTIONS 说明 -a : 列出本地所有的镜像（含历史映像层）\n-q：只显示镜像ID\n3.2.3 docker search 某个XXX镜像名字 # 网站 https://hub.docker.com # 命令 docker search [OPTIONS]镜像名字 # OPTIONS说明 # --limit ：只列出N个镜像，默认25个 docker search --limit 5 redis 案例：\n3.2.4 docker pull 某个XXX镜像名字 # 下载镜像 docker pull 镜像名字[:TAG] docker pull 镜像名字 # 没有TAG就是最新版本 等价于 docker pull 镜像名字：latest docker pull ubuntu 3.2.5 docker system df 查看镜像/容器/数据卷所占用的空间 3.2.6 docker rmi 删除镜像 # 删除单个 docker rmi -f 镜像ID # 删除多个 docker rmi -f 镜像名1:TAG 镜像名2:TAG # 删除全部 docker rmi -f $(docker images -qa) 3.2.7 谈谈docker虚悬镜像是什么？ 仓库名称，标签都是\u0026lt;none\u0026gt;的镜像，俗称虚悬镜像dangling image 长什么样子 后续Dockerfile章节在介绍 3.3 容器命令 有镜像才能创建容器，这是根本前提（下载一个CentOS或者ubuntu镜像演示）\n1.说明 2.docker pull centos 3.docker pull ubuntu 4.本次演示用ubuntu演示 3.3.1 新建+启动容器 新建+启动容器 命令 docker run [OPTIONS] IMAGE [COMMAND] [ARG\u0026hellip;]\nOPTIONS说明 OPTIONS说明（常用）：有些是一个减号，有些是两个减号\n\u0026ndash;name=\u0026ldquo;容器新名字\u0026rdquo; 为容器指定一个名称； -d: 后台运行容器并返回容器ID，也即启动守护式容器(后台运行)；\n-i：以交互模式运行容器，通常与 -t 同时使用； -t：为容器重新分配一个伪输入终端，通常与 -i 同时使用； 也即 启动交互式容器(前台有伪终端，等待交互) ；\n-P: 随机 端口映射，大写P -p: 指定 端口映射，小写p\n启动交互式容器（前台命令行）\n使用镜像centos:latest以 交互模式 启动一个容器,在容器内执行/bin/bash命令。\ndocker run -it centos /bin/bash\n参数说明：\n-i: 交互式操作。\n-t: 终端。\ncentos : centos 镜像。\n/bin/bash：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用的是 /bin/bash。 要退出终端，直接输入 exit:\n3.3.2 列出当前所有正在运行的容器 # 列出当前所有正在运行的容器 docker ps [OPTIONS] # OPTIONS说明 -a : 列出当前所有 正在运行 的容器 + 历史上运行过 的 -l :显示最近创建的容器。 -n：显示最近n个创建的容器。 -q :静默模式，只显示容器编号。 3.3.3 退出容器 # 两种退出方式 # 1、run进去容器，exit退出，容器停止 exit # 2、run进去容器，ctrl+p+q退出，容器不停止 ctrl+p+q 3.3.4 启动已停止运行的容器 # 启动已停止运行的容器 docker start 容器ID或者容器名 # 重启容器 docker restart 容器ID或者容器名 # 停止容器 docker stop 容器ID或者容器名 # 强制停止容器 docker kill 容器ID或容器名 # 删除已停止的容器 docker rm 容器ID # 一次性删除多个容器实例 docker rm -rf $(docker ps -a -q) docker ps -a -q | xargs docker rm 3.3.5 重要 启动守护式容器（后台服务器）：\n有镜像才能创建容器，这是根本前提（下载一个Redis6.0.8镜像演示） 在大部分的场景下，我们希望docker的服务是在后台运行的，我们可以通过 -d 指定容器的后台运行模式。 docker run -d 容器名 # 使用镜像centos:latest以后台模式启动一个容器 docker run -d centos 问题：然后docker ps -a 进行查看, 会发现容器已经退出 很重要的要说明的一点: Docker容器后台运行,就必须有一个前台进程. 容器运行的命令如果不是那些 一直挂起的命令 （比如运行top，tail），就是会自动退出的。 这个是docker的机制问题,比如你的web容器,我们以nginx为例，正常情况下, 我们配置启动服务只需要启动响应的service即可。例如service nginx start 但是,这样做,nginx为后台进程模式运行,就导致docker前台没有运行的应用, 这样的容器后台启动后,会立即自杀因为他觉得他没事可做了. 所以，最佳的解决方案是, 将你要运行的程序以前台进程的形式运行， 常见就是命令行模式，表示我还有交互操作，别中断，O(∩_∩)O哈哈~ redis前后台启动演示case\n# 前台交互式启动 docker run -it redis:6.0.8 # 后台交互式启动 docker run -d redis:6.0.8 查看容器日志\n# 查看容器日志 docker logs 容器ID 查看容器内运行的进程\n# 查看容器内运行的进程 docker top 容器ID 查看容器内部细节\n# 查看容器内部细节 docker inspect 容器ID 进入正在运行的容器并以命令行交互\ndocker exec -it 容器ID bashShell 重新进入docker attach 容器ID\n案例演示，用centos或者unbuntu都可以 上述两个区别：\nattach 直接进入容器启动命令的终端，不会启动新的进程用exit退出，会导致容器的停止。 2. exec 是在容器中打开新的终端，并且可以启动新的进程用exit退出，不会导致容器的停止。\n推荐大家使用docker exec 命令，因为退出容器终端，不会导致容器的停止。\n使用之前的redis容器实例进入试试\ndocker exec -it 容器ID /bin/bash docker exec -it 容器ID redis-cli 一般用-d后台启动的程序，在用exec进入对应容器实例 从容器内拷贝文件到主机上\n容器 -\u0026gt; 主机\ndocker cp 容器ID:容器内路径 目的主机路径\n公式： docker cp 容器 ID: 容器内路径 目的主机路径\n导入和导出容器\nExport 导出容器的内容留作为一个tar归档文件[对应import命令]\nimport 从tar 包中的内容创建一个新的文件系统在导入为镜像[对应export]\n【案例】：\ndocker export 容器ID \u0026gt; 文件.tar\ncat 文件名.tar | docker import -镜像用户/镜像名:镜像版本号\n3.4 小总结 attach Attach to a running container # 当前 shell 下 attach 连接指定运行镜像 build Build an image from a Dockerfile # 通过 Dockerfile 定制镜像 commit Create a new image from a container changes # 提交当前容器为新的镜像 cp Copy files/folders from the containers filesystem to the host path #从容器中拷贝指定文件或者目录到宿主机中 create Create a new container # 创建一个新的容器，同 run，但不启动容器 diff Inspect changes on a container\u0026#39;s filesystem # 查看 docker 容器变化 events Get real time events from the server # 从 docker 服务获取容器实时事件 exec Run a command in an existing container # 在已存在的容器上运行命令 export Stream the contents of a container as a tar archive # 导出容器的内容流作为一个 tar 归档文件[对应 import ] history Show the history of an image # 展示一个镜像形成历史 images List images # 列出系统当前镜像 import Create a new filesystem image from the contents of a tarball # 从tar包中的内容创建一个新的文件系统映像[对应export] info Display system-wide information # 显示系统相关信息 inspect Return low-level information on a container # 查看容器详细信息 kill Kill a running container # kill 指定 docker 容器 load Load an image from a tar archive # 从一个 tar 包中加载一个镜像[对应 save] login Register or Login to the docker registry server # 注册或者登陆一个 docker 源服务器 logout Log out from a Docker registry server # 从当前 Docker registry 退出 logs Fetch the logs of a container # 输出当前容器日志信息 port Lookup the public-facing port which is NAT-ed to PRIVATE_PORT # 查看映射端口对应的容器内部源端口 pause Pause all processes within a container # 暂停容器 ps List containers # 列出容器列表 pull Pull an image or a repository from the docker registry server # 从docker镜像源服务器拉取指定镜像或者库镜像 push Push an image or a repository to the docker registry server # 推送指定镜像或者库镜像至docker源服务器 restart Restart a running container # 重启运行的容器 rm Remove one or more containers # 移除一个或者多个容器 rmi Remove one or more images # 移除一个或多个镜像[无容器使用该镜像才可删除，否则需删除相关容器才可继续或 -f 强制删除] run Run a command in a new container # 创建一个新的容器并运行一个命令 save Save an image to a tar archive # 保存一个镜像为一个 tar 包[对应 load] search Search for an image on the Docker Hub # 在 docker hub 中搜索镜像 start Start a stopped containers # 启动容器 stop Stop a running containers # 停止容器 tag Tag an image into a repository # 给源中镜像打标签 top Lookup the running processes of a container # 查看容器中运行的进程信息 unpause Unpause a paused container # 取消暂停容器 version Show the docker version information # 查看 docker 版本号 wait Block until a container stops, then print its exit code # 截取容器停止时的退出状态值 四、Docker镜像 4.1 是什么 【镜像】 是一种轻量级、可执行的独立软件包，它包含运行某个软件所需的所有内容，我们把应用程序和配置依赖打包好形成一个可交付的运行环境(包括代码、运行时需要的库、环境变量和配置文件等)，这个打包好的运行环境就是image镜像文件。 只有通过这个镜像文件才能生成Docker容器实例(类似Java中new出来一个对象)。 【分层镜像】 以我们的pull为例，在下载的过程中我们可以看到docker的镜像好像是在一层一层的在下载 。 【UnionFS（联合文件系统）】 UnionFS（联合文件系统）：Union文件系统（UnionFS）是一种分层、轻量级并且高性能的文件系统，它支持 对文件系统的修改作为一次提交来一层层的叠加， 同时可以将不同目录挂载到同一个虚拟文件系统下(unite several directories into a single virtual filesystem)。Union 文件系统是 Docker 镜像的基础。 镜像可以通过分层来进行继承 ，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。 特性：一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录 Docker镜像加载原理\nDocker镜像加载原理：\ndocker的镜像实际上由一层一层的文件系统组成，这种层级的文件系统UnionFS。\nbootfs(boot file system)主要包含bootloader和kernel, bootloader主要是引导加载kernel, Linux刚启动时会加载bootfs文件系统， 在Docker镜像的最底层是引导文件系统bootfs。 这一层与我们典型的Linux/Unix系统是一样的，包含boot加载器和内核。当boot加载完成之后整个内核就都在内存中了，此时内存的使用权已由bootfs转交给内核，此时系统也会卸载bootfs。\nrootfs (root file system) ，在bootfs之上 。包含的就是典型 Linux 系统中的 /dev, /proc, /bin, /etc 等标准目录和文件。rootfs就是各种不同的操作系统发行版，比如Ubuntu，Centos等等。\n平时我们安装进虚拟机的CentOS都是好几个G，为什么docker这里才200M？？\n对于一个精简的OS，rootfs可以很小，只需要包括最基本的命令、工具和程序库就可以了，因为底层直接用Host的kernel，自己只需要提供 rootfs 就行了。由此可见对于不同的linux发行版, bootfs基本是一致的, rootfs会有差别, 因此不同的发行版可以公用bootfs。\n为什么Docker镜像要采用这种分层结构呢\n镜像分层最大的一个好处就是共享资源，方便复制迁移，就是为了复用。\n比如说有多个镜像都从相同的 base 镜像构建而来，那么 Docker Host 只需在磁盘上保存一份 base 镜像； 同时内存中也只需加载一份 base 镜像，就可以为所有容器服务了。而且镜像的每一层都可以被共享。\n4.2 重点理解 Docker镜像层都是只读的，容器层是可写的，当容器启动时，一个新的可写层被加载到镜像的顶部。这一层通常被称作\u0026quot;容器层\u0026quot;，\u0026ldquo;容器层\u0026quot;之下的都叫\u0026quot;镜像层\u0026rdquo;。\n所有对容器的改动 - 无论添加、删除、还是修改文件都只会发生在容器层中。只有容器层是可写的，容器层下面的所有镜像层都是只读的。\n4.3 Docker镜像commit操作案例 docker commit 提交容器副本使之成为一个新的镜像\ndocker commit -m=\u0026ldquo;提交的描述信息\u0026rdquo; -a=\u0026ldquo;作者\u0026rdquo; 容器ID 要创建的目标镜像名:[标签名]\n【案例演示】ubuntu安装vim\n从Hub上下ubuntu镜像到笨地并成功运行 原始默认Ubuntu镜像是不带着vim命令的 外网连通情况下，安装vim # 先更新我们的包管理工具 apt-get update # 然后安装我们需要的vim apt-get install vim docker容器内执行上述两条命令：\napt-get update\napt-get -y install vim\n安装完成后，commit我们自己的新镜像\n启动我们的新镜像并和原来的对比\n官网是默认下载的Ubuntu没有vim命令\n我们自己commit构建的镜像，新增加了vim功能，可以成功使用。\n总结\nDocker中的镜像分层， 支持通过扩展现有镜像，创建新的镜像 。类似Java继承于一个Base基础类，自己再按需扩展。 新镜像是从 base 镜像一层一层叠加生成的。每安装一个软件，就在现有镜像的基础上增加一层 五、本地镜像发布到阿里云 5.1 本地镜像发布到阿里云流程 5.2 镜像生成的方法 上一讲已经介绍过\n基于当前容器创建一个新的镜像，新功能增强\ndocker commit [OPTIONS]容器ID [REPOSOTORY[:TAG]]\nOPTIONS说明：\n-a :提交的镜像作者；\n-m :提交时的说明文字；\n本次案例centos+ubuntu两个，当堂讲解一个，家庭作业一个，请大家务必动手，亲自实操。\n5.3 将本地镜像推送到阿里云 本地镜像素材原型\n阿里云开发者平台\n地址：https://promotion.aliyun.com/ntms/act/kubernetes.html\n将镜像推送到阿里云\n将镜像推送到阿里云registry ，管理界面脚本\n脚本实例\ndocker login --username=zzyybuy registry.cn-hangzhou.aliyuncs.com docker tag cea1bb40441c registry.cn-hangzhou.aliyuncs.com/atguiguwh/myubuntu:1.1 docker push registry.cn-hangzhou.aliyuncs.com/atguiguwh/myubuntu:1.1 上面命令是阳哥自己本地的，你自己酌情处理，不要粘贴我的。 5.4 将阿里云上的镜像下载到本地 docker pull registry.cn-hangzhou.aliyuncs.com/atguiguwh/myubuntu:1.1 六、本地镜像发布到私有库 6.1 本地镜像发布到私有库流程 下载镜像Docker Registry\ndocker pull registry\n运行私有库Registry，相当于本地有个私有库Docker hub\ndocker run -d -p 5000:5000 -v /zzyyuse/myregistry/:/tmp/registry \u0026ndash;privileged=true registry\n默认情况，仓库被创建在容器的/var/lib/registry目录下，建议自行用容器卷映射，方便于宿主机联调\n案例演示创建一个新镜像，ubuntu安装ifconfig命令\n从Hub上下载ubuntu镜像到本地并成功运行\n原始Ubuntu镜像是不带着ifconfig命令的\n从Hub上下载ubuntu镜像到本地并成功运行\n原始Ubuntu镜像是不带着ifconfig命令的\n外网连通情况下，安装ifconfig命令通过测试\ndocker容器内 执行上述两条命令：\napt-get update\napt-get install net-tools\n安装完成后，commit我们自己的新镜像\n公式：\ndocker commit -m=\u0026quot; 提交的描述信息 \u0026quot; -a=\u0026quot; 作者 \u0026quot; 容器 ID 要创建的目标镜像名 :[ 标签名 ]\n命令： 在容器外执行，记得\ndocker commit -m=\u0026quot; ifconfig cmd add \u0026quot; -a=\u0026quot; zzyy \u0026quot; a69d7c825c4f zzyyubuntu:1.2\n启动我们的新镜像并和原来的对比\n1.官网是默认下载的Ubuntu没有ifconfig命令\n2.我们自己commit构建的新镜像，新增加了ifconfig功能，可以成功使用。\ncurl验证私服库上有什么镜像\ncurl -XGET http://192.168.111.162:5000/v2/_catalog\n可以看到，目前私服库没有任何镜像上传过。。。。。。\n将新镜像zzyyubuntu:1.2修改符合私服规范的Tag\n按照公式： docker tag 镜像:Tag Host:Port/Repository:Tag 自己host主机IP地址，填写同学你们自己的，不要粘贴错误，O(∩_∩)O 使用命令 docker tag 将zzyyubuntu:1.2 这个镜像修改为192.168.111.162:5000/zzyyubuntu:1.2 docker tag zzyyubuntu:1.2 192.168.111.162:5000/zzyyubuntu:1.2 修改配置文件使之支持http 别无脑照着复制，registry-mirrors 配置的是国内阿里提供的镜像加速地址，不用加速的话访问官网的会很慢。 2个配置中间有个逗号 \u0026#39;,\u0026#39;别漏了 ，这个配置是json格式的。 2个配置中间有个逗号 \u0026#39;,\u0026#39;别漏了 ，这个配置是json格式的。 2个配置中间有个逗号 \u0026#39;,\u0026#39;别漏了 ，这个配置是json格式的。 vim命令新增如下红色内容：vim /etc/docker/daemon.json\n{ \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://aa25jngu.mirror.aliyuncs.com\u0026#34;] , \u0026#34;insecure-registries\u0026#34;: [\u0026#34;192.168.111.162:5000\u0026#34;] } 上述理由：docker默认不允许http方式推送镜像，通过配置选项来取消这个限制。====\u0026gt; 修改完后如果不生效，建议重启docker\npush推送到私服库\ndocker push 192.168.111.162:5000/zzyyubuntu:1.2 curl验证私服库上有什么镜像2\ncurl -XGET http://192.168.111.162:5000/v2/_catalog\npull到本地并运行\ndocker pull 192.168.111.162:5000/zzyyubuntu:1.2 docker run -it 镜像ID /bin/bash\n七、Docker容器数据卷 7.1 坑：容器卷记得加入 --privileged=true # 原因 Docker挂载主机目录访问 如果出现cannot open directory .: Permission denied 解决办法：在挂载目录后多加一个--privileged=true参数即可 如果是CentOS7安全模块会比之前系统版本加强，不安全的会先禁止，所以目录挂载的情况被默认为不安全的行为， 在SELinux里面挂载目录被禁止掉了额，如果要开启，我们一般使用--privileged=true命令，扩大容器的权限解决挂载目录没有权限的问题，也即 使用该参数，container内的root拥有真正的root权限，否则，container内的root只是外部的一个普通用户权限。 7.2 回顾下上一将的知识点，参数V 还记得蓝色框框中的内容嘛\n7.3 是什么 一句话：有点类似我们Redis里面的rdb和aof文件 将docker容器内的数据保存进宿主机的磁盘中 运行一个带有容器卷存储功能的容器实例 docker run -it --privileged=true -v /宿主机绝对路径目录:/容器内目录 镜像名 7.4 能干什么 将运用与运行的环境打包镜像，run后形成容器实例运行 ，但是我们对数据的要求希望是 持久化的 Docker容器产生的数据，如果不备份，那么当容器实例删除后，容器内的数据自然也就没有了。 为了能保存数据在docker中我们使用卷。 特点： 1：数据卷可在容器之间共享或重用数据 2：卷中的更改可以直接实时生效，爽 3：数据卷中的更改不会包含在镜像的更新中 4：数据卷的生命周期一直持续到没有容器使用它为止 7.5 数据卷案例 7.5.1 宿主vs容器之间映射添加容器卷 直接命令添加\n公式：docker run -it -v /宿主机目录:/容器内目录 ubuntu /bin/bash docker run -it --name myu3 --privileged=true -v /tmp/myHostData:/tmp/myDockerData ubuntu /bin/bash 查看数据卷是否挂成功\ndocker inspect 容器ID 容器和宿主机之间数据共享\n1. docker修改，主机同步获得 2. 主机修改，docker同步获得 3. docker容器stop，主机修改，docker容器重启看数据是否同步。 7.5.2 读写规则映射添加说明 读写(默认)\ndocker run -it --privileged=true -v /宿主机绝对路径目录:/容器内目录:rw 镜像名 默认同上案例，默认就是rw 默认同上案例，默认就是rw\n只读\n容器实例内部被限制，只能读取不能写\n/容器目录:ro 镜像名 就能完成功能，此时容器自己只能读取不能写 ro = read only 此时如果宿主机写入内容，可以同步给容器内，容器可以读取到。 docker run -it --privileged=true -v /宿主机绝对路径目录:/容器内目录:ro 镜像名 7.5.3 卷的集成和共享 容器1完成和宿主机的映射\ndocker run -it --privileged=true -v /mydocker/u:/tmp --name u1 ubuntu 容器2集成容器1的卷规则\ndocker run -it --privileged=true --volumes-from 父类 --name u2 ubuntu 八、Docker常规安装简介 8.1 总体步骤 1. 搜索镜像 2. 拉去镜像 3. 查看镜像 4. 查看镜像 5. 启动镜像 服务端口映射 6. 停止容器 8.2 安装tomcat 1、docker hub 上面查找tomcat镜像\n# 命令 docker search tomcat 2、从docker hub 上拉去tomcat镜像到本地\n# 命令 docker pull tomcat 3、docker images 查看是否有拉去到tomcat\n# 命令 docker images tomcat 4、使用tomcat镜像创建容器实例（也叫运行镜像）\n# 命令 docker run -it -p 8080:8080 tomcat -p 小写，主机端口:docker容器端口 -P 大写，随机分配端口 i:交互 t:终端 d:后台 5、访问tomcat首页\n可能出现404 的情况 解决 1、可能没有映射端口或者没有关闭防火墙 2、把webapps.dist 目录换成webapps 先成功启动tomcat 查看webapps文件夹查看为空\n6、免修改版说明\ndocker pull billygoo/tomcat8-jdk8\nDocker run -d -p 8080:8080 \u0026ndash;name mytomcat8 billygoo/tomcat8-djk8\n8.3 安装mysql 1、docker hub上面查找mysql镜像\n# 命令 docker search mysql 2、从docker hub上（阿里云加速器）拉去mysql镜像到本地标签为5.7\n# 命令 docker pull mysql:5.7 3、使用mysql5.7 镜像创建容器（也叫运行镜像）\n# 1、命令出处，哪里来的 地址：https://hub.docker.com/_/mysql # 2、简单版 docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.7 docker ps docker exec -it 容器ID /bin/bash mysql -uroot -p # 4、 建库建表插入数据 外部Win10也来连接运行在dokcer上的mysql容器实例服务 【问题】 插入中文数据试试，为什么报错？ docker 上默认字符集编码隐患 docker里面的mysql容器实例查看，内容如下： SHOW VARIABLES LIKE \u0026#39;character%\u0026#39; 删除容器后，里面的mysql数据如何办 容器实例一删除，你还有什么？ 删容器到跑路。。。。。？ 【实战版】\n#1、新建mysql容器实例 docker run -d -p 3306:3306 --privileged=true -v /zzyyuse/mysql/log:/var/log/mysql -v /zzyyuse/mysql/data:/var/lib/mysql -v /zzyyuse/mysql/conf:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=123456 --name mysql mysql:5.7 #2、新建my.cnf 通过容器卷同步给MySQL容器实例 [client] default_character_set=utf8 [mysqld] collation_server = utf8_general_ci character_set_server = utf8 #3、重新启动mysql容器实例在重新进入并查看字符编码 docker restart mysql docker exec -it mysql_bash show variables like \u0026#39;character%\u0026#39;; #4、再新建库新建表再插入中文测试 完全正常 #5、结论 之前的DB 无效 修改字符集操作+重启mysql容器实例 之后的DB 有效，需要新建 结论： docker安装完MySQL并run出容器后，建议请先修改完字符集编码后再新建mysql库-表-插数据 #6、假如将当前容器实例删除，再重新来一次，之前建的db01实例还有吗？trytry 8.4 安装redis 1、从docker hub上（阿里云加速器）拉去redis镜像到本地标签6.0.8\n# 拉去镜像 docker pull redis:6.0.8 # 查看镜像 docker images 2、入门命令\n# 启动命令 docker run -d -p 6379:6379 redis:6.0.8 # docker ps # 后台启动 docker exec -it CONTAINER ID /bin/bash 3、命令提醒：容器卷记得加入 \u0026ndash;privileged=true\nDocker挂载主机目录Docker访问出现cannot open directory .: Permission denied 解决办法：在挂载目录后多加一个--privileged=true参数即可 4、在CentOS宿主机下新建目录/app/redis\n# 新建目录 mkdir -p /app/redis 5、将一个redis.conf文件模板拷贝进 /app/redis目录下\nmkdir -p /app/redis cp /myredis/redis.conf /app/redis/ cp /app/redis 6、/app/redis 目录下修改redis.conf\n# 修改redis.conf文件 /app/redis目录下修改redis.conf文件 开启redis验证 可选 requirepass 123 允许redis外地连接 必须 注释掉 # bind 127.0.0.1 # 注释daemonize no daemonize no 将daemonize yes注释起来或者 daemonize no设置，因为该配置和docker run中-d参数冲突，会导致容器一直启动失败 # 开启redis数据持久化 appendonly yes 可选 7、使用redis6.0.8 镜像创建容器(也叫运行镜像)\ndocker run -p 6379:6379 --name myr3 --privileged=true -v /app/redis/redis.conf:/etc/redis/redis.conf -v /app/redis/data:/data -d redis:6.0.8 redis-server /etc/redis/redis.conf 8、测试redis-cli连接上\ndocker exec -it 运行着Rediis服务的容器ID redis-cli\n9、请证明docker启动使用了我们自己指定的配置文件\n【修改前】\n【修改后】\n10、测试redis-cli连接上来第2次\n","permalink":"http://hugo.itshare.work/posts/docker/docker/","summary":"Docker 详细教程 一、Docker简介 1.1 docker是什么 【问题】：问什么会有docker出现 ​ Docker的出现 使得Docker得以打破过去「程序即应用」的观念。透过镜像(images)将作业系统核心除外，运作应用程式所需要的系统环境，由下而上打包，达到应用程式跨平台间的无缝接轨运作。","title":"Docker基础详细教程"},{"content":"搭建Kubernetes集群 部署前提 使用kubeadm部署Kubernetes集群的前提条件 支持Kubernetes运行的Linux主机，例如Debian、RedHat及其变体等 每主机2GB以上的内存，以及2颗以上的CPU 各主机间能够通过网络无障碍通信 独占的hostname、MAC地址以及product_uuid，主机名能够正常解析 放行由Kubernetes使用到的各端口，或直接禁用iptables 禁用各主机的上的Swap设备 各主机时间同步 部署环境 OS: Ubuntu 20.04.2 LTS Docker：20.10.10，CGroup Driver: systemd Kubernetes：v1.26.3, CRI: containerd, CNI: Flannel 主机 主机IP 主机名称 角色 192.168.32.200 k8s-master01.org master 192.168.32.203 k8s-node01.org node01 192.168.32.204 k8s-node02.org node02 192.168.32.205 k8s-node03.org node03 修改主机名称 # 修改192.168.32.200的主机名称为k8s-master01.org # 修改192.168.32.203的主机名称为k8s-node01.org # 修改192.168.32.204的主机名称为k8s-node02.org # 修改192.168.32.205的主机名称为k8s-node03.org 主机时间同步 在所有主机上安装 chrony\n## 所有主机上执行 root@k8s-master01:~# apt install -y chrony 建议用户配置使用本地的的时间服务器，在节点数量众多时尤其如此。存在可用的本地时间服务器时，修改节点的/etc/chrony/chrony.conf配置文件，并将时间服务器指向相应的主机即可，配置格式如下：\nserver CHRONY-SERVER-NAME-OR-IP iburst 主机名称解析 出于简化配置步骤的目的，本测试环境使用hosts文件进行各节点名称解析，文件内容如下所示。其中，我们使用kubeapi主机名作为API Server在高可用环境中的专用接入名称，也为控制平面的高可用配置留下便于配置的余地。\n# 编辑/etc/hosts文件加入如下内容 root@k8s-master01:~# vim /etc/hosts 192.168.32.200 k8s-master01.org 192.168.32.203 k8s-node01.org 192.168.32.204 k8s-node02.org 192.168.32.205 k8s-node03.org 禁用Swap设备 部署集群时，kubeadm默认会预先检查当前主机是否禁用了Swap设备，并在未禁用时强制终止部署过程。因此，在主机内存资源充裕的条件下，需要禁用所有的Swap设备，否则，就需要在后文的kubeadm init及kubeadm join命令执行时额外使用相关的选项忽略检查错误。\n关闭Swap设备，需要分两步完成。首先是关闭当前已启用的所有Swap设备：\n# 临时关闭，所有机器执行 root@k8s-master01:~# swapoff -a 而后编辑/etc/fstab配置文件，注释用于挂载Swap设备的所有行\n# 所有机器执行 root@k8s-master01:~#vim /etc/fstab # 注释如下一行 #/swap.img none swap sw 0 0 禁用默认的防火墙服务 Ubuntu和Debian等Linux发行版默认使用ufw（Uncomplicated FireWall）作为前端来简化 iptables的使用，处于启用状态时，它默认会生成一些规则以加强系统安全。出于降低配置复杂度之目的，本文选择直接将其禁用。\nroot@k8s-master01:~# ufw disable Firewall stopped and disabled on system startup root@k8s-master01:~# ufw status Status: inactive root@k8s-master01:~# 安装程序包 提示：以下操作需要在本示例中的所有四台主机上分别进行\n安装并启动docker 首先，生成docker-ce相关程序包的仓库，这里以阿里云的镜像服务器为例进行说明\ndocker-ce镜像_docker-ce下载地址_docker-ce安装教程-阿里巴巴开源镜像站 (aliyun.com)\n# step 1: 安装必要的一些系统工具 sudo apt-get update sudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common # step 2: 安装GPG证书 curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add - # Step 3: 写入软件源信息 sudo add-apt-repository \u0026#34;deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable\u0026#34; # Step 4: 更新并安装Docker-CE sudo apt-get -y update sudo apt-get -y install docker-ce # 安装指定版本的Docker-CE: # Step 1: 查找Docker-CE的版本: # apt-cache madison docker-ce # docker-ce | 17.03.1~ce-0~ubuntu-xenial | https://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages # docker-ce | 17.03.0~ce-0~ubuntu-xenial | https://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages # Step 2: 安装指定版本的Docker-CE: (VERSION例如上面的17.03.1~ce-0~ubuntu-xenial) # sudo apt-get -y install docker-ce=[VERSION] 本文以为20.10.10版本为例\nroot@k8s-node3:~# apt install docker-ce=5:20.10.10~3-0~ubuntu-focal docker-ce-cli=5:20.10.10~3-0~ubuntu-focal kubelet需要让docker容器引擎使用systemd作为CGroup的驱动，其默认值为cgroupfs，因而，我们还需要编辑docker的配置文件/etc/docker/daemon.json，添加如下内容，其中的registry-mirrors用于指明使用的镜像加速服务。\nvim /etc/docker/daemon.json { \u0026#34;registry-mirrors\u0026#34;: [ \u0026#34;https://ung2thfc.mirror.aliyuncs.com\u0026#34;, \u0026#34;https://mirror.ccs.tencentyun.com\u0026#34;, \u0026#34;https://registry.docker-cn.com\u0026#34;, \u0026#34;http://hub-mirror.c.163.com\u0026#34;, \u0026#34;https://docker.mirrors.ustc.edu.cn\u0026#34;], \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;] } systemctl daemon-reload \u0026amp;\u0026amp; systemctl restart docker 安装cri-dockerd Kubernetes自v1.24移除了对docker-shim的支持，而Docker Engine默认又不支持CRI规范，因而二者将无法直接完成整合。为此，Mirantis和Docker联合创建了cri-dockerd项目，用于为Docker Engine提供一个能够支持到CRI规范的垫片，从而能够让Kubernetes基于CRI控制Docker 。\n项目地址\ncri-dockerd项目提供了预制的二进制格式的程序包，用户按需下载相应的系统和对应平台的版本即可完成安装，这里以Ubuntu 2004 64bits系统环境，以及cri-dockerd目前最新的程序版本v0.3.0为例。\nwget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.0/cri-dockerd_0.3.0.3-0.ubuntu-focal_amd64.deb dpkg -i cri-dockerd_0.3.0.3-0.ubuntu-focal_amd64.deb 完成安装后，相应的服务cri-dockerd.service便会自动启动。我们也可以使用如下命令进行验证，若服务处于Running状态即可进行后续步骤 。\nroot@k8s-master01:~# systemctl status cri-docker.service ● cri-docker.service - CRI Interface for Docker Application Container Engine Loaded: loaded (/lib/systemd/system/cri-docker.service; enabled; vendor preset: enabled) Active: active (running) since Tue 2023-03-21 10:59:57 CST; 1min 20s ago TriggeredBy: ● cri-docker.socket Docs: https://docs.mirantis.com Main PID: 17591 (cri-dockerd) Tasks: 7 Memory: 11.9M CGroup: /system.slice/cri-docker.service └─17591 /usr/bin/cri-dockerd --container-runtime-endpoint fd:// Mar 21 10:59:57 k8s-master01.org cri-dockerd[17591]: time=\u0026#34;2023-03-21T10:59:57+08:00\u0026#34; level=info msg=\u0026#34;Start docker client with request timeout 0s\u0026#34; Mar 21 10:59:57 k8s-master01.org cri-dockerd[17591]: time=\u0026#34;2023-03-21T10:59:57+08:00\u0026#34; level=info msg=\u0026#34;Hairpin mode is set to none\u0026#34; Mar 21 10:59:57 k8s-master01.org cri-dockerd[17591]: time=\u0026#34;2023-03-21T10:59:57+08:00\u0026#34; level=info msg=\u0026#34;Loaded network plugin cni\u0026#34; Mar 21 10:59:57 k8s-master01.org cri-dockerd[17591]: time=\u0026#34;2023-03-21T10:59:57+08:00\u0026#34; level=info msg=\u0026#34;Docker cri networking managed by network plugin cni\u0026#34; Mar 21 10:59:57 k8s-master01.org systemd[1]: Started CRI Interface for Docker Application Container Engine. Mar 21 10:59:58 k8s-master01.org cri-dockerd[17591]: time=\u0026#34;2023-03-21T10:59:57+08:00\u0026#34; level=info msg=\u0026#34;Docker Info: \u0026amp;{ID:WQBA:P7R2:H6ZI:KWU3:FVFW:MHLC:QTT7:CJCX\u0026gt; Mar 21 10:59:58 k8s-master01.org cri-dockerd[17591]: time=\u0026#34;2023-03-21T10:59:57+08:00\u0026#34; level=info msg=\u0026#34;Setting cgroupDriver cgroupfs\u0026#34; Mar 21 10:59:58 k8s-master01.org cri-dockerd[17591]: time=\u0026#34;2023-03-21T10:59:57+08:00\u0026#34; level=info msg=\u0026#34;Docker cri received runtime config \u0026amp;RuntimeConfig{Network\u0026gt; Mar 21 10:59:58 k8s-master01.org cri-dockerd[17591]: time=\u0026#34;2023-03-21T10:59:57+08:00\u0026#34; level=info msg=\u0026#34;Starting the GRPC backend for the Docker CRI interface.\u0026#34; Mar 21 10:59:58 k8s-master01.org cri-dockerd[17591]: time=\u0026#34;2023-03-21T10:59:57+08:00\u0026#34; level=info msg=\u0026#34;Start cri-dockerd grpc backend\u0026#34; lines 1-21/21 (END) 安装kubelet、kubeadm和kubectl 首先，在各主机上生成kubelet和kubeadm等相关程序包的仓库，这里以阿里云的镜像服务为例\napt-get update \u0026amp;\u0026amp; apt-get install -y apt-transport-https curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat \u0026lt;\u0026lt;EOF \u0026gt;/etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF apt-get update apt-get install -y kubelet kubeadm kubectl 安装完成后，要确保kubeadm等程序文件的版本，这将也是后面初始化Kubernetes集群时需要明确指定的版本号\n整合kubelet和cri-dockerd 仅支持CRI规范的kubelet需要经由遵循该规范的cri-dockerd完成与docker-ce的整合。\n配置cri-dockerd 配置cri-dockerd，确保其能够正确加载到CNI插件。编辑/usr/lib/systemd/system/cri-docker.service文件，确保其[Service]配置段中的ExecStart的值类似如下内容\nExecStart=/usr/bin/cri-dockerd --container-runtime-endpoint fd:// --network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.7 --cni-bin-dir=/opt/cni/bin --cni-cache-dir=/var/lib/cni/cache --cni-conf-dir=/etc/cni/net.d 需要添加的各配置参数（各参数的值要与系统部署的CNI插件的实际路径相对应）：\n\u0026ndash;network-plugin：指定网络插件规范的类型，这里要使用CNI； \u0026ndash;cni-bin-dir：指定CNI插件二进制程序文件的搜索目录； \u0026ndash;cni-cache-dir：CNI插件使用的缓存目录； \u0026ndash;cni-conf-dir：CNI插件加载配置文件的目录； 配置完成后，重载并重启cri-docker.service服务。\nroot@k8s-master01:~# systemctl daemon-reload;systemctl restart cri-docker 配置kubelet 配置kubelet，为其指定cri-dockerd在本地打开的Unix Sock文件的路径，该路径一般默认为“/run/cri-dockerd.sock“。编辑文件/etc/sysconfig/kubelet，为其添加 如下指定参数。\n提示：若/etc/sysconfig目录不存在，则需要先创建该目录。\nKUBELET_KUBEADM_ARGS=\u0026#34;--container-runtime=remote --container-runtime-endpoint=/run/cri-dockerd.sock\u0026#34; 需要说明的是，该配置也可不进行，而是直接在后面的各kubeadm命令上使用“\u0026ndash;cri-socket unix:///run/cri-dockerd.sock”选项。\n初始化第一个主节点 该步骤开始尝试构建Kubernetes集群的master节点，配置完成后，各worker节点直接加入到集群中的即可。需要特别说明的是，由kubeadm部署的Kubernetes集群上，集群核心组件kube-apiserver、kube-controller-manager、kube-scheduler和etcd等均会以静态Pod的形式运行，它们所依赖的镜像文件默认来自于registry.k8s.io这一Registry服务之上。但我们无法直接访问该服务，常用的解决办法有如下两种\n使用能够到达该服务的代理服务； 使用国内的镜像服务器上的服务，例如registry.aliyuncs.com/google_containers等。 初始化master节点（在k8s-master01上完成如下操作） 运行如下命令完成k8s-master01节点的初始化：\nkubeadm init --image-repository registry.aliyuncs.com/google_containers --kubernetes-version=v1.26.3 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --token-ttl=0 --cri-socket unix:///run/cri-dockerd.sock 命令中的各选项简单说明如下：\n\u0026ndash;image-repository：指定要使用的镜像仓库，默认为registry.k8s.io； \u0026ndash;kubernetes-version：kubernetes程序组件的版本号，它必须要与安装的kubelet程序包的版本号相同； \u0026ndash;control-plane-endpoint：控制平面的固定访问端点，可以是IP地址或DNS名称，会被用于集群管理员及集群组件的kubeconfig配置文件的API Server的访问地址；单控制平面部署时可以不使用该选项； \u0026ndash;pod-network-cidr：Pod网络的地址范围，其值为CIDR格式的网络地址，通常，Flannel网络插件的默认为10.244.0.0/16，Project Calico插件的默认值为192.168.0.0/16； \u0026ndash;service-cidr：Service的网络地址范围，其值为CIDR格式的网络地址，默认为10.96.0.0/12；通常，仅Flannel一类的网络插件需要手动指定该地址； \u0026ndash;apiserver-advertise-address：apiserver通告给其他组件的IP地址，一般应该为Master节点的用于集群内部通信的IP地址，0.0.0.0表示节点上所有可用地址； \u0026ndash;token-ttl：共享令牌（token）的过期时长，默认为24小时，0表示永不过期；为防止不安全存储等原因导致的令牌泄露危及集群安全，建议为其设定过期时长。未设定该选项时，在token过期后，若期望再向集群中加入其它节点，可以使用如下命令重新创建token，并生成节点加入命令。 初始化完成后的操作步骤 对于Kubernetes系统的新用户来说，无论使用上述哪种方法，命令运行结束后，请记录最后的kubeadm join命令输出的最后提示的操作步骤。下面的内容是需要用户记录的一个命令输出示例，它提示了后续需要的操作步骤。\n# 下面是成功完成第一个控制平面节点初始化的提示信息及后续需要完成的步骤 Your Kubernetes control-plane has initialized successfully! # 为了完成初始化操作，管理员需要额外手动完成几个必要的步骤 To start using your cluster, you need to run the following as a regular user: # 第1个步骤提示， Kubernetes集群管理员认证到Kubernetes集群时使用的kubeconfig配置文件 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # 我们也可以不做上述设定，而使用环境变量KUBECONFIG为kubectl等指定默认使用的kubeconfig； Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf # 第2个步骤提示，为Kubernetes集群部署一个网络插件，具体选用的插件则取决于管理员； You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ # 第3个步骤提示，向集群添加额外的控制平面节点，但本文会略过该步骤，并将在其它文章介绍其实现方式。 You can now join any number of the control-plane node running the following command on each as root: # 第4个步骤提示，向集群添加工作节点 Then you can join any number of worker nodes by running the following on each as root: # 在部署好kubeadm等程序包的各工作节点上以root用户运行类似如下命令； # 提示：与cri-dockerd结合使用docker-ce作为container runtime时，通常需要为下面的命令 # 额外附加“--cri-socket unix:///run/cri-dockerd.sock”选项； kubeadm join 192.168.32.200:6443 --token ivu3t7.pogk70dd5pualoz2 \\ --discovery-token-ca-cert-hash sha256:3edb3c8e3e6c944afe65b2616d46b49305c1420e6967c1fab966ddf8f149502d 设定kubectl kubectl是kube-apiserver的命令行客户端程序，实现了除系统部署之外的几乎全部的管理操作，是kubernetes管理员使用最多的命令之一。kubectl需经由API server认证及授权后方能执行相应的管理操作，kubeadm部署的集群为其生成了一个具有管理员权限的认证配置文件/etc/kubernetes/admin.conf，它可由kubectl通过默认的“$HOME/.kube/config”的路径进行加载。当然，用户也可在kubectl命令上使用\u0026ndash;kubeconfig选项指定一个别的位置。\n下面复制认证为Kubernetes系统管理员的配置文件至目标用户（例如当前用户root）的家目录下：\n~# mkdir ~/.kube\n~# cp /etc/kubernetes/admin.conf ~/.kube/config\n部署网络插件 Kubernetes系统上Pod网络的实现依赖于第三方插件进行，这类插件有近数十种之多，较为著名的有flannel、calico、canal和kube-router等，简单易用的实现是为CoreOS提供的flannel项目。下面的命令用于在线部署flannel至Kubernetes系统之上：\n首先，下载适配系统及硬件平台环境的flanneld至每个节点，并放置于/opt/bin/目录下。我们这里选用flanneld-amd64，目前最新的版本为v0.21.3，因而，我们需要在集群的每个节点上执行如下命令：\n~# mkdir /opt/cni/bin/ ~# curl -L https://github.com/flannel-io/flannel/releases/download/v0.20.2/flanneld-amd64 -o /opt/cni/bin/flanneld ~# chmod +x /opt/cni/bin/flanneld 提示：下载flanneld的地址为 https://github.com/flannel-io/flannel/releases 随后，在初始化的第一个master节点k8s-master01上运行如下命令，向Kubernetes部署kube-flannel\nkubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/v0.21.3/Documentation/kube-flannel.yml 而后使用如下命令确认其输出结果中Pod的状态为“Running”，类似如下命令及其输入的结果所示：\nroot@k8s-master01:~# kubectl get pods -n kube-flannel NAME READY STATUS RESTARTS AGE kube-flannel-ds-jgkxd 1/1 Running 0 2m59s root@k8s-master01:~# 验证master节点已经就绪 kubectl get nodes\n上述命令应该会得到类似如下输出，这表示k8s-master01节点已经就绪\nroot@k8s-master01:~# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master01.org Ready control-plane 62m v1.26.3 root@k8s-master01:~# 添加节点到集群中 下面的两个步骤，需要分别在k8s-node01、k8s-node02和k8s-node03上各自完成。\n1、若未禁用Swap设备，编辑kubelet的配置文件/etc/default/kubelet，设置其忽略Swap启用的状态错误，内容如下：KUBELET_EXTRA_ARGS=\u0026quot;\u0026ndash;fail-swap-on=false\u0026quot;\n2、将节点加入第二步中创建的master的集群中，要使用主节点初始化过程中记录的kubeadm join命令；\nroot@k8s-node01:/opt/cni/bin# kubeadm join 192.168.32.200:6443 --token ivu3t7.pogk70dd5pualoz2 --discovery-token-ca-cert-hash sha256:3edb3c8e3e6c944afe65b2616d46b49305c1420e6967c1fab966ddf8f149502d --cri-socket unix:///run/cri-dockerd.sock 验证节点添加结果 在每个节点添加完成后，即可通过kubectl验证添加结果。下面的命令及其输出是在所有的三个节点均添加完成后运行的，其输出结果表明三个Worker Node已经准备就绪。\n~# kubectl get nodes\nroot@k8s-master01:~# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master01.org Ready control-plane 80m v1.26.3 k8s-node01.org Ready \u0026lt;none\u0026gt; 15m v1.26.3 k8s-node2.org Ready \u0026lt;none\u0026gt; 114s v1.26.3 k8s-node3.org Ready \u0026lt;none\u0026gt; 11m v1.26.3 root@k8s-master01:~# 测试应用编排及服务访问 到此为止，一个master，并附带有三个worker的kubernetes集群基础设施已经部署完成，用户随后即可测试其核心功能。\nroot@k8s-master01:~# kubectl create deployment test-nginx --image=nginx:latest --replicas=3 deployment.apps/test-nginx created root@k8s-master01:~# root@k8s-master01:~# kubectl create service nodeport test-nginx --tcp=80:80 service/test-nginx created root@k8s-master01:~# 而后，使用如下命令了解Service对象test-nginx使用的NodePort，以便于在集群外部进行访问：\nroot@k8s-master01:~# kubectl get svc -l app=test-nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE test-nginx NodePort 10.100.229.239 \u0026lt;none\u0026gt; 80:31888/TCP 50s root@k8s-master01:~# 因此，用户可以于集群外部通过\u0026#34;http://NodeIP:31888\u0026#34;这个URL访问we应用，例如于集群外通过浏览器访问\u0026#34;http://192.168.32.203:31888\u0026#34; 小结 本文给出了部署Kubernetes分布式集群的具体步骤，并在最后测试了将应用部署并运行于Kubernetes系统上的结果。在读者朋友们自行测试时，cri-dockerd、docker-ce、flannel、kubeadm、kubectl和kubelet的版本均可能存在版本上的不同，也因此可能会存在一定程度上的配置差异，具体调整方式请大家自行参考相关的文档进行\n","permalink":"http://hugo.itshare.work/posts/kubernetes/kubernetes/","summary":"搭建Kubernetes集群 部署前提 使用kubeadm部署Kubernetes集群的前提条件 支持Kubernetes运行的Linux主机，例如Debian、RedHat及其变体等 每主机2GB以上的内存，以及2颗以上的CPU 各主机间能够通过网络无障碍通信 独占的hostname、MA","title":"搭建Kubernetes集群"},{"content":"\rSulv\u0026#39;s Blog\r一个记录技术、阅读、生活的博客\r👉友链格式\r名称： Sulv\u0026rsquo;s Blog 网址： https://www.sulvblog.cn 图标： https://www.sulvblog.cn/img/Q.gif 描述： 一个记录技术、阅读、生活的博客 👉友链申请要求\r秉承互换友链原则、文章定期更新、不能有太多广告、个人描述字数控制在15字内\n👉Hugo博客交流群\r787018782\n","permalink":"http://hugo.itshare.work/links/","summary":"Sulv\u0026#39;s Blog 一个记录技术、阅读、生活的博客 👉友链格式 名称： Sulv\u0026rsquo;s Blog 网址： https://www.sulvblog.cn 图标： https://www.sulvblog.cn/img/Q.gif 描述： 一个记录技术、阅读、生活的博客 👉友链申请要求 秉承互换友链原则、文章定期更新、不能有太多广告、个人描述字数控制在15字内 👉Hugo博客交流群 787018782","title":"🤝友链"},{"content":"关于我\n英文名: Kevin Xu 职业: 程序员 运动: 跑步、乒乓球、爬山 ","permalink":"http://hugo.itshare.work/about/","summary":"关于我 英文名: Kevin Xu 职业: 程序员 运动: 跑步、乒乓球、爬山","title":"🙋🏻‍♂️关于"}]